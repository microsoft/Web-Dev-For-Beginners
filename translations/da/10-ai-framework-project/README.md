<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "3925b6a1c31c60755eaae4d578232c25",
  "translation_date": "2026-01-06T23:21:28+00:00",
  "source_file": "10-ai-framework-project/README.md",
  "language_code": "da"
}
-->
# AI-rammev√¶rk

Har du nogensinde f√∏lt dig overv√¶ldet ved at bygge AI-applikationer fra bunden? Du er ikke alene! AI-rammev√¶rk er som at have en schweizerkniv til AI-udvikling ‚Äì de er kraftfulde v√¶rkt√∏jer, der kan spare dig tid og hovedpiner, n√•r du bygger intelligente applikationer. T√¶nk p√• et AI-rammev√¶rk som et velorganiseret bibliotek: det tilbyder forbyggede komponenter, standardiserede API'er og smarte abstraktioner, s√• du kan fokusere p√• at l√∏se problemer i stedet for at k√¶mpe med implementeringsdetaljer.

I denne lektion vil vi udforske, hvordan rammev√¶rker som LangChain kan g√∏re det, der tidligere var komplekse AI-integrationsopgaver, til ren og l√¶sbar kode. Du vil opdage, hvordan du tackler virkelighedsn√¶re udfordringer som at holde styr p√• samtaler, implementere v√¶rkt√∏jskald og jonglere forskellige AI-modeller gennem √©n samlet gr√¶nseflade.

N√•r vi er f√¶rdige, vil du vide, hvorn√•r du skal v√¶lge rammev√¶rker frem for r√• API-kald, hvordan du effektivt bruger deres abstraktioner, og hvordan du bygger AI-applikationer, der er klar til virkelighedens brug. Lad os udforske, hvad AI-rammev√¶rk kan g√∏re for dine projekter.

## ‚ö° Hvad du kan n√• p√• de n√¶ste 5 minutter

**Hurtig start-rute for travle udviklere**

```mermaid
flowchart LR
    A[‚ö° 5 minutter] --> B[Installer LangChain]
    B --> C[Opret ChatOpenAI-klient]
    C --> D[Send f√∏rste prompt]
    D --> E[Se frameworkets kraft]
```
- **Minut 1**: Installer LangChain: `pip install langchain langchain-openai`
- **Minut 2**: Ops√¶t dit GitHub-token og importer ChatOpenAI-klienten
- **Minut 3**: Opret en simpel samtale med system- og menneskebeskeder
- **Minut 4**: Tilf√∏j et grundl√¶ggende v√¶rkt√∏j (som en add-funktion) og se AI-v√¶rkt√∏jskald
- **Minut 5**: Oplev forskellen mellem r√• API-kald og rammev√¶rksabstraktion

**Hurtig testkode**:
```python
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage

llm = ChatOpenAI(
    api_key=os.environ["GITHUB_TOKEN"],
    base_url="https://models.github.ai/inference",
    model="openai/gpt-4o-mini"
)

response = llm.invoke([
    SystemMessage(content="You are a helpful coding assistant"),
    HumanMessage(content="Explain Python functions briefly")
])
print(response.content)
```

**Hvorfor det betyder noget**: P√• 5 minutter vil du opleve, hvordan AI-rammev√¶rk forvandler kompleks AI-integration til simple metodekald. Dette er fundamentet, der driver produktionsklare AI-applikationer.

## Hvorfor v√¶lge et rammev√¶rk?

S√• du er klar til at bygge en AI-app ‚Äì fantastisk! Men sagen er den: du har flere forskellige veje at g√•, og hver har sine fordele og ulemper. Det er lidt ligesom at v√¶lge mellem at g√•, cykle eller k√∏re for at komme et sted hen ‚Äì de vil alle bringe dig derhen, men oplevelsen (og indsatsen) vil v√¶re helt forskellig.

Lad os bryde de tre hovedm√•der ned, som du kan integrere AI i dine projekter:

| Tilgang | Fordele | Bedst til | Overvejelser |
|----------|------------|----------|--------------|
| **Direkte HTTP-foresp√∏rgsler** | Fuld kontrol, ingen afh√¶ngigheder | Simple foresp√∏rgsler, l√¶ringsgrundlag | Mere omst√¶ndig kode, manuel fejlbehandling |
| **SDK-integration** | Mindre boilerplate, modelspecifik optimering | Applikationer med √©n model | Begr√¶nset til specifikke udbydere |
| **AI-rammev√¶rk** | Enheds-API, indbyggede abstraktioner | Multi-model-applikationer, komplekse workflows | L√¶ringskurve, potentiel overabstraktion |

### Rammev√¶rkets fordele i praksis

```mermaid
graph TD
    A[Din applikation] --> B[AI-rammev√¶rk]
    B --> C[OpenAI GPT]
    B --> D[Anthropic Claude]
    B --> E[GitHub-modeller]
    B --> F[Lokal modeller]
    
    B --> G[Indbyggede v√¶rkt√∏jer]
    G --> H[Hukommelsesstyring]
    G --> I[Samtalehistorik]
    G --> J[Funktionsopkald]
    G --> K[Fejlh√•ndtering]
```
**Hvorfor rammev√¶rk er vigtige:**
- **Forener** flere AI-udbydere under √©n gr√¶nseflade
- **H√•ndterer** samtalememorien automatisk
- **Tilbyder** f√¶rdigbyggede v√¶rkt√∏jer til almindelige opgaver som embeddings og funktionskald
- **Styrer** fejlbehandling og genfors√∏g-logik
- **G√∏r** komplekse workflows til l√¶sbare metodekald

> üí° **Pro Tip**: Brug rammev√¶rk, n√•r du skifter mellem forskellige AI-modeller eller bygger komplekse funktioner som agenter, hukommelse eller v√¶rkt√∏jskald. Hold dig til direkte API‚Äôer, n√•r du l√¶rer det grundl√¶ggende eller bygger simple, fokuserede applikationer.

**Bundlinjen**: Som n√•r man v√¶lger mellem en h√•ndv√¶rkers specialv√¶rkt√∏j og et komplet v√¶rksted, handler det om at matche v√¶rkt√∏jet til opgaven. Rammev√¶rk excellerer til komplekse, funktionsrige applikationer, mens direkte API‚Äôer fungerer godt til mere ligetil brug.

## üó∫Ô∏è Din l√¶ringsrejse mod mestring af AI-rammev√¶rk

```mermaid
journey
    title Fra r√• API'er til produktions-AI-applikationer
    section Framework Foundations
      Forst√• fordelene ved abstraktion: 4: You
      Mestre LangChain grundl√¶ggende: 6: You
      Sammenlign tilgange: 7: You
    section Conversation Systems
      Byg chatgr√¶nseflader: 5: You
      Implementer hukommelsesm√∏nstre: 7: You
      H√•ndter streaming-svar: 8: You
    section Advanced Features
      Opret brugerdefinerede v√¶rkt√∏jer: 6: You
      Mestre struktureret output: 8: You
      Byg dokumentsystemer: 8: You
    section Production Applications
      Kombiner alle funktioner: 7: You
      H√•ndter fejlscenarier: 8: You
      Udrul komplette systemer: 9: You
```
**Din rejsem√•l**: N√•r du er f√¶rdig med denne lektion, har du mestret AI-rammev√¶rksudvikling og kan bygge sofistikerede, produktionsklare AI-applikationer, der kan m√•le sig med kommercielle AI-assistenter.

## Introduktion

I denne lektion l√¶rer vi at:

- Bruge et almindeligt AI-rammev√¶rk.
- L√∏se almindelige problemer som chat-samtaler, brug af v√¶rkt√∏jer, hukommelse og kontekst.
- Udnytte dette til at bygge AI-apps.

## üß† AI-rammev√¶rksudviklings√∏kosystem

```mermaid
mindmap
  root((AI Frameworks))
    Abstraktionsfordele
      Kodesimplificering
        Forenede API'er
        Indbygget Fejlh√•ndtering
        Konsistente M√∏nstre
        Reduceret Gentagelseskode
      Multi-Model Support
        Udbyderagnostisk
        Let Skift
        Fallbackmuligheder
        Omkostningsoptimering
    Kernekomponenter
      Samtaleadministration
        Beskedtyper
        Hukommelsessystemer
        Kontekstsporing
        Historikbevarelse
      V√¶rkt√∏jsintegration
        Funktionsopkald
        API-forbindelser
        Tilpassede V√¶rkt√∏jer
        Workflow Automatisering
    Avancerede Funktioner
      Struktureret Output
        Pydantic-modeller
        JSON-skemaer
        Typetilg√¶ngelighed
        Valideringsregler
      Dokumentbehandling
        Indlejringer
        Vektorbutikker
        Lighedss√∏gning
        RAG-systemer
    Produktionsm√∏nstre
      Applikationsarkitektur
        Modul√¶rt Design
        Fejlgr√¶nser
        Asynkrone Operationer
        Tilstandsadministration
      Implementeringsstrategier
        Skalerbarhed
        Overv√•gning
        Ydeevne
        Sikkerhed
```
**Kerneprincip**: AI-rammev√¶rk abstraherer kompleksitet og tilbyder st√¶rke abstraktioner til samtaleh√•ndtering, v√¶rkt√∏jsintegration og dokumentbehandling, hvilket g√∏r det muligt for udviklere at bygge sofistikerede AI-applikationer med ren og vedligeholdelig kode.

## Din f√∏rste AI-prompt

Lad os starte med det grundl√¶ggende ved at skabe din f√∏rste AI-applikation, der sender et sp√∏rgsm√•l og f√•r et svar tilbage. Som Archimedes, der opdagede forskydningsprincippet i sit bad, f√∏rer de enkleste observationer sommetider til de mest kraftfulde indsigter ‚Äì og rammev√¶rk g√∏r disse indsigter tilg√¶ngelige.

### Ops√¶tning af LangChain med GitHub-modeller

Vi vil bruge LangChain til at forbinde til GitHub Models, hvilket er ret fedt, fordi det giver dig gratis adgang til forskellige AI-modeller. Det bedste? Du beh√∏ver kun nogle f√• simple konfigurationsparametre for at komme i gang:

```python
from langchain_openai import ChatOpenAI
import os

llm = ChatOpenAI(
    api_key=os.environ["GITHUB_TOKEN"],
    base_url="https://models.github.ai/inference",
    model="openai/gpt-4o-mini",
)

# Send en simpel prompt
response = llm.invoke("What's the capital of France?")
print(response.content)
```

**Lad os bryde ned, hvad der sker her:**
- **Opretter** en LangChain-klient ved hj√¶lp af klassen `ChatOpenAI` ‚Äì dette er din port til AI!
- **Konfigurerer** forbindelsen til GitHub Models med dit autentifikationstoken
- **Angiver** hvilken AI-model der skal bruges (`gpt-4o-mini`) ‚Äì t√¶nk p√• det som at v√¶lge din AI-assistent
- **Sender** dit sp√∏rgsm√•l med `invoke()`-metoden ‚Äì her sker magien
- **Ekstraherer** og viser svaret ‚Äì voil√†, du chatter med AI!

> üîß **Ops√¶tningsnotat**: Hvis du bruger GitHub Codespaces, er du heldig ‚Äì `GITHUB_TOKEN` er allerede opsat for dig! Arbejder du lokalt? Ingen problemer, du skal blot oprette et personligt adgangstoken med de rette tilladelser.

**Forventet output:**
```text
The capital of France is Paris.
```

```mermaid
sequenceDiagram
    participant App as Din Python App
    participant LC as LangChain
    participant GM as GitHub Modeller
    participant AI as GPT-4o-mini
    
    App->>LC: llm.invoke("Hvad er hovedstaden i Frankrig?")
    LC->>GM: HTTP anmodning med prompt
    GM->>AI: Behandl prompt
    AI->>GM: Genereret svar
    GM->>LC: Returner svar
    LC->>App: response.content
```
## Opbygning af samtale-AI

Det f√∏rste eksempel demonstrerer det grundl√¶ggende, men det er kun en enkelt udveksling ‚Äì du stiller et sp√∏rgsm√•l, f√•r et svar, og det var det. I virkelige applikationer vil du have, at din AI husker, hvad I har talt om, ligesom Watson og Holmes byggede deres efterforskende samtaler over tid.

Her bliver LangChain s√¶rligt nyttigt. Det tilbyder forskellige beskedtyper, der hj√¶lper med at strukturere samtaler og lader dig give din AI en personlighed. Du vil bygge chatoplevelser, der bevarer kontekst og karakter.

### Forst√•else af beskedtyper

T√¶nk p√• disse beskedtyper som forskellige "hatte", som deltagerne b√¶rer i en samtale. LangChain bruger forskellige beskedklasser til at holde styr p√•, hvem der siger hvad:

| Beskedtype | Form√•l | Eksempel p√• brug |
|--------------|---------|------------------|
| `SystemMessage` | Definerer AI-personlighed og adf√¶rd | "Du er en hj√¶lpsom kodeassistent" |
| `HumanMessage` | Repr√¶senterer brugerinput | "Forklar, hvordan funktioner virker" |
| `AIMessage` | Gemmer AI's svar | Tidligere AI-svar i samtalen |

### Opret din f√∏rste samtale

Lad os skabe en samtale, hvor vores AI antager en specifik rolle. Vi lader den indtage rollen som kaptajn Picard ‚Äì en karakter kendt for sin diplomatiske visdom og lederskab:

```python
messages = [
    SystemMessage(content="You are Captain Picard of the Starship Enterprise"),
    HumanMessage(content="Tell me about you"),
]
```

**Nedbrydning af denne samtaleops√¶tning:**
- **Etablerer** AI'ens rolle og personlighed gennem `SystemMessage`
- **Giver** den f√∏rste brugerforesp√∏rgsel via `HumanMessage`
- **Skaber** et grundlag for en samtale med flere omgange

Den fulde kode til dette eksempel ser s√•dan ud:

```python
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_openai import ChatOpenAI
import os

llm = ChatOpenAI(
    api_key=os.environ["GITHUB_TOKEN"],
    base_url="https://models.github.ai/inference",
    model="openai/gpt-4o-mini",
)

messages = [
    SystemMessage(content="You are Captain Picard of the Starship Enterprise"),
    HumanMessage(content="Tell me about you"),
]


# virker
response  = llm.invoke(messages)
print(response.content)
```

Du b√∏r se et resultat svarende til:

```text
I am Captain Jean-Luc Picard, the commanding officer of the USS Enterprise (NCC-1701-D), a starship in the United Federation of Planets. My primary mission is to explore new worlds, seek out new life and new civilizations, and boldly go where no one has gone before. 

I believe in the importance of diplomacy, reason, and the pursuit of knowledge. My crew is diverse and skilled, and we often face challenges that test our resolve, ethics, and ingenuity. Throughout my career, I have encountered numerous species, grappled with complex moral dilemmas, and have consistently sought peaceful solutions to conflicts.

I hold the ideals of the Federation close to my heart, believing in the importance of cooperation, understanding, and respect for all sentient beings. My experiences have shaped my leadership style, and I strive to be a thoughtful and just captain. How may I assist you further?
```

For at bevare kontinuiteten i samtalen (i stedet for at nulstille konteksten hver gang) skal du forts√¶tte med at tilf√∏je svar til din beskedliste. Ligesom mundtlige traditioner, der bevarede historier p√• tv√¶rs af generationer, bygger denne tilgang varig hukommelse:

```python
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_openai import ChatOpenAI
import os

llm = ChatOpenAI(
    api_key=os.environ["GITHUB_TOKEN"],
    base_url="https://models.github.ai/inference",
    model="openai/gpt-4o-mini",
)

messages = [
    SystemMessage(content="You are Captain Picard of the Starship Enterprise"),
    HumanMessage(content="Tell me about you"),
]


# virker
response  = llm.invoke(messages)

print(response.content)

print("---- Next ----")

messages.append(response)
messages.append(HumanMessage(content="Now that I know about you, I'm Chris, can I be in your crew?"))

response  = llm.invoke(messages)

print(response.content)

```

Ret smart, ikke? Hvad der sker her, er, at vi kalder LLM to gange ‚Äì f√∏rst med bare vores oprindelige to beskeder, men s√• igen med hele samtalehistorikken. Det er som om, AI‚Äôen virkelig f√∏lger med i vores chat!

N√•r du k√∏rer denne kode, vil du f√• et andet svar, der lyder noget i retning af:

```text
Welcome aboard, Chris! It's always a pleasure to meet those who share a passion for exploration and discovery. While I cannot formally offer you a position on the Enterprise right now, I encourage you to pursue your aspirations. We are always in need of talented individuals with diverse skills and backgrounds. 

If you are interested in space exploration, consider education and training in the sciences, engineering, or diplomacy. The values of curiosity, resilience, and teamwork are crucial in Starfleet. Should you ever find yourself on a starship, remember to uphold the principles of the Federation: peace, understanding, and respect for all beings. Your journey can lead you to remarkable adventures, whether in the stars or on the ground. Engage!
```

```mermaid
sequenceDiagram
    participant User
    participant App
    participant LangChain
    participant AI
    
    User->>App: "Fort√¶l mig om dig"
    App->>LangChain: [SystemMessage, HumanMessage]
    LangChain->>AI: Formateret samtale
    AI->>LangChain: Captain Picard svar
    LangChain->>App: AIMessage objekt
    App->>User: Vis svar
    
    Note over App: Tilf√∏j AIMessage til samtalen
    
    User->>App: "Kan jeg slutte mig til dit mandskab?"
    App->>LangChain: [SystemMessage, HumanMessage, AIMessage, HumanMessage]
    LangChain->>AI: Fuldt samtalekontekst
    AI->>LangChain: Kontekstuel respons
    LangChain->>App: Nyt AIMessage
    App->>User: Vis kontekstuel respons
```
Det tager jeg som et m√•ske ;)

## Streaming svar

Har du nogensinde lagt m√¶rke til, hvordan ChatGPT ser ud til at "skrive" sine svar i realtid? Det er streaming i aktion. Som at se en dygtig kalligraf arbejde ‚Äì hvor tegnene dukker op streg for streg i stedet for at materialisere sig √∏jeblikkeligt ‚Äì g√∏r streaming interaktionen mere naturlig og giver √∏jeblikkelig feedback.

### Implementering af streaming med LangChain

```python
from langchain_openai import ChatOpenAI
import os

llm = ChatOpenAI(
    api_key=os.environ["GITHUB_TOKEN"],
    base_url="https://models.github.ai/inference",
    model="openai/gpt-4o-mini",
    streaming=True
)

# Stream svaret
for chunk in llm.stream("Write a short story about a robot learning to code"):
    print(chunk.content, end="", flush=True)
```

**Hvorfor streaming er fedt:**
- **Viser** indhold, mens det bliver skabt ‚Äì ikke mere akavet venten!
- **F√•r** brugerne til at f√∏le, at der rent faktisk sker noget
- **F√∏les** hurtigere, selv om det teknisk set ikke er det
- **Lader** brugerne begynde at l√¶se, mens AI‚Äôen stadig "t√¶nker"

> üí° **Brugeroplevelsestip**: Streaming g√∏r sig virkelig godt, n√•r du arbejder med l√¶ngere svar som kodeforklaringer, kreativ skrivning eller detaljerede vejledninger. Dine brugere vil elske at se fremskridtene i stedet for at stirre p√• en tom sk√¶rm!

### üéØ P√¶dagogisk gennemgang: Fordele ved rammev√¶rksabstraktion

**Pause og refleksion**: Du har lige oplevet styrken ved AI-rammev√¶rksabstraktioner. Sammenlign, hvad du har l√¶rt, med r√• API-kald fra tidligere lektioner.

**Hurtig selvevaluering**:
- Kan du forklare, hvordan LangChain forenkler samtaleh√•ndtering sammenlignet med manuelt beskedsoverblik?
- Hvad er forskellen p√• `invoke()`- og `stream()`-metoderne, og hvorn√•r vil du bruge dem hver is√¶r?
- Hvordan forbedrer rammev√¶rkets beskedtyper kodens organisering?

**Virkelighedsforbindelse**: De abstraheringsm√∏nstre, du har l√¶rt (beskedtyper, streaminggr√¶nseflader, samtaleminde), bruges i alle st√∏rre AI-applikationer ‚Äì fra ChatGPT‚Äôs interface til GitHub Copilot‚Äôs kodeassistance. Du mestrer de samme arkitektoniske m√∏nstre, som professionelle AI-udviklingsteams benytter.

**Udfordrende sp√∏rgsm√•l**: Hvordan ville du designe en rammev√¶rksabstraktion for h√•ndtering af forskellige AI-modeludbydere (OpenAI, Anthropic, Google) gennem en enkelt gr√¶nseflade? Overvej fordele og ulemper.

## Prompt-templates

Prompt-templates fungerer som de retoriske strukturer, der bruges i klassisk oratorik ‚Äì t√¶nk p√•, hvordan Cicero ville tilpasse sine talepatterner til forskellige publikum, mens han bevarer den samme overbevisende ramme. De lader dig skabe genanvendelige prompts, hvor du kan udskifte forskellige informationer uden at skulle omskrive alt fra bunden. N√•r du har sat templaten op, skal du bare udfylde variablerne med de v√¶rdier, du har brug for.

### Oprettelse af genanvendelige prompts

```python
from langchain_core.prompts import ChatPromptTemplate

# Definer en skabelon til kodeforklaringer
template = ChatPromptTemplate.from_messages([
    ("system", "You are an expert programming instructor. Explain concepts clearly with examples."),
    ("human", "Explain {concept} in {language} with a practical example for {skill_level} developers")
])

# Brug skabelonen med forskellige v√¶rdier
questions = [
    {"concept": "functions", "language": "JavaScript", "skill_level": "beginner"},
    {"concept": "classes", "language": "Python", "skill_level": "intermediate"},
    {"concept": "async/await", "language": "JavaScript", "skill_level": "advanced"}
]

for question in questions:
    prompt = template.format_messages(**question)
    response = llm.invoke(prompt)
    print(f"Topic: {question['concept']}\n{response.content}\n---\n")
```

**Hvorfor du vil elske at bruge templates:**
- **Holder** dine prompts konsistente p√• tv√¶rs af hele din app
- **Ingen mere** rodet strengsammenk√¶dning ‚Äì bare rene, simple variabler
- **Din AI** opf√∏rer sig forudsigeligt, fordi strukturen er ensartet
- **Opdateringer** er lette ‚Äì √¶ndr templaten √©n gang, og det er fikset overalt

## Struktureret output

Er du nogensinde blevet frustreret over at skulle parse AI-svar, der kommer tilbage som ustruktureret tekst? Struktureret output er som at l√¶re din AI den systematiske tilgang, som Linnaeus brugte til biologisk klassifikation ‚Äì organiseret, forudsigeligt og let at arbejde med. Du kan bede om JSON, specifikke datastrukturer eller et hvilket som helst format, du har brug for.

### Definering af output-skemaer

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser
from pydantic import BaseModel, Field

class CodeReview(BaseModel):
    score: int = Field(description="Code quality score from 1-10")
    strengths: list[str] = Field(description="List of code strengths")
    improvements: list[str] = Field(description="List of suggested improvements")
    overall_feedback: str = Field(description="Summary feedback")

# Ops√¶t parseren
parser = JsonOutputParser(pydantic_object=CodeReview)

# Opret prompt med formateringsinstruktioner
prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a code reviewer. {format_instructions}"),
    ("human", "Review this code: {code}")
])

# Format√©r prompten med instruktioner
chain = prompt | llm | parser

# Hent struktureret svar
code_sample = """
def calculate_average(numbers):
    return sum(numbers) / len(numbers)
"""

result = chain.invoke({
    "code": code_sample,
    "format_instructions": parser.get_format_instructions()
})

print(f"Score: {result['score']}")
print(f"Strengths: {', '.join(result['strengths'])}")
```

**Hvorfor struktureret output er en game-changer:**
- **Ikke mere** g√¶tteri om, hvilket format du f√•r ‚Äì det er konsekvent hver gang
- **Kan** kobles direkte til dine databaser og API‚Äôer uden ekstra arbejde
- **Fanger** underlige AI-svar, f√∏r de √∏del√¶gger din app
- **G√∏r** din kode renere, fordi du ved pr√¶cis, hvad du arbejder med

## V√¶rkt√∏jskald

Nu n√•r vi til en af de mest kraftfulde funktioner: v√¶rkt√∏jer. Det er s√•dan, du giver din AI praktiske evner ud over samtale. Ligesom middelalderlige laug udviklede specialiserede v√¶rkt√∏jer til specifikke h√•ndv√¶rk, kan du udstyre din AI med fokuserede instrumenter. Du beskriver, hvilke v√¶rkt√∏jer der er tilg√¶ngelige, og n√•r nogen anmoder om noget, der matcher, kan din AI handle.

### Brug af Python

Lad os tilf√∏je nogle v√¶rkt√∏jer som f√∏lger:

```python
from typing_extensions import Annotated, TypedDict

class add(TypedDict):
    """Add two integers."""

    # Annotationer skal have typen og kan valgfrit inkludere en standardv√¶rdi og beskrivelse (i den r√¶kkef√∏lge).
    a: Annotated[int, ..., "First integer"]
    b: Annotated[int, ..., "Second integer"]

tools = [add]

functions = {
    "add": lambda a, b: a + b
}
```

S√• hvad sker der her? Vi opretter en k√∏replan for et v√¶rkt√∏j kaldet `add`. Ved at arve fra `TypedDict` og bruge de smarte `Annotated`-typer for `a` og `b` giver vi LLM et klart billede af, hvad dette v√¶rkt√∏j g√∏r, og hvad det har brug for. Ordbogen `functions` er som vores v√¶rkt√∏jskasse ‚Äì den fort√¶ller vores kode pr√¶cis, hvad den skal g√∏re, n√•r AI‚Äôen beslutter at bruge et bestemt v√¶rkt√∏j.

Lad os se, hvordan vi kalder LLM med dette v√¶rkt√∏j n√¶ste gang:

```python
llm = ChatOpenAI(
    api_key=os.environ["GITHUB_TOKEN"],
    base_url="https://models.github.ai/inference",
    model="openai/gpt-4o-mini",
)

llm_with_tools = llm.bind_tools(tools)
```

Her kalder vi `bind_tools` med vores `tools`-array, og dermed har LLM `llm_with_tools` nu kendskab til dette v√¶rkt√∏j.

For at bruge denne nye LLM kan vi skrive f√∏lgende kode:

```python
query = "What is 3 + 12?"

res = llm_with_tools.invoke(query)
if(res.tool_calls):
    for tool in res.tool_calls:
        print("TOOL CALL: ", functions[tool["name"]](../../../10-ai-framework-project/**tool["args"]))
print("CONTENT: ",res.content)
```

Nu hvor vi kalder `invoke` p√• denne nye LLM, der har v√¶rkt√∏jer, bliver egenskaben `tool_calls` muligvis udfyldt. Hvis det sker, har ethvert identificeret v√¶rkt√∏j en `name`- og `args`-egenskab, som identificerer, hvilket v√¶rkt√∏j der skal kaldes og med hvilke argumenter. Den fulde kode ser s√•ledes ud:

```python
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_openai import ChatOpenAI
import os
from typing_extensions import Annotated, TypedDict

class add(TypedDict):
    """Add two integers."""

    # Anm√¶rkninger skal have typen og kan valgfrit inkludere en standardv√¶rdi og beskrivelse (i den r√¶kkef√∏lge).
    a: Annotated[int, ..., "First integer"]
    b: Annotated[int, ..., "Second integer"]

tools = [add]

functions = {
    "add": lambda a, b: a + b
}

llm = ChatOpenAI(
    api_key=os.environ["GITHUB_TOKEN"],
    base_url="https://models.github.ai/inference",
    model="openai/gpt-4o-mini",
)

llm_with_tools = llm.bind_tools(tools)

query = "What is 3 + 12?"

res = llm_with_tools.invoke(query)
if(res.tool_calls):
    for tool in res.tool_calls:
        print("TOOL CALL: ", functions[tool["name"]](../../../10-ai-framework-project/**tool["args"]))
print("CONTENT: ",res.content)
```

N√•r du k√∏rer denne kode, skulle du se output lignende:

```text
TOOL CALL:  15
CONTENT: 
```

AI‚Äôen unders√∏gte "What is 3 + 12" og genkendte dette som en opgave for v√¶rkt√∏jet `add`. Ligesom en dygtig bibliotekar ved, hvilket referencev√¶rk man skal konsultere afh√¶ngigt af sp√∏rgsm√•lets type, traf den denne afg√∏relse ud fra v√¶rkt√∏jets navn, beskrivelse og feltspecifikationer. Resultatet p√• 15 kommer fra vores `functions`-ordbog, der udf√∏rer v√¶rkt√∏jet:

```python
print("TOOL CALL: ", functions[tool["name"]](../../../10-ai-framework-project/**tool["args"]))
```

### Et mere interessant v√¶rkt√∏j, der kalder en web-API
At l√¶gge tal sammen demonstrerer konceptet, men rigtige v√¶rkt√∏jer udf√∏rer typisk mere komplekse operationer, som at kalde web-API'er. Lad os udvide vores eksempel, s√• AI henter indhold fra internettet - p√• samme m√•de som telegrafister engang forbandt fjerne steder:

```python
class joke(TypedDict):
    """Tell a joke."""

    # Annotationer skal have typen og kan valgfrit indeholde en standardv√¶rdi og beskrivelse (i den r√¶kkef√∏lge).
    category: Annotated[str, ..., "The joke category"]

def get_joke(category: str) -> str:
    response = requests.get(f"https://api.chucknorris.io/jokes/random?category={category}", headers={"Accept": "application/json"})
    if response.status_code == 200:
        return response.json().get("value", f"Here's a {category} joke!")
    return f"Here's a {category} joke!"

functions = {
    "add": lambda a, b: a + b,
    "joke": lambda category: get_joke(category)
}

query = "Tell me a joke about animals"

# resten af koden er den samme
```

Hvis du nu k√∏rer denne kode, vil du f√• et svar, der siger noget i retning af:

```text
TOOL CALL:  Chuck Norris once rode a nine foot grizzly bear through an automatic car wash, instead of taking a shower.
CONTENT:  
```

```mermaid
flowchart TD
    A[Brugerforesp√∏rgsel: "Fort√¶l mig en vittighed om dyr"] --> B[LangChain Analyse]
    B --> C{V√¶rkt√∏j tilg√¶ngeligt?}
    C -->|Ja| D[V√¶lg vittighedsv√¶rkt√∏j]
    C -->|Nej| E[Generer direkte svar]
    
    D --> F[Udtr√¶k parametre]
    F --> G[Ring til joke(kategori="dyr")]
    G --> H[API-anmodning til chucknorris.io]
    H --> I[Returner vittighedsindhold]
    I --> J[Vis for bruger]
    
    E --> K[AI-genereret svar]
    K --> J
    
    subgraph "V√¶rkt√∏jsdefinitionslag"
        L[TypedDict Skema]
        M[Funktionsimplementering]
        N[Parameter validering]
    end
    
    D --> L
    F --> N
    G --> M
```
Her er koden i sin helhed:

```python
from langchain_openai import ChatOpenAI
import requests
import os
from typing_extensions import Annotated, TypedDict

class add(TypedDict):
    """Add two integers."""

    # Anm√¶rkninger skal have typen og kan valgfrit inkludere en standardv√¶rdi og beskrivelse (i den r√¶kkef√∏lge).
    a: Annotated[int, ..., "First integer"]
    b: Annotated[int, ..., "Second integer"]

class joke(TypedDict):
    """Tell a joke."""

    # Anm√¶rkninger skal have typen og kan valgfrit inkludere en standardv√¶rdi og beskrivelse (i den r√¶kkef√∏lge).
    category: Annotated[str, ..., "The joke category"]

tools = [add, joke]

def get_joke(category: str) -> str:
    response = requests.get(f"https://api.chucknorris.io/jokes/random?category={category}", headers={"Accept": "application/json"})
    if response.status_code == 200:
        return response.json().get("value", f"Here's a {category} joke!")
    return f"Here's a {category} joke!"

functions = {
    "add": lambda a, b: a + b,
    "joke": lambda category: get_joke(category)
}

llm = ChatOpenAI(
    api_key=os.environ["GITHUB_TOKEN"],
    base_url="https://models.github.ai/inference",
    model="openai/gpt-4o-mini",
)

llm_with_tools = llm.bind_tools(tools)

query = "Tell me a joke about animals"

res = llm_with_tools.invoke(query)
if(res.tool_calls):
    for tool in res.tool_calls:
        # print("V√ÜRKT√òJSKALD: ", tool)
        print("TOOL CALL: ", functions[tool["name"]](../../../10-ai-framework-project/**tool["args"]))
print("CONTENT: ",res.content)
```

## Embeddings og dokumentbehandling

Embeddings repr√¶senterer en af de mest elegante l√∏sninger i moderne AI. Forestil dig, at du kan tage et hvilket som helst tekststykke og konvertere det til numeriske koordinater, der fanger dets betydning. Det er pr√¶cis, hvad embeddings g√∏r - de omdanner tekst til punkter i et flerdimensionelt rum, hvor lignende begreber samles. Det er som at have et koordinatsystem for id√©er, der minder om, hvordan Mendeleev organiserede det periodiske system efter atomare egenskaber.

### Oprettelse og brug af embeddings

```python
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import CharacterTextSplitter

# Initialiser indlejringer
embeddings = OpenAIEmbeddings(
    api_key=os.environ["GITHUB_TOKEN"],
    base_url="https://models.github.ai/inference",
    model="text-embedding-3-small"
)

# Indl√¶s og del dokumenter
loader = TextLoader("documentation.txt")
documents = loader.load()

text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
texts = text_splitter.split_documents(documents)

# Opret vektorlager
vectorstore = FAISS.from_documents(texts, embeddings)

# Udf√∏r s√∏gning efter lighed
query = "How do I handle user authentication?"
similar_docs = vectorstore.similarity_search(query, k=3)

for doc in similar_docs:
    print(f"Relevant content: {doc.page_content[:200]}...")
```

### Dokumentindl√¶sere til forskellige formater

```python
from langchain_community.document_loaders import (
    PyPDFLoader,
    CSVLoader,
    JSONLoader,
    WebBaseLoader
)

# Indl√¶s forskellige dokumenttyper
pdf_loader = PyPDFLoader("manual.pdf")
csv_loader = CSVLoader("data.csv")
json_loader = JSONLoader("config.json")
web_loader = WebBaseLoader("https://example.com/docs")

# Behandl alle dokumenter
all_documents = []
for loader in [pdf_loader, csv_loader, json_loader, web_loader]:
    docs = loader.load()
    all_documents.extend(docs)
```

**Det kan du g√∏re med embeddings:**
- **Bygge** s√∏gning, der faktisk forst√•r, hvad du mener, ikke bare n√∏gleordsmatch
- **Skabe** AI, der kan besvare sp√∏rgsm√•l om dine dokumenter
- **Lave** anbefalingssystemer, som foresl√•r virkelig relevant indhold
- **Automatisk** organisere og kategorisere dit indhold

```mermaid
flowchart LR
    A[Dokumenter] --> B[Tekstopdeler]
    B --> C[Opret Indlejringer]
    C --> D[Vektorlager]
    
    E[Brugerforesp√∏rgsel] --> F[Foresp√∏rgselsindlejring]
    F --> G[Lighedss√∏gning]
    G --> D
    D --> H[Relevante Dokumenter]
    H --> I[AI Svar]
    
    subgraph "Vektorrum"
        J[Dokument A: [0.1, 0.8, 0.3...]]
        K[Dokument B: [0.2, 0.7, 0.4...]]
        L[Foresp√∏rgsel: [0.15, 0.75, 0.35...]]
    end
    
    C --> J
    C --> K
    F --> L
    G --> J
    G --> K
```
## Opbygning af en komplet AI-applikation

Nu integrerer vi alt, hvad du har l√¶rt, i en omfattende applikation - en kodeassistent, der kan svare p√• sp√∏rgsm√•l, bruge v√¶rkt√∏jer og bevare samtaleminder. Ligesom trykpressen kombinerede eksisterende teknologier (flytbar type, bl√¶k, papir og tryk) til noget banebrydende, vil vi kombinere vores AI-komponenter til noget praktisk og brugbart.

### Komplekt applikationseksempel

```python
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage
from langchain_community.vectorstores import FAISS
from typing_extensions import Annotated, TypedDict
import os
import requests

class CodingAssistant:
    def __init__(self):
        self.llm = ChatOpenAI(
            api_key=os.environ["GITHUB_TOKEN"],
            base_url="https://models.github.ai/inference",
            model="openai/gpt-4o-mini"
        )
        
        self.conversation_history = [
            SystemMessage(content="""You are an expert coding assistant. 
            Help users learn programming concepts, debug code, and write better software.
            Use tools when needed and maintain a helpful, encouraging tone.""")
        ]
        
        # Definer v√¶rkt√∏jer
        self.setup_tools()
    
    def setup_tools(self):
        class web_search(TypedDict):
            """Search for programming documentation or examples."""
            query: Annotated[str, "Search query for programming help"]
        
        class code_formatter(TypedDict):
            """Format and validate code snippets."""
            code: Annotated[str, "Code to format"]
            language: Annotated[str, "Programming language"]
        
        self.tools = [web_search, code_formatter]
        self.llm_with_tools = self.llm.bind_tools(self.tools)
    
    def chat(self, user_input: str):
        # Tilf√∏j brugermeddelelse til samtale
        self.conversation_history.append(HumanMessage(content=user_input))
        
        # F√• AI-svar
        response = self.llm_with_tools.invoke(self.conversation_history)
        
        # H√•ndter kald til v√¶rkt√∏jer, hvis nogen
        if response.tool_calls:
            for tool_call in response.tool_calls:
                tool_result = self.execute_tool(tool_call)
                print(f"üîß Tool used: {tool_call['name']}")
                print(f"üìä Result: {tool_result}")
        
        # Tilf√∏j AI-svar til samtale
        self.conversation_history.append(response)
        
        return response.content
    
    def execute_tool(self, tool_call):
        tool_name = tool_call['name']
        args = tool_call['args']
        
        if tool_name == 'web_search':
            return f"Found documentation for: {args['query']}"
        elif tool_name == 'code_formatter':
            return f"Formatted {args['language']} code: {args['code'][:50]}..."
        
        return "Tool execution completed"

# Brugs eksempel
assistant = CodingAssistant()

print("ü§ñ Coding Assistant Ready! Type 'quit' to exit.\n")

while True:
    user_input = input("You: ")
    if user_input.lower() == 'quit':
        break
    
    response = assistant.chat(user_input)
    print(f"ü§ñ Assistant: {response}\n")
```

**Applikationsarkitektur:**

```mermaid
graph TD
    A[Brugerinput] --> B[Kodeassistent]
    B --> C[Samtaleminne]
    B --> D[V√¶rkt√∏jsregistrering]
    B --> E[LLM-behandling]
    
    D --> F[Webs√∏gningsv√¶rkt√∏j]
    D --> G[Kodeformateringsv√¶rkt√∏j]
    
    E --> H[Svar generering]
    F --> H
    G --> H
    
    H --> I[Brugergr√¶nseflade]
    H --> C
```
**N√∏glefunktioner vi har implementeret:**
- **Husker** hele din samtale for kontekstuel kontinuitet
- **Udf√∏rer handlinger** via v√¶rkt√∏jskald, ikke bare samtale
- **F√∏lger** forudsigelige interaktionsm√∏nstre
- **H√•ndterer** fejlh√•ndtering og komplekse arbejdsgange automatisk

### üéØ P√¶dagogisk status: Produktionsarkitektur for AI

**Forst√•else af arkitektur**: Du har bygget en komplet AI-applikation, der kombinerer samtalestyring, v√¶rkt√∏jskald og strukturerede arbejdsgange. Dette repr√¶senterer AI-applikationsudvikling p√• produktionsniveau.

**N√∏glebegreber, der er mestret**:
- **Klassesbaseret arkitektur**: Organiseret, vedligeholdelig AI-applikationsstruktur
- **V√¶rkt√∏jsintegration**: Tilpasset funktionalitet ud over samtale
- **Hukommelsesstyring**: Vedvarende samtalekontekst
- **Fejlh√•ndtering**: Robust applikationsadf√¶rd

**Branchens forbindelse**: De arkitekturm√∏nstre, du har implementeret (samtaleklasser, v√¶rkt√∏jssystemer, hukommelsesstyring), er de samme m√∏nstre, der anvendes i virksomheders AI-applikationer som Slack‚Äôs AI-assistent, GitHub Copilot og Microsoft Copilot. Du bygger med professionel arkitekturt√¶nkning.

**Refleksionssp√∏rgsm√•l**: Hvordan ville du udvide denne applikation til at h√•ndtere flere brugere, vedvarende lagring eller integration med eksterne databaser? Overvej udfordringer med skalerbarhed og tilstandsstyring.

## Opgave: Byg din egen AI-drevne studieassistent

**M√•l**: Skab en AI-applikation, der hj√¶lper studerende med at l√¶re programmeringskoncepter ved at give forklaringer, kodeeksempler og interaktive quizzer.

### Krav

**Kernefunktioner (p√•kr√¶vet):**
1. **Samtaleinterface**: Implementer et chatsystem, der bevarer kontekst p√• tv√¶rs af flere sp√∏rgsm√•l
2. **Uddannelsesv√¶rkt√∏jer**: Lav mindst to v√¶rkt√∏jer, der hj√¶lper med l√¶ring:
   - V√¶rkt√∏j til kodeforklaring
   - Quizgenerator til koncepter
3. **Personlig l√¶ring**: Brug systembeskeder til at tilpasse svar til forskellige f√¶rdighedsniveauer
4. **Svarformatering**: Implementer struktureret output til quizsp√∏rgsm√•l

### Implementeringstrin

**Trin 1: Ops√¶t dit milj√∏**
```bash
pip install langchain langchain-openai
```

**Trin 2: Grundl√¶ggende chatfunktionalitet**
- Opret en `StudyAssistant`-klasse
- Implementer samtaleminde
- Tilf√∏j personlighedskonfiguration til uddannelsesst√∏tte

**Trin 3: Tilf√∏j uddannelsesv√¶rkt√∏jer**
- **Kodeforklarer**: Opdeler kode i forst√•elige dele
- **Quizgenerator**: Opretter sp√∏rgsm√•l om programmeringskoncepter
- **Fremskridtssporer**: Holder styr p√• d√¶kkede emner

**Trin 4: Forbedrede funktioner (valgfrit)**
- Implementer streaming-svar for bedre brugeroplevelse
- Tilf√∏j dokumentindl√¶sning for at inkorporere kursusmaterialer
- Opret embeddings til lighedsbaseret indholdshentning

### Evalueringskriterier

| Funktion | Fremragende (4) | God (3) | Tilfredsstillende (2) | Skal forbedres (1) |
|---------|---------------|----------|------------------|----------------|
| **Samtaleflow** | Naturlige, kontekstf√∏lsomme svar | God kontekstbevarelse | Grundl√¶ggende samtale | Ingen hukommelse mellem udvekslinger |
| **V√¶rkt√∏jsintegration** | Flere nyttige v√¶rkt√∏jer fungerer problemfrit | 2+ v√¶rkt√∏jer implementeret korrekt | 1-2 grundl√¶ggende v√¶rkt√∏jer | V√¶rkt√∏jer ikke funktionelle |
| **Kodekvalitet** | Ren, veldokumenteret, fejlh√•ndtering | God struktur, noget dokumentation | Grundl√¶ggende funktionalitet virker | D√•rlig struktur, ingen fejlh√•ndtering |
| **Uddannelsesm√¶ssig v√¶rdi** | Virkelig hj√¶lpsom til l√¶ring, adaptiv | God l√¶ringsst√∏tte | Grundl√¶ggende forklaringer | Begr√¶nset p√¶dagogisk v√¶rdi |

### Eksempelkodestruktur

```python
class StudyAssistant:
    def __init__(self, skill_level="beginner"):
        # Initialiser LLM, v√¶rkt√∏jer og samtalehukommelse
        pass
    
    def explain_code(self, code, language):
        # V√¶rkt√∏j: Forklar hvordan kode fungerer
        pass
    
    def generate_quiz(self, topic, difficulty):
        # V√¶rkt√∏j: Opret √∏velsessp√∏rgsm√•l
        pass
    
    def chat(self, user_input):
        # Hovedsamtalegr√¶nseflade
        pass

# Eksempel p√• anvendelse
assistant = StudyAssistant(skill_level="intermediate")
response = assistant.chat("Explain how Python functions work")
```

**Bonusudfordringer:**
- Tilf√∏j stemmeinput/-output-muligheder
- Implementer et webinterface med Streamlit eller Flask
- Opret en vidensbase fra kursusmaterialer ved hj√¶lp af embeddings
- Tilf√∏j fremskridtssporing og personlige l√¶ringsforl√∏b

## üìà Din tidslinje for mestring af AI-frameworkudvikling

```mermaid
timeline
    title Produktions AI Framework Udviklingsrejse
    
    section Framework Grundlag
        Forst√• Abstraktioner
            : Mestre framework vs API beslutninger
            : L√¶r LangChain kernekoncepter
            : Implementer beskedtypers systemer
        
        Grundl√¶ggende Integration
            : Forbind til AI-udbydere
            : H√•ndter autentificering
            : Administrer konfiguration
    
    section Samtalesystemer
        Hukommelsesstyring
            : Byg samtalehistorik
            : Implementer kontekstspor
            : H√•ndter session vedholdenhed
        
        Avancerede Interaktioner
            : Mestre streaming svar
            : Opret promptskabeloner
            : Implementer struktureret output
    
    section V√¶rkt√∏jsintegration
        Egenudviklet V√¶rkt√∏j
            : Design v√¶rkt√∏jsskemaer
            : Implementer funktionsopkald
            : H√•ndter eksterne API'er
        
        Arbejdsgangsautomatisering
            : K√¶d flere v√¶rkt√∏jer sammen
            : Opret beslutningstr√¶er
            : Byg agentadf√¶rd
    
    section Produktionsapplikationer
        Kompleks Systemarkitektur
            : Kombiner alle framework-funktioner
            : Implementer fejlkontroller
            : Opret vedligeholdelsesvenlig kode
        
        Virksomhedsklarhed
            : H√•ndter skalerbarhedsproblemer
            : Implementer overv√•gning
            : Byg udrulningsstrategier
```
**üéì Eksamenstrin**: Du har med succes mestret AI-frameworkudvikling ved brug af de samme v√¶rkt√∏jer og m√∏nstre, der driver moderne AI-applikationer. Disse f√¶rdigheder repr√¶senterer frontlinjen i AI-applikationsudvikling og forbereder dig p√• at bygge intelligente systemer p√• virksomhedsplan.

**üîÑ N√¶ste niveau kapaciteter**:
- Klar til at udforske avancerede AI-arkitekturer (agenter, multi-agent systemer)
- Forberedt til at bygge RAG-systemer med vektordatabaser
- Udstyret til at skabe multimodale AI-applikationer
- Fundament lagt til skalerbarhed og optimering af AI-applikationer

## Resum√©

üéâ Du har nu mestret grundl√¶ggende AI-frameworkudvikling og l√¶rt at bygge sofistikerede AI-applikationer med LangChain. Ligesom at fuldf√∏re en omfattende l√¶replads har du opn√•et et stort s√¶t f√¶rdigheder. Lad os gennemg√•, hvad du har opn√•et.

### Hvad du har l√¶rt

**Kerneframework-koncepter:**
- **Framework-fordele**: Forst√• hvorn√•r man v√¶lger frameworks frem for direkte API-kald
- **LangChain basics**: Ops√¶tning og konfiguration af AI-modelforbindelser
- **Beskedtyper**: Brug af `SystemMessage`, `HumanMessage` og `AIMessage` til strukturerede samtaler

**Avancerede funktioner:**
- **V√¶rkt√∏jskald**: Oprettelse og integration af tilpassede v√¶rkt√∏jer for forbedret AI-funktionalitet
- **Samtaleminde**: Vedligeholdelse af kontekst over flere samtaletrin
- **Streaming-svar**: Implementering af realtidsresponslevering
- **Promptskabeloner**: Opbygning af genanvendelige, dynamiske prompts
- **Struktureret output**: Sikring af konsistente, parsebare AI-svar
- **Embeddings**: Opbygning af semantisk s√∏gning og dokumentbehandlingsfunktioner

**Praktiske anvendelser:**
- **Opbygning af komplette apps**: Kombination af flere funktioner til produktionsklare applikationer
- **Fejlh√•ndtering**: Implementering af robust fejlstyring og validering
- **V√¶rkt√∏jsintegration**: Oprettelse af specialv√¶rkt√∏jer, der udvider AI‚Äôs muligheder

### Vigtige pointer

> üéØ **Husk**: AI-frameworks som LangChain er i bund og grund dine kompleksitetsskjulende, feature-rige bedste venner. De er perfekte, n√•r du har brug for samtaleminde, v√¶rkt√∏jskald eller vil arbejde med flere AI-modeller uden at miste forstanden.

**Beslutningsframework for AI-integration:**

```mermaid
flowchart TD
    A[Behov for AI-integration] --> B{Simpel enkelt foresp√∏rgsel?}
    B -->|Ja| C[Direkte API-kald]
    B -->|Nej| D{Behov for samtaleminde?}
    D -->|Nej| E[SDK-integration]
    D -->|Ja| F{Behov for v√¶rkt√∏jer eller komplekse funktioner?}
    F -->|Nej| G[Rammev√¶rk med grundl√¶ggende ops√¶tning]
    F -->|Ja| H[Fuldt rammev√¶rksimplementering]
    
    C --> I[HTTP-anmodninger, minimale afh√¶ngigheder]
    E --> J[Udbyder SDK, model-specifik]
    G --> K[LangChain grundl√¶ggende chat]
    H --> L[LangChain med v√¶rkt√∏jer, hukommelse, agenter]
```
### Hvor g√•r du hen herfra?

**Begynd at bygge nu:**
- Tag disse koncepter og byg noget, der begejstrer DIG!
- Eksperiment√©r med forskellige AI-modeller via LangChain - det er som at have en legeplads med AI-modeller
- Lav v√¶rkt√∏jer, der l√∏ser reelle problemer, du m√∏der i dit arbejde eller projekter

**Klar til n√¶ste niveau?**
- **AI-agenter**: Byg AI-systemer, der faktisk kan planl√¶gge og udf√∏re komplekse opgaver selvst√¶ndigt
- **RAG (Retrieval-Augmented Generation)**: Kombin√©r AI med dine egne vidensbaser til superkraftfulde applikationer
- **Multimodal AI**: Arbejd med tekst, billeder og lyd samlet - mulighederne er uendelige!
- **Produktionsdrift**: L√¶r, hvordan du skalerer dine AI-apps og overv√•ger dem i den virkelige verden

**Deltag i f√¶llesskabet:**
- LangChain-f√¶llesskabet er fremragende til at holde sig opdateret og l√¶re bedst praksis
- GitHub Models giver dig adgang til banebrydende AI-funktioner - perfekt til eksperimenter
- √òv dig med forskellige brugsscenarier - hvert projekt l√¶rer dig noget nyt

Du har nu viden til at bygge intelligente, samtalebaserede applikationer, der kan hj√¶lpe folk med at l√∏se rigtige problemer. Ligesom ren√¶ssancens h√•ndv√¶rkere, der forenede kunstnerisk vision med teknisk dygtighed, kan du nu fusionere AI-kapaciteter med praktisk anvendelse. Sp√∏rgsm√•let er: hvad vil du skabe? üöÄ

## GitHub Copilot Agent Challenge üöÄ

Brug Agent-tilstand til at l√∏se f√∏lgende udfordring:

**Beskrivelse:** Byg en avanceret AI-drevet kodegennemgangsassistent, der kombinerer flere LangChain-funktioner inklusive v√¶rkt√∏jskald, struktureret output og samtaleminde for at give omfattende feedback p√• kodeindsendelser.

**Prompt:** Opret en CodeReviewAssistant-klasse, der implementerer:
1. Et v√¶rkt√∏j til analyse af kodekompleksitet og forslag til forbedringer
2. Et v√¶rkt√∏j til kontrol af kode i forhold til best practices
3. Struktureret output ved brug af Pydantic-modeller for konsistent gennemgangsformat
4. Samtaleminde til at spore gennemgangssessioner
5. Et hovedchatinterface, der kan h√•ndtere kodeindsendelser og levere detaljeret, handlingsrettet feedback

Assistenten skal kunne gennemg√• kode i flere programmeringssprog, bevare kontekst p√• tv√¶rs af flere kodeindsendelser i en session og levere b√•de resum√©bed√∏mmelser og detaljerede forbedringsforslag.

L√¶s mere om [agent-tilstand](https://code.visualstudio.com/blogs/2025/02/24/introducing-copilot-agent-mode) her.

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**Ansvarsfraskrivelse**:
Dette dokument er blevet oversat ved hj√¶lp af AI-overs√¶ttelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selvom vi bestr√¶ber os p√• n√∏jagtighed, bedes du v√¶re opm√¶rksom p√•, at automatiserede overs√¶ttelser kan indeholde fejl eller un√∏jagtigheder. Det oprindelige dokument p√• dets modersm√•l b√∏r betragtes som den autoritative kilde. For kritisk information anbefales professionel menneskelig overs√¶ttelse. Vi p√•tager os intet ansvar for eventuelle misforst√•elser eller fejltolkninger, der opst√•r som f√∏lge af brugen af denne overs√¶ttelse.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->