<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "e2c4ae5688e34b4b8b09d52aec56c79e",
  "translation_date": "2025-10-23T21:59:53+00:00",
  "source_file": "10-ai-framework-project/README.md",
  "language_code": "da"
}
-->
# AI Framework

Har du nogensinde f칮lt dig overv칝ldet af at skulle bygge AI-applikationer fra bunden? Du er ikke alene! AI-frameworks er som en schweizerkniv for AI-udvikling - kraftfulde v칝rkt칮jer, der kan spare dig tid og besv칝r, n친r du bygger intelligente applikationer. T칝nk p친 et AI-framework som et velorganiseret bibliotek: det tilbyder forudbyggede komponenter, standardiserede API'er og smarte abstraktioner, s친 du kan fokusere p친 at l칮se problemer i stedet for at k칝mpe med implementeringsdetaljer.

I denne lektion vil vi udforske, hvordan frameworks som LangChain kan forvandle tidligere komplekse AI-integrationer til ren og overskuelig kode. Du vil opdage, hvordan du kan tackle virkelige udfordringer som at holde styr p친 samtaler, implementere v칝rkt칮jskald og jonglere med forskellige AI-modeller gennem 칠n samlet gr칝nseflade.

N친r vi er f칝rdige, vil du vide, hvorn친r du skal v칝lge frameworks frem for r친 API-kald, hvordan du bruger deres abstraktioner effektivt, og hvordan du bygger AI-applikationer, der er klar til brug i den virkelige verden. Lad os udforske, hvad AI-frameworks kan g칮re for dine projekter.

## Hvorfor v칝lge et framework?

S친 du er klar til at bygge en AI-app - fantastisk! Men her er sagen: du har flere forskellige veje, du kan tage, og hver har sine egne fordele og ulemper. Det er lidt som at v칝lge mellem at g친, cykle eller k칮re for at komme et sted hen - de vil alle f친 dig derhen, men oplevelsen (og indsatsen) vil v칝re helt forskellig.

Lad os bryde de tre hovedm친der ned, som du kan integrere AI i dine projekter:

| Metode | Fordele | Bedst til | Overvejelser |
|--------|---------|----------|--------------|
| **Direkte HTTP-anmodninger** | Fuld kontrol, ingen afh칝ngigheder | Enkle foresp칮rgsler, l칝re grundprincipper | Mere omfattende kode, manuel fejlbehandling |
| **SDK-integration** | Mindre boilerplate, model-specifik optimering | Applikationer med 칠n model | Begr칝nset til specifikke udbydere |
| **AI-frameworks** | Samlet API, indbyggede abstraktioner | Apps med flere modeller, komplekse arbejdsgange | L칝ringskurve, potentiel over-abstraktion |

### Fordele ved frameworks i praksis

```mermaid
graph TD
    A[Your Application] --> B[AI Framework]
    B --> C[OpenAI GPT]
    B --> D[Anthropic Claude]
    B --> E[GitHub Models]
    B --> F[Local Models]
    
    B --> G[Built-in Tools]
    G --> H[Memory Management]
    G --> I[Conversation History]
    G --> J[Function Calling]
    G --> K[Error Handling]
```

**Hvorfor frameworks er vigtige:**
- **Samler** flere AI-udbydere under 칠n gr칝nseflade
- **H친ndterer** samtaleminde automatisk
- **Tilbyder** f칝rdiglavede v칝rkt칮jer til almindelige opgaver som embeddings og funktionskald
- **Administrerer** fejlbehandling og retry-logik
- **Forvandler** komplekse arbejdsgange til overskuelige metodekald

> 游눠 **Pro Tip**: Brug frameworks, n친r du skifter mellem forskellige AI-modeller eller bygger komplekse funktioner som agenter, hukommelse eller v칝rkt칮jskald. Hold dig til direkte API'er, n친r du l칝rer det grundl칝ggende eller bygger enkle, fokuserede applikationer.

**Konklusion**: Ligesom valget mellem en h친ndv칝rkers specialv칝rkt칮jer og et komplet v칝rksted handler det om at matche v칝rkt칮jet til opgaven. Frameworks er fremragende til komplekse, funktionsrige applikationer, mens direkte API'er fungerer godt til enkle anvendelser.

## Introduktion

I denne lektion vil vi l칝re at:

- Bruge et almindeligt AI-framework.
- L칮se almindelige problemer som chatsamtaler, v칝rkt칮jsbrug, hukommelse og kontekst.
- Udnytte dette til at bygge AI-applikationer.

## Din f칮rste AI-prompt

Lad os starte med det grundl칝ggende ved at oprette din f칮rste AI-applikation, der sender et sp칮rgsm친l og f친r et svar tilbage. Ligesom Archimedes, der opdagede opdriftens princip i sit bad, kan de enkleste observationer f칮re til de mest kraftfulde indsigter - og frameworks g칮r disse indsigter tilg칝ngelige.

### Ops칝tning af LangChain med GitHub-modeller

Vi vil bruge LangChain til at forbinde til GitHub-modeller, hvilket er ret fedt, fordi det giver dig gratis adgang til forskellige AI-modeller. Det bedste? Du beh칮ver kun nogle f친 enkle konfigurationsparametre for at komme i gang:

```python
from langchain_openai import ChatOpenAI
import os

llm = ChatOpenAI(
    api_key=os.environ["GITHUB_TOKEN"],
    base_url="https://models.github.ai/inference",
    model="openai/gpt-4o-mini",
)

# Send a simple prompt
response = llm.invoke("What's the capital of France?")
print(response.content)
```

**Lad os bryde ned, hvad der sker her:**
- **Opretter** en LangChain-klient ved hj칝lp af klassen `ChatOpenAI` - dette er din adgang til AI!
- **Konfigurerer** forbindelsen til GitHub-modeller med din autentificeringstoken
- **Angiver** hvilken AI-model der skal bruges (`gpt-4o-mini`) - t칝nk p친 det som at v칝lge din AI-assistent
- **Sender** dit sp칮rgsm친l ved hj칝lp af metoden `invoke()` - her sker magien
- **Ekstraherer** og viser svaret - og voil, du chatter med AI!

> 游댢 **Ops칝tningsnotat**: Hvis du bruger GitHub Codespaces, er du heldig - `GITHUB_TOKEN` er allerede sat op for dig! Arbejder du lokalt? Bare rolig, du skal blot oprette en personlig adgangstoken med de rigtige tilladelser.

**Forventet output:**
```text
The capital of France is Paris.
```

```mermaid
sequenceDiagram
    participant App as Your Python App
    participant LC as LangChain
    participant GM as GitHub Models
    participant AI as GPT-4o-mini
    
    App->>LC: llm.invoke("What's the capital of France?")
    LC->>GM: HTTP request with prompt
    GM->>AI: Process prompt
    AI->>GM: Generated response
    GM->>LC: Return response
    LC->>App: response.content
```

## Bygning af samtale-AI

Det f칮rste eksempel demonstrerer det grundl칝ggende, men det er kun en enkelt udveksling - du stiller et sp칮rgsm친l, f친r et svar, og det er det. I virkelige applikationer vil du have, at din AI husker, hvad du har diskuteret, ligesom Watson og Holmes byggede deres unders칮gende samtaler op over tid.

Her bliver LangChain s칝rligt nyttig. Det tilbyder forskellige beskedtyper, der hj칝lper med at strukturere samtaler og giver dig mulighed for at give din AI en personlighed. Du vil bygge chatoplevelser, der opretholder kontekst og karakter.

### Forst친else af beskedtyper

T칝nk p친 disse beskedtyper som forskellige "roller", som deltagerne har i en samtale. LangChain bruger forskellige beskedklasser til at holde styr p친, hvem der siger hvad:

| Beskedtype | Form친l | Eksempel p친 brug |
|------------|--------|------------------|
| `SystemMessage` | Definerer AI's personlighed og adf칝rd | "Du er en hj칝lpsom kodeassistent" |
| `HumanMessage` | Repr칝senterer brugerinput | "Forklar hvordan funktioner fungerer" |
| `AIMessage` | Gemmer AI-svar | Tidligere AI-svar i samtalen |

### Oprettelse af din f칮rste samtale

Lad os oprette en samtale, hvor vores AI antager en bestemt rolle. Vi f친r den til at indtage rollen som Captain Picard - en karakter kendt for sin diplomatiske visdom og lederskab:

```python
messages = [
    SystemMessage(content="You are Captain Picard of the Starship Enterprise"),
    HumanMessage(content="Tell me about you"),
]
```

**Gennemgang af denne samtaleops칝tning:**
- **Etablerer** AI's rolle og personlighed gennem `SystemMessage`
- **Giver** den f칮rste brugerforesp칮rgsel via `HumanMessage`
- **Skaber** grundlaget for en samtale med flere ture

Den fulde kode for dette eksempel ser s친dan ud:

```python
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_openai import ChatOpenAI
import os

llm = ChatOpenAI(
    api_key=os.environ["GITHUB_TOKEN"],
    base_url="https://models.github.ai/inference",
    model="openai/gpt-4o-mini",
)

messages = [
    SystemMessage(content="You are Captain Picard of the Starship Enterprise"),
    HumanMessage(content="Tell me about you"),
]


# works
response  = llm.invoke(messages)
print(response.content)
```

Du b칮r se et resultat, der ligner:

```text
I am Captain Jean-Luc Picard, the commanding officer of the USS Enterprise (NCC-1701-D), a starship in the United Federation of Planets. My primary mission is to explore new worlds, seek out new life and new civilizations, and boldly go where no one has gone before. 

I believe in the importance of diplomacy, reason, and the pursuit of knowledge. My crew is diverse and skilled, and we often face challenges that test our resolve, ethics, and ingenuity. Throughout my career, I have encountered numerous species, grappled with complex moral dilemmas, and have consistently sought peaceful solutions to conflicts.

I hold the ideals of the Federation close to my heart, believing in the importance of cooperation, understanding, and respect for all sentient beings. My experiences have shaped my leadership style, and I strive to be a thoughtful and just captain. How may I assist you further?
```

For at opretholde samtalekontinuitet (i stedet for at nulstille konteksten hver gang) skal du forts칝tte med at tilf칮je svar til din beskedliste. Ligesom de mundtlige traditioner, der bevarede historier gennem generationer, bygger denne tilgang en varig hukommelse:

```python
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_openai import ChatOpenAI
import os

llm = ChatOpenAI(
    api_key=os.environ["GITHUB_TOKEN"],
    base_url="https://models.github.ai/inference",
    model="openai/gpt-4o-mini",
)

messages = [
    SystemMessage(content="You are Captain Picard of the Starship Enterprise"),
    HumanMessage(content="Tell me about you"),
]


# works
response  = llm.invoke(messages)

print(response.content)

print("---- Next ----")

messages.append(response)
messages.append(HumanMessage(content="Now that I know about you, I'm Chris, can I be in your crew?"))

response  = llm.invoke(messages)

print(response.content)

```

Ret smart, ikke? Det, der sker her, er, at vi kalder LLM to gange - f칮rst med kun vores f칮rste to beskeder, men derefter igen med hele samtalehistorikken. Det er som om, AI'en faktisk f칮lger med i vores chat!

N친r du k칮rer denne kode, f친r du et andet svar, der lyder noget i retning af:

```text
Welcome aboard, Chris! It's always a pleasure to meet those who share a passion for exploration and discovery. While I cannot formally offer you a position on the Enterprise right now, I encourage you to pursue your aspirations. We are always in need of talented individuals with diverse skills and backgrounds. 

If you are interested in space exploration, consider education and training in the sciences, engineering, or diplomacy. The values of curiosity, resilience, and teamwork are crucial in Starfleet. Should you ever find yourself on a starship, remember to uphold the principles of the Federation: peace, understanding, and respect for all beings. Your journey can lead you to remarkable adventures, whether in the stars or on the ground. Engage!
```

Det tager jeg som et m친ske ;)

## Streaming-svar

Har du nogensinde bem칝rket, hvordan ChatGPT ser ud til at "skrive" sine svar i realtid? Det er streaming i aktion. Ligesom at se en dygtig kalligraf arbejde - se tegnene dukke op streg for streg i stedet for at materialisere sig med det samme - g칮r streaming interaktionen mere naturlig og giver 칮jeblikkelig feedback.

### Implementering af streaming med LangChain

```python
from langchain_openai import ChatOpenAI
import os

llm = ChatOpenAI(
    api_key=os.environ["GITHUB_TOKEN"],
    base_url="https://models.github.ai/inference",
    model="openai/gpt-4o-mini",
    streaming=True
)

# Stream the response
for chunk in llm.stream("Write a short story about a robot learning to code"):
    print(chunk.content, end="", flush=True)
```

**Hvorfor streaming er fantastisk:**
- **Viser** indhold, mens det bliver skabt - ikke mere akavet ventetid!
- **F친r** brugerne til at f칮le, at der faktisk sker noget
- **F칮les** hurtigere, selv n친r det teknisk set ikke er det
- **Lader** brugerne begynde at l칝se, mens AI stadig "t칝nker"

> 游눠 **Brugeroplevelse Tip**: Streaming er virkelig effektivt, n친r du arbejder med l칝ngere svar som kodeforklaringer, kreativ skrivning eller detaljerede vejledninger. Dine brugere vil elske at se fremskridt i stedet for at stirre p친 en tom sk칝rm!

## Prompt-skabeloner

Prompt-skabeloner fungerer som de retoriske strukturer, der blev brugt i klassisk retorik - t칝nk p친, hvordan Cicero ville tilpasse sine taleformer til forskellige publikum, mens han bevarede den samme overbevisende ramme. De giver dig mulighed for at skabe genanvendelige prompts, hvor du kan udskifte forskellige oplysninger uden at skulle omskrive alt fra bunden. N친r du har oprettet skabelonen, skal du bare fylde variablerne med de n칮dvendige v칝rdier.

### Oprettelse af genanvendelige prompts

```python
from langchain_core.prompts import ChatPromptTemplate

# Define a template for code explanations
template = ChatPromptTemplate.from_messages([
    ("system", "You are an expert programming instructor. Explain concepts clearly with examples."),
    ("human", "Explain {concept} in {language} with a practical example for {skill_level} developers")
])

# Use the template with different values
questions = [
    {"concept": "functions", "language": "JavaScript", "skill_level": "beginner"},
    {"concept": "classes", "language": "Python", "skill_level": "intermediate"},
    {"concept": "async/await", "language": "JavaScript", "skill_level": "advanced"}
]

for question in questions:
    prompt = template.format_messages(**question)
    response = llm.invoke(prompt)
    print(f"Topic: {question['concept']}\n{response.content}\n---\n")
```

**Hvorfor du vil elske at bruge skabeloner:**
- **Holder** dine prompts konsistente i hele din app
- **Ingen flere** rodede strengsammenk칝dninger - bare rene, enkle variabler
- **Din AI** opf칮rer sig forudsigeligt, fordi strukturen forbliver den samme
- **Opdateringer** er nemme - 칝ndr skabelonen 칠n gang, og det er rettet overalt

## Struktureret output

Har du nogensinde v칝ret frustreret over at skulle analysere AI-svar, der kommer tilbage som ustruktureret tekst? Struktureret output er som at l칝re din AI at f칮lge den systematiske tilgang, som Linnaeus brugte til biologisk klassifikation - organiseret, forudsigelig og nem at arbejde med. Du kan anmode om JSON, specifikke datastrukturer eller ethvert format, du har brug for.

### Definere outputskemaer

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser
from pydantic import BaseModel, Field

class CodeReview(BaseModel):
    score: int = Field(description="Code quality score from 1-10")
    strengths: list[str] = Field(description="List of code strengths")
    improvements: list[str] = Field(description="List of suggested improvements")
    overall_feedback: str = Field(description="Summary feedback")

# Set up the parser
parser = JsonOutputParser(pydantic_object=CodeReview)

# Create prompt with format instructions
prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a code reviewer. {format_instructions}"),
    ("human", "Review this code: {code}")
])

# Format the prompt with instructions
chain = prompt | llm | parser

# Get structured response
code_sample = """
def calculate_average(numbers):
    return sum(numbers) / len(numbers)
"""

result = chain.invoke({
    "code": code_sample,
    "format_instructions": parser.get_format_instructions()
})

print(f"Score: {result['score']}")
print(f"Strengths: {', '.join(result['strengths'])}")
```

**Hvorfor struktureret output er en game-changer:**
- **Ingen flere** g칝t om, hvilket format du f친r tilbage - det er konsekvent hver gang
- **Plugs** direkte ind i dine databaser og API'er uden ekstra arbejde
- **Fanger** m칝rkelige AI-svar, f칮r de 칮del칝gger din app
- **G칮r** din kode renere, fordi du ved pr칝cis, hvad du arbejder med

## V칝rkt칮jskald

Nu n친r vi til en af de mest kraftfulde funktioner: v칝rkt칮jer. Dette er, hvordan du giver din AI praktiske evner ud over samtale. Ligesom hvordan middelalderlige laug udviklede specialv칝rkt칮jer til specifikke h친ndv칝rk, kan du udstyre din AI med fokuserede instrumenter. Du beskriver, hvilke v칝rkt칮jer der er tilg칝ngelige, og n친r nogen anmoder om noget, der matcher, kan din AI tage handling.

### Brug af Python

Lad os tilf칮je nogle v칝rkt칮jer som s친:

```python
from typing_extensions import Annotated, TypedDict

class add(TypedDict):
    """Add two integers."""

    # Annotations must have the type and can optionally include a default value and description (in that order).
    a: Annotated[int, ..., "First integer"]
    b: Annotated[int, ..., "Second integer"]

tools = [add]

functions = {
    "add": lambda a, b: a + b
}
```

S친 hvad sker der her? Vi opretter en skabelon for et v칝rkt칮j kaldet `add`. Ved at arve fra `TypedDict` og bruge de smarte `Annotated` typer for `a` og `b`, giver vi LLM et klart billede af, hvad dette v칝rkt칮j g칮r, og hvad det har brug for. Ordbogen `functions` er som vores v칝rkt칮jskasse - den fort칝ller vores kode pr칝cis, hvad der skal g칮res, n친r AI'en beslutter sig for at bruge et specifikt v칝rkt칮j.

Lad os se, hvordan vi kalder LLM med dette v칝rkt칮j n칝ste gang:

```python
llm = ChatOpenAI(
    api_key=os.environ["GITHUB_TOKEN"],
    base_url="https://models.github.ai/inference",
    model="openai/gpt-4o-mini",
)

llm_with_tools = llm.bind_tools(tools)
```

Her kalder vi `bind_tools` med vores `tools` array, og dermed har LLM `llm_with_tools` nu kendskab til dette v칝rkt칮j.

For at bruge denne nye LLM kan vi skrive f칮lgende kode:

```python
query = "What is 3 + 12?"

res = llm_with_tools.invoke(query)
if(res.tool_calls):
    for tool in res.tool_calls:
        print("TOOL CALL: ", functions[tool["name"]](../../../10-ai-framework-project/**tool["args"]))
print("CONTENT: ",res.content)
```

Nu hvor vi kalder `invoke` p친 denne nye LLM, der har v칝rkt칮jer, kan egenskaben `tool_calls` blive udfyldt. Hvis det er tilf칝ldet, har ethvert identificeret v칝rkt칮j en `name` og `args` egenskab, der identificerer, hvilket v칝rkt칮j der skal kaldes og med hvilke argumenter. Den fulde kode ser s친dan ud:

```python
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_openai import ChatOpenAI
import os
from typing_extensions import Annotated, TypedDict

class add(TypedDict):
    """Add two integers."""

    # Annotations must have the type and can optionally include a default value and description (in that order).
    a: Annotated[int, ..., "First integer"]
    b: Annotated[int, ..., "Second integer"]

tools = [add]

functions = {
    "add": lambda a, b: a + b
}

llm = ChatOpenAI(
    api_key=os.environ["GITHUB_TOKEN"],
    base_url="https://models.github.ai/inference",
    model="openai/gpt-4o-mini",
)

llm_with_tools = llm.bind_tools(tools)

query = "What is 3 + 12?"

res = llm_with_tools.invoke(query)
if(res.tool_calls):
    for tool in res.tool_calls:
        print("TOOL CALL: ", functions[tool["name"]](../../../10-ai-framework-project/**tool["args"]))
print("CONTENT: ",res.content)
```

N친r du k칮rer denne kode, b칮r du se output, der ligner:

```text
TOOL CALL:  15
CONTENT: 
```

AI'en unders칮gte "Hvad er 3 + 12" og genkendte dette som en opgave for v칝rkt칮jet `add`. Ligesom hvordan en dygtig bibliotekar ved, hvilken reference der skal konsulteres baseret p친 typen af sp칮rgsm친l, tog den denne beslutning ud fra v칝rkt칮jets navn, beskrivelse og felt-specifikationer. Resultatet p친 15 kommer fra vores ordbog `functions`, der udf칮rer v칝rkt칮jet:

```python
print("TOOL CALL: ", functions[tool["name"]](../../../10-ai-framework-project/**tool["args"]))
```

### Et mere interessant v칝rkt칮j, der kalder et web-API

At l칝gge tal sammen demonstrerer konceptet, men rigtige v칝rkt칮jer udf칮rer typisk mere komplekse operationer, som at kalde web-API'er. Lad os udvide vores eksempel, s친 AI'en kan hente indhold fra internettet - ligesom hvordan telegrafoperat칮rer engang forbandt fjerne steder:

```python
class joke(TypedDict):
    """Tell a joke."""

    # Annotations must have the type and can optionally include a default value and description (in that order).
    category: Annotated[str, ..., "The joke category"]

def get_joke(category: str) -> str:
    response = requests.get(f"https://api.chucknorris.io/jokes/random?category={category}", headers={"Accept": "application/json"})
    if response.status_code == 200:
        return response.json().get("value", f"Here's a {category} joke!")
    return f"Here's a {category} joke!"

functions = {
    "add": lambda a, b: a + b,
    "joke": lambda category: get_joke(category)
}

query = "Tell me a joke about animals"

# the rest of the code is the same
```

Nu, hvis du k칮rer denne kode, vil du f친 et svar, der siger noget i retning af:

```text
TOOL CALL:  Chuck Norris once rode a nine foot grizzly bear through an automatic car wash, instead of taking a shower.
CONTENT:  
```

Her er koden i sin helhed:

```python
from langchain_openai import ChatOpenAI
import requests
import os
from typing_extensions import Annotated, TypedDict

class add(TypedDict):
    """Add two integers."""

    # Annotations must have the type and can optionally include a default value and description (in that order).
    a: Annotated[int, ..., "First integer"]
    b: Annotated[int, ..., "Second integer"]

class joke(TypedDict):
    """Tell a joke."""

    # Annotations must have the type and can optionally include a default value and description (in that order).
    category: Annotated[str, ..., "The joke category"]

tools = [add, joke]

def get_joke(category: str) -> str:
    response = requests.get(f"https://api.chucknorris.io/jokes/random?category={category}", headers={"Accept": "application/json"})
    if response.status_code == 200:
        return response.json().get("value", f"Here's a {category} joke!")
    return f"Here's a {category} joke!"

functions = {
    "add": lambda a, b: a + b,
    "joke": lambda category: get_joke(category)
}

llm = ChatOpenAI(
    api_key=os.environ["GITHUB_TOKEN"],
    base_url="https://models.github.ai/inference",
    model="openai/gpt-4o-mini",
)

llm_with_tools = llm.bind_tools(tools)

query = "Tell me a joke about animals"

res = llm_with_tools.invoke(query)
if(res.tool_calls):
    for tool in res.tool_calls:
        # print("TOOL CALL: ", tool)
        print("TOOL CALL: ", functions[tool["name"]](../../../10-ai-framework-project/**tool["args"]))
print("CONTENT: ",res.content)
```

## Embeddings og dokumentbehandling

Embeddings repr칝senterer en af de mest elegante l칮sninger i moderne AI. Forestil dig, hvis du kunne tage et hvilket som helst stykke tekst og konvertere det til numeriske koordinater, der fanger dets betydning. Det er pr칝cis, hvad embeddings g칮r - de transformerer tekst til punkter i et multidimensionelt rum, hvor lignende begreber samles. Det er som at have et koordinatsystem for ideer, der minder om, hvordan Mendeleev organiserede det periodiske system efter atomare egenskaber.

### Oprettelse og brug af embeddings

```python
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import CharacterTextSplitter

# Initialize embeddings
embeddings = OpenAIEmbeddings(
    api_key=os.environ["GITHUB_TOKEN"],
    base_url="https://models.github.ai/inference",
    model="text-embedding-3-small"
)

# Load and split documents
loader = TextLoader("documentation.txt")
documents = loader.load()

text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
texts = text_splitter.split_documents(documents)

# Create vector store
vectorstore = FAISS.from_documents(texts, embeddings)

# Perform similarity search
query = "How do I handle user authentication?"
similar_docs = vectorstore.similarity_search(query, k=3)

for doc in similar_docs:
    print(f"Relevant content: {doc.page_content[:200]}...")
```

### Dokumentindl칝sere til forskellige formater

```python
from langchain_community.document_loaders import (
    PyPDFLoader,
    CSVLoader,
    JSONLoader,
    WebBaseLoader
)

# Load different document types
pdf_loader = PyPDFLoader("manual.pdf")
csv_loader = CSVLoader("data.csv")
json_loader = JSONLoader("config.json")
web_loader = WebBaseLoader("https://example.com/docs")

# Process all documents
all_documents = []
for loader in [pdf_loader, csv_loader, json_loader, web_loader]:
    docs = loader.load()
    all_documents.extend(docs)
```

**Hvad du kan g칮re med embeddings:**
- **Byg** s칮gning, der faktisk forst친r, hvad du mener, ikke kun n칮gleord
- **Skab** AI, der kan besvare sp칮rgsm친l om dine dokumenter
- **Lav** anbefalingssystemer, der foresl친r virkelig relevant indhold
- **Organiser** og kategoriser automatisk dit indhold

## Bygning af en komplet AI-applikation

Nu vil vi integrere alt, hvad du har l칝rt, i en omfattende applikation - en kodeassistent, der kan besvare sp칮rgsm친l, bruge v칝rkt칮jer og opretholde samtaleminde. Ligesom hvordan trykpressen kombinerede eksisterende teknologier (bev칝gelige typer, bl칝k, papir og tryk) til noget transformerende, vil vi kombinere vores AI-komponenter til noget praktisk og nyttigt.

### Komplet applikationseksempel

```python
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage
from langchain_community.vectorstores import FAISS
from typing_extensions import Annotated, TypedDict
import os
import requests

class CodingAssistant:
    def __init__(self):
        self.llm = ChatOpenAI(
            api_key=os.environ["GITHUB_TOKEN"],
            base_url="https://models.github.ai/inference",
            model="openai/gpt-4o-mini"
        )
        
        self.conversation_history = [
            SystemMessage(content="""You are an expert coding assistant. 
            Help users learn programming concepts, debug code, and write better software.
            Use tools when needed and maintain a helpful, encouraging tone.""")
        ]
        
        # Define tools
        self.setup_tools()
    
    def setup_tools(self):
        class web_search(TypedDict):
            """Search for programming documentation or examples."""
            query: Annotated[str, "Search query for programming help"]
        
        class code_formatter(TypedDict):
            """Format and validate code snippets."""
            code: Annotated[str, "Code to format"]
            language: Annotated[str, "Programming language"]
        
        self.tools = [web_search, code_formatter]
        self.llm_with_tools = self.llm.bind_tools(self.tools)
    
    def chat(self, user_input: str):
        # Add user message to conversation
        self.conversation_history.append(HumanMessage(content=user_input))
        
        # Get AI response
        response = self.llm_with_tools.invoke(self.conversation_history)
        
        # Handle tool calls if any
        if response.tool_calls:
            for tool_call in response.tool_calls:
                tool_result = self.execute_tool(tool_call)
                print(f"游댢 Tool used: {tool_call['name']}")
                print(f"游늵 Result: {tool_result}")
        
        # Add AI response to conversation
        self.conversation_history.append(response)
        
        return response.content
    
    def execute_tool(self, tool_call):
        tool_name = tool_call['name']
        args = tool_call['args']
        
        if tool_name == 'web_search':
            return f"Found documentation for: {args['query']}"
        elif tool_name == 'code_formatter':
            return f"Formatted {args['language']} code: {args['code'][:50]}..."
        
        return "Tool execution completed"

# Usage example
assistant = CodingAssistant()

print("游뱄 Coding Assistant Ready! Type 'quit' to exit.\n")

while True:
    user_input = input("You: ")
    if user_input.lower() == 'quit':
        break
    
    response = assistant.chat(user_input)
    print(f"游뱄 Assistant: {response}\n")
```

**Applikationsarkitektur:**

```mermaid
graph TD
    A[User Input] --> B[Coding Assistant]
    B --> C[Conversation Memory]
    B --> D[Tool Detection]
    B --> E[LLM Processing]
    
    D --> F[Web Search Tool]
    D --> G[Code Formatter Tool]
    
    E --> H[Response Generation]
    F --> H
    G --> H
    
    H --> I[User Interface]
    H --> C
```

**N칮glefunktioner vi har implementeret:**
- **Husker** hele din samtale for kontekstkontinuitet
- **Udf칮rer handlinger** gennem v칝rkt칮jskald, ikke kun samtale
- **F칮lger** forudsigelige interaktionsm칮nstre
- **Administrerer** fejlbehandling og komplekse arbejdsgange automatisk

## Opgave: Byg din egen AI-drevne studieassistent

**M친l**: Opret en AI-applikation, der hj칝lper studerende med at l칝re programmeringskoncepter ved at give forklaringer, kodeeksempler og interaktive quizzer.

### Krav

**Kernefunktioner (p친kr칝vet):**
1. **Samtalegr칝nseflade**: Implementer et chatsystem, der opretholder kontekst p친 tv칝rs af flere sp칮rgsm친l
2. **Uddannelsesv칝rkt칮jer**: Opret mindst to v칝rkt칮jer, der hj칝lper med l칝ring:
   - V칝rkt칮j til kodeforklaring
   - Generator til konceptquiz
3. **Personlig l칝ring**: Brug systemmeddelelser til at tilpasse svar til forskellige f칝rdighedsniveauer  
4. **Svarformatering**: Implementer struktureret output til quizsp칮rgsm친l  

### Implementeringstrin  

**Trin 1: Ops칝t dit milj칮**  
```bash
pip install langchain langchain-openai
```
  
**Trin 2: Grundl칝ggende chatfunktionalitet**  
- Opret en `StudyAssistant`-klasse  
- Implementer samtaleminde  
- Tilf칮j personlighedskonfiguration til p칝dagogisk st칮tte  

**Trin 3: Tilf칮j undervisningsv칝rkt칮jer**  
- **Kodeforklarer**: Bryder kode ned i forst친elige dele  
- **Quizgenerator**: Skaber sp칮rgsm친l om programmeringskoncepter  
- **Fremskridtssporing**: Holder styr p친 d칝kkede emner  

**Trin 4: Avancerede funktioner (valgfrit)**  
- Implementer streaming-svar for bedre brugeroplevelse  
- Tilf칮j dokumentindl칝sning for at integrere kursusmateriale  
- Opret embeddings til indholdsgenkendelse baseret p친 lighed  

### Evalueringskriterier  

| Funktion | Fremragende (4) | God (3) | Tilfredsstillende (2) | Kr칝ver arbejde (1) |  
|----------|-----------------|---------|-----------------------|--------------------|  
| **Samtaleflow** | Naturlige, kontekstbevidste svar | God kontekstbevarelse | Grundl칝ggende samtale | Ingen hukommelse mellem udvekslinger |  
| **V칝rkt칮jsintegration** | Flere nyttige v칝rkt칮jer fungerer problemfrit | 2+ v칝rkt칮jer implementeret korrekt | 1-2 grundl칝ggende v칝rkt칮jer | V칝rkt칮jer fungerer ikke |  
| **Kodekvalitet** | Ren, veldokumenteret, fejlh친ndtering | God struktur, noget dokumentation | Grundl칝ggende funktionalitet fungerer | D친rlig struktur, ingen fejlh친ndtering |  
| **Uddannelsesm칝ssig v칝rdi** | Virkelig nyttig for l칝ring, tilpasset | God l칝ringsst칮tte | Grundl칝ggende forklaringer | Begr칝nset uddannelsesm칝ssig fordel |  

### Eksempel p친 kodestruktur  

```python
class StudyAssistant:
    def __init__(self, skill_level="beginner"):
        # Initialize LLM, tools, and conversation memory
        pass
    
    def explain_code(self, code, language):
        # Tool: Explain how code works
        pass
    
    def generate_quiz(self, topic, difficulty):
        # Tool: Create practice questions
        pass
    
    def chat(self, user_input):
        # Main conversation interface
        pass

# Example usage
assistant = StudyAssistant(skill_level="intermediate")
response = assistant.chat("Explain how Python functions work")
```
  
**Bonusudfordringer:**  
- Tilf칮j stemmeinput/output-funktioner  
- Implementer en webgr칝nseflade ved hj칝lp af Streamlit eller Flask  
- Opret en vidensbase fra kursusmateriale ved hj칝lp af embeddings  
- Tilf칮j fremskridtssporing og personlige l칝ringsforl칮b  

## Resum칠  

游꿀 Du har nu mestret grundl칝ggende AI-rammeudvikling og l칝rt, hvordan man bygger sofistikerede AI-applikationer ved hj칝lp af LangChain. Som at fuldf칮re en omfattende l칝rlingeuddannelse har du erhvervet en betydelig v칝rkt칮jskasse af f칝rdigheder. Lad os gennemg친, hvad du har opn친et.  

### Hvad du har l칝rt  

**Kernekoncepter for rammer:**  
- **Fordele ved rammer**: Forst친else af, hvorn친r man skal v칝lge rammer frem for direkte API-kald  
- **Grundl칝ggende om LangChain**: Ops칝tning og konfiguration af AI-modelforbindelser  
- **Meddelelsestyper**: Brug af `SystemMessage`, `HumanMessage` og `AIMessage` til strukturerede samtaler  

**Avancerede funktioner:**  
- **V칝rkt칮jskald**: Oprettelse og integration af brugerdefinerede v칝rkt칮jer til forbedrede AI-funktioner  
- **Samtaleminde**: Bevare kontekst p친 tv칝rs af flere samtaleomgange  
- **Streaming-svar**: Implementering af realtidslevering af svar  
- **Promptskabeloner**: Opbygning af genanvendelige, dynamiske prompts  
- **Struktureret output**: Sikring af konsistente, parsebare AI-svar  
- **Embeddings**: Oprettelse af semantisk s칮gning og dokumentbehandlingsfunktioner  

**Praktiske anvendelser:**  
- **Opbygning af komplette apps**: Kombinere flere funktioner til produktionsklare applikationer  
- **Fejlh친ndtering**: Implementering af robust fejlstyring og validering  
- **V칝rkt칮jsintegration**: Oprettelse af brugerdefinerede v칝rkt칮jer, der udvider AI-funktioner  

### Vigtige pointer  

> 游꿢 **Husk**: AI-rammer som LangChain er i bund og grund dine kompleksitetsskjulende, funktionsfyldte bedste venner. De er perfekte, n친r du har brug for samtaleminde, v칝rkt칮jskald eller 칮nsker at arbejde med flere AI-modeller uden at miste overblikket.  

**Beslutningsramme for AI-integration:**  

```mermaid
flowchart TD
    A[AI Integration Need] --> B{Simple single query?}
    B -->|Yes| C[Direct API calls]
    B -->|No| D{Need conversation memory?}
    D -->|No| E[SDK Integration]
    D -->|Yes| F{Need tools or complex features?}
    F -->|No| G[Framework with basic setup]
    F -->|Yes| H[Full framework implementation]
    
    C --> I[HTTP requests, minimal dependencies]
    E --> J[Provider SDK, model-specific]
    G --> K[LangChain basic chat]
    H --> L[LangChain with tools, memory, agents]
```
  
### Hvor g친r du hen herfra?  

**Begynd at bygge med det samme:**  
- Tag disse koncepter og byg noget, der begejstrer DIG!  
- Eksperimenter med forskellige AI-modeller gennem LangChain - det er som en legeplads for AI-modeller  
- Skab v칝rkt칮jer, der l칮ser reelle problemer, du st친r over for i dit arbejde eller projekter  

**Klar til n칝ste niveau?**  
- **AI-agenter**: Byg AI-systemer, der faktisk kan planl칝gge og udf칮re komplekse opgaver p친 egen h친nd  
- **RAG (Retrieval-Augmented Generation)**: Kombiner AI med dine egne vidensbaser for superkraftige applikationer  
- **Multi-modal AI**: Arbejd med tekst, billeder og lyd sammen - mulighederne er uendelige!  
- **Produktionsimplementering**: L칝r hvordan du skalerer dine AI-apps og overv친ger dem i den virkelige verden  

**Bliv en del af f칝llesskabet:**  
- LangChain-f칝llesskabet er fantastisk til at holde sig opdateret og l칝re bedste praksis  
- GitHub Models giver dig adgang til avancerede AI-funktioner - perfekt til eksperimenter  
- Bliv ved med at 칮ve dig med forskellige anvendelsesscenarier - hvert projekt vil l칝re dig noget nyt  

Du har nu viden til at bygge intelligente, samtalebaserede applikationer, der kan hj칝lpe folk med at l칮se reelle problemer. Ligesom ren칝ssancens h친ndv칝rkere, der kombinerede kunstnerisk vision med teknisk dygtighed, kan du nu kombinere AI-funktioner med praktisk anvendelse. Sp칮rgsm친let er: hvad vil du skabe? 游  

## GitHub Copilot Agent-udfordring 游  

Brug Agent-tilstand til at fuldf칮re f칮lgende udfordring:  

**Beskrivelse:** Byg en avanceret AI-drevet kodegennemgangsassistent, der kombinerer flere LangChain-funktioner, herunder v칝rkt칮jskald, struktureret output og samtaleminde for at give omfattende feedback p친 kodeindsendelser.  

**Prompt:** Opret en CodeReviewAssistant-klasse, der implementerer:  
1. Et v칝rkt칮j til at analysere kodekompleksitet og foresl친 forbedringer  
2. Et v칝rkt칮j til at kontrollere kode mod bedste praksis  
3. Struktureret output ved hj칝lp af Pydantic-modeller for konsistent gennemgangsformat  
4. Samtaleminde til at spore gennemgangssessioner  
5. En hovedchatgr칝nseflade, der kan h친ndtere kodeindsendelser og give detaljeret, handlingsrettet feedback  

Assistenten skal kunne gennemg친 kode i flere programmeringssprog, bevare kontekst p친 tv칝rs af flere kodeindsendelser i en session og give b친de sammenfattende vurderinger og detaljerede forbedringsforslag.  

L칝s mere om [agent mode](https://code.visualstudio.com/blogs/2025/02/24/introducing-copilot-agent-mode) her.  

---

**Ansvarsfraskrivelse**:  
Dette dokument er blevet oversat ved hj칝lp af AI-overs칝ttelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selvom vi bestr칝ber os p친 n칮jagtighed, skal du v칝re opm칝rksom p친, at automatiserede overs칝ttelser kan indeholde fejl eller un칮jagtigheder. Det originale dokument p친 dets oprindelige sprog b칮r betragtes som den autoritative kilde. For kritisk information anbefales professionel menneskelig overs칝ttelse. Vi er ikke ansvarlige for eventuelle misforst친elser eller fejltolkninger, der opst친r som f칮lge af brugen af denne overs칝ttelse.