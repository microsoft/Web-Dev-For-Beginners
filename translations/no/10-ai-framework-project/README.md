# AI-rammeverk

Har du noen gang f√∏lt deg overveldet n√•r du pr√∏vde √• bygge AI-applikasjoner fra bunnen av? Du er ikke alene! AI-rammeverk er som √• ha en sveitsisk lommekniv for AI-utvikling - de er kraftige verkt√∏y som kan spare deg for tid og hodebry n√•r du bygger intelligente applikasjoner. Tenk p√• et AI-rammeverk som et godt organisert bibliotek: det tilbyr ferdigbygde komponenter, standardiserte API-er og smarte abstraksjoner slik at du kan fokusere p√• √• l√∏se problemer i stedet for √• slite med implementasjonsdetaljer.

I denne leksjonen skal vi utforske hvordan rammeverk som LangChain kan gj√∏re det som tidligere var komplekse AI-integrasjonsoppgaver om til ren, lettlest kode. Du vil oppdage hvordan du kan takle virkelige utfordringer som √• holde oversikt over samtaler, implementere verkt√∏ysamtaler og h√•ndtere ulike AI-modeller gjennom ett samlet grensesnitt.

N√•r vi er ferdige, vil du vite n√•r du skal bruke rammeverk i stedet for r√• API-kall, hvordan du bruker abstraksjonene deres effektivt, og hvordan du bygger AI-applikasjoner som er klare for bruk i virkelige situasjoner. La oss utforske hva AI-rammeverk kan gj√∏re for prosjektene dine.

## ‚ö° Hva du kan gj√∏re de neste 5 minuttene

**Rask start for travle utviklere**

```mermaid
flowchart LR
    A[‚ö° 5 minutter] --> B[Installer LangChain]
    B --> C[Opprett ChatOpenAI-klient]
    C --> D[Send f√∏rste prompt]
    D --> E[Se rammeverkets kraft]
```
- **Minutt 1**: Installer LangChain: `pip install langchain langchain-openai`
- **Minutt 2**: Sett opp GitHub-tokenet ditt og importer ChatOpenAI-klienten
- **Minutt 3**: Lag en enkel samtale med system- og menneskemeldinger
- **Minutt 4**: Legg til et grunnleggende verkt√∏y (som en add-funksjon) og se AI-verkt√∏ysamtale
- **Minutt 5**: Opplev forskjellen mellom r√• API-kall og rammeverksabstraksjon

**Rask testkode**:
```python
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage

llm = ChatOpenAI(
    api_key=os.environ["GITHUB_TOKEN"],
    base_url="https://models.github.ai/inference",
    model="openai/gpt-4o-mini"
)

response = llm.invoke([
    SystemMessage(content="You are a helpful coding assistant"),
    HumanMessage(content="Explain Python functions briefly")
])
print(response.content)
```

**Hvorfor dette er viktig**: P√• 5 minutter vil du oppleve hvordan AI-rammeverk forvandler kompleks AI-integrasjon til enkle metodetilkall. Dette er grunnlaget som driver produksjonsklare AI-applikasjoner.

## Hvorfor velge et rammeverk?

S√• du er klar til √• bygge en AI-app ‚Äì flott! Men her er tingen: du har flere forskjellige veier du kan g√•, og hver av dem har sine fordeler og ulemper. Det er litt som √• velge mellom √• g√•, sykle eller kj√∏re bil for √• komme til et sted ‚Äì alle vil komme deg dit, men opplevelsen (og innsatsen) blir helt forskjellig.

La oss bryte ned de tre hovedm√•tene du kan integrere AI i prosjektene dine p√•:

| Tiln√¶rming | Fordeler | Best for | Vurderinger |
|------------|----------|----------|-------------|
| **Direkte HTTP-foresp√∏rsler** | Full kontroll, ingen avhengigheter | Enkle foresp√∏rsler, l√¶re grunnleggende | Mer omfattende kode, manuell feilbehandling |
| **SDK-integrasjon** | Mindre boilerplate, modellspesifikk optimalisering | Applikasjoner med √©n modell | Begrenset til spesifikke leverand√∏rer |
| **AI-rammeverk** | Enhetlig API, innebygde abstraksjoner | Apper med flere modeller, komplekse arbeidsflyter | L√¶ringskurve, potensiell over-abstraksjon |

### Fordeler med rammeverk i praksis

```mermaid
graph TD
    A[Din Applikasjon] --> B[AI-rammeverk]
    B --> C[OpenAI GPT]
    B --> D[Anthropic Claude]
    B --> E[GitHub-modeller]
    B --> F[Lokale modeller]
    
    B --> G[Innebygde verkt√∏y]
    G --> H[Minneh√•ndtering]
    G --> I[Samtalehistorikk]
    G --> J[Funksjonsanrop]
    G --> K[Feilh√•ndtering]
```
**Hvorfor rammeverk betyr noe:**
- **Samler** flere AI-leverand√∏rer under ett grensesnitt
- **H√•ndterer** samtaleminne automatisk
- **Tilbyr** ferdige verkt√∏y for vanlige oppgaver som embeddings og funksjonskall
- **Administrerer** feilbehandling og retry-logikk
- **Gj√∏r** komplekse arbeidsflyter om til lettleste metodetilkall

> üí° **Profftips**: Bruk rammeverk n√•r du bytter mellom forskjellige AI-modeller eller bygger komplekse funksjoner som agenter, minne eller verkt√∏ysamtaler. Hold deg til direkte API-er n√•r du l√¶rer det grunnleggende eller bygger enkle, fokuserte apper.

**Konklusjon**: Som n√•r du velger mellom et h√•ndverkerverkt√∏y og et komplett verksted, handler det om √• matche verkt√∏yet med oppgaven. Rammeverk utmerker seg for komplekse, funksjonsrike applikasjoner, mens direkte API-er fungerer bra for enkle bruksomr√•der.

## üó∫Ô∏è Din l√¶ringsreise gjennom AI-rammeverk-mestring

```mermaid
journey
    title Fra r√• API-er til produksjonsklare AI-applikasjoner
    section Rammeverkets grunnlag
      Forst√• fordeler med abstraksjon: 4: You
      Mestre LangChain-grunnleggende: 6: You
      Sammenligne tiln√¶rminger: 7: You
    section Samtalesystemer
      Bygg chattegrensesnitt: 5: You
      Implementere minnem√∏nstre: 7: You
      H√•ndtere str√∏mmede svar: 8: You
    section Avanserte funksjoner
      Lag egendefinerte verkt√∏y: 6: You
      Mestre strukturert output: 8: You
      Bygg dokumentsystemer: 8: You
    section Produksjonsapplikasjoner
      Kombiner alle funksjoner: 7: You
      H√•ndtere feilsituasjoner: 8: You
      Distribuer komplette systemer: 9: You
```
**Ditt m√•l for reisen**: Innen slutten av denne leksjonen vil du ha mestret AI-rammeverksutvikling og kunne bygge sofistikerte, produksjonsklare AI-applikasjoner som kan konkurrere med kommersielle AI-assistenter.

## Introduksjon

I denne leksjonen skal vi l√¶re √•:

- Bruke et vanlig AI-rammeverk.
- Ta tak i vanlige problemer som chat-samtaler, verkt√∏ybruk, minne og kontekst.
- Utnytte dette til √• bygge AI-apper.

## üß† AI-rammeverksutviklings√∏kosystem

```mermaid
mindmap
  root((AI Frameworks))
    Fordeler med Abstraksjon
      Forenkling av Kode
        Enhetlige API-er
        Innebygget Feilh√•ndtering
        Konsistente M√∏nstre
        Redusert Kode Boilerplate
      St√∏tte for Flere Modeller
        Leverand√∏ruavhengig
        Enkel Bytting
        Tilbakefallsalternativer
        Kostnadsoptimalisering
    Kjernekomponenter
      Samtaleh√•ndtering
        Meldings Typer
        Minne Systemer
        Kontekst Sporing
        Historikk Persistens
      Verkt√∏y Integrasjon
        Funksjonskall
        API Forbindelser
        Tilpassede Verkt√∏y
        Arbeidsflyt Automatisering
    Avanserte Funksjoner
      Strukturert Utdata
        Pydantic Modeller
        JSON Skjemaer
        Typesikkerhet
        Valideringsregler
      Dokumentbehandling
        Embeddings
        Vektorbutikker
        Likhetss√∏k
        RAG-systemer
    Produksjonsm√∏nstre
      Applikasjonsarkitektur
        Modul√¶rt Design
        Feilgenskaper
        Asynkrone Operasjoner
        Tilstandsh√•ndtering
      Distribusjonsstrategier
        Skalerbarhet
        Overv√•king
        Ytelse
        Sikkerhet
```
**Kjernprinsipp**: AI-rammeverk abstraherer kompleksitet samtidig som de gir kraftige abstraksjoner for samtaleh√•ndtering, verkt√∏yintegrasjon og dokumentprosessering, som gj√∏r det mulig for utviklere √• bygge sofistikerte AI-applikasjoner med ren, vedlikeholdbar kode.

## Din f√∏rste AI-prompt

La oss starte med det grunnleggende ved √• lage din f√∏rste AI-applikasjon som sender et sp√∏rsm√•l og f√•r et svar tilbake. Som Archimedes som oppdaget prinsippet om forskyvning i badekaret sitt, er det noen ganger de enkleste observasjonene som f√∏rer til de mest kraftfulle innsiktene ‚Äì og rammeverk gj√∏r disse innsiktene tilgjengelige.

### Sette opp LangChain med GitHub-modeller

Vi skal bruke LangChain for √• koble til GitHub-modeller, noe som er ganske kult fordi det gir deg gratis tilgang til forskjellige AI-modeller. Det beste? Du trenger bare noen f√• enkle konfigurasjonsparametere for √• komme i gang:

```python
from langchain_openai import ChatOpenAI
import os

llm = ChatOpenAI(
    api_key=os.environ["GITHUB_TOKEN"],
    base_url="https://models.github.ai/inference",
    model="openai/gpt-4o-mini",
)

# Send en enkel oppfordring
response = llm.invoke("What's the capital of France?")
print(response.content)
```

**La oss bryte ned hva som skjer her:**
- **Oppretter** en LangChain-klient med `ChatOpenAI`-klassen ‚Äì dette er din inngangsport til AI!
- **Konfigurerer** tilkoblingen til GitHub-modeller med ditt autentiseringstoken
- **Spesifiserer** hvilken AI-modell som skal brukes (`gpt-4o-mini`) ‚Äì tenk p√• dette som √• velge din AI-assistent
- **Sender** sp√∏rsm√•let ditt med `invoke()`-metoden ‚Äì det er her magien skjer
- **Henter ut** og viser svaret ‚Äì og voil√†, du chatter med AI!

> üîß **Oppsettstips**: Hvis du bruker GitHub Codespaces, er du heldig ‚Äì `GITHUB_TOKEN` er allerede satt opp for deg! Jobber du lokalt? Ikke noe problem, du m√• bare lage et personlig tilgangstoken med riktige tillatelser.

**Forventet utdata:**
```text
The capital of France is Paris.
```

```mermaid
sequenceDiagram
    participant App as Din Python-app
    participant LC as LangChain
    participant GM as GitHub-modeller
    participant AI as GPT-4o-mini
    
    App->>LC: llm.invoke("Hva er hovedstaden i Frankrike?")
    LC->>GM: HTTP-foresp√∏rsel med prompt
    GM->>AI: Behandle prompt
    AI->>GM: Generert svar
    GM->>LC: Returner svar
    LC->>App: response.content
```
## Bygge samtaleorientert AI

Det f√∏rste eksempelet viser det grunnleggende, men det er bare en enkel utveksling ‚Äì du stiller et sp√∏rsm√•l, f√•r et svar, og det er det. I virkelige applikasjoner vil du at AI-en skal huske hva dere har diskutert, slik Watson og Holmes bygde sine etterforskningssamtaler over tid.

Her blir LangChain spesielt nyttig. Det tilbyr ulike meldingstyper som hjelper til med √• strukturere samtaler og lar deg gi AI-en en personlighet. Du vil bygge chatteopplevelser som opprettholder kontekst og karakter.

### Forst√• meldingstyper

Tenk p√• disse meldingstypene som forskjellige ‚Äúhatter‚Äù deltakerne har i en samtale. LangChain bruker ulike meldingsklasser for √• holde styr p√• hvem som sier hva:

| Meldingstype | Form√•l | Eksempelbruk |
|--------------|---------|--------------|
| `SystemMessage` | Definerer AI-personlighet og oppf√∏rsel | "Du er en hjelpsom kodeassistent" |
| `HumanMessage` | Representerer brukerinput | "Forklar hvordan funksjoner fungerer" |
| `AIMessage` | Lagrer AI-svar | Tidligere AI-responser i samtalen |

### Lage din f√∏rste samtale

La oss lage en samtale der AI-en antar en bestemt rolle. Vi lar den v√¶re Kaptein Picard ‚Äì en karakter kjent for sin diplomatiske visdom og lederskap:

```python
messages = [
    SystemMessage(content="You are Captain Picard of the Starship Enterprise"),
    HumanMessage(content="Tell me about you"),
]
```

**Slik bryter vi ned oppsettet for denne samtalen:**
- **Etablerer** AI-ens rolle og personlighet via `SystemMessage`
- **Gir** den innledende brukerforesp√∏rselen via `HumanMessage`
- **Lager** et grunnlag for samtaler som g√•r over flere runder

Den fullstendige koden for dette eksempelet ser slik ut:

```python
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_openai import ChatOpenAI
import os

llm = ChatOpenAI(
    api_key=os.environ["GITHUB_TOKEN"],
    base_url="https://models.github.ai/inference",
    model="openai/gpt-4o-mini",
)

messages = [
    SystemMessage(content="You are Captain Picard of the Starship Enterprise"),
    HumanMessage(content="Tell me about you"),
]


# fungerer
response  = llm.invoke(messages)
print(response.content)
```

Du b√∏r f√• et resultat som ligner dette:

```text
I am Captain Jean-Luc Picard, the commanding officer of the USS Enterprise (NCC-1701-D), a starship in the United Federation of Planets. My primary mission is to explore new worlds, seek out new life and new civilizations, and boldly go where no one has gone before. 

I believe in the importance of diplomacy, reason, and the pursuit of knowledge. My crew is diverse and skilled, and we often face challenges that test our resolve, ethics, and ingenuity. Throughout my career, I have encountered numerous species, grappled with complex moral dilemmas, and have consistently sought peaceful solutions to conflicts.

I hold the ideals of the Federation close to my heart, believing in the importance of cooperation, understanding, and respect for all sentient beings. My experiences have shaped my leadership style, and I strive to be a thoughtful and just captain. How may I assist you further?
```

For √• opprettholde kontinuitet i samtalen (i stedet for √• nullstille konteksten hver gang), m√• du fortsette √• legge til svar i meldingslisten din. Som muntlige tradisjoner som bevarte historier gjennom generasjoner, bygger denne tiln√¶rmingen varig minne:

```python
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_openai import ChatOpenAI
import os

llm = ChatOpenAI(
    api_key=os.environ["GITHUB_TOKEN"],
    base_url="https://models.github.ai/inference",
    model="openai/gpt-4o-mini",
)

messages = [
    SystemMessage(content="You are Captain Picard of the Starship Enterprise"),
    HumanMessage(content="Tell me about you"),
]


# fungerer
response  = llm.invoke(messages)

print(response.content)

print("---- Next ----")

messages.append(response)
messages.append(HumanMessage(content="Now that I know about you, I'm Chris, can I be in your crew?"))

response  = llm.invoke(messages)

print(response.content)

```

Kult, ikke sant? Det som skjer her er at vi kaller LLM to ganger ‚Äì f√∏rst med bare de to innledende meldingene v√•re, men s√• igjen med hele samtalehistorikken. Det er som om AI-en faktisk f√∏lger med p√• samtalen v√•r!

N√•r du kj√∏rer denne koden, vil du f√• et andre svar som h√∏res slik ut:

```text
Welcome aboard, Chris! It's always a pleasure to meet those who share a passion for exploration and discovery. While I cannot formally offer you a position on the Enterprise right now, I encourage you to pursue your aspirations. We are always in need of talented individuals with diverse skills and backgrounds. 

If you are interested in space exploration, consider education and training in the sciences, engineering, or diplomacy. The values of curiosity, resilience, and teamwork are crucial in Starfleet. Should you ever find yourself on a starship, remember to uphold the principles of the Federation: peace, understanding, and respect for all beings. Your journey can lead you to remarkable adventures, whether in the stars or on the ground. Engage!
```

```mermaid
sequenceDiagram
    participant User
    participant App
    participant LangChain
    participant AI
    
    User->>App: "Fortell meg om deg selv"
    App->>LangChain: [SystemMessage, HumanMessage]
    LangChain->>AI: Formaterte samtale
    AI->>LangChain: Captain Picard svar
    LangChain->>App: AIMessage objekt
    App->>User: Vis svar
    
    Note over App: Legg til AIMessage i samtalen
    
    User->>App: "Kan jeg bli med i mannskapet ditt?"
    App->>LangChain: [SystemMessage, HumanMessage, AIMessage, HumanMessage]
    LangChain->>AI: Full samtalekontekst
    AI->>LangChain: Kontekstuell respons
    LangChain->>App: Ny AIMessage
    App->>User: Vis kontekstuell respons
```
Jeg tar det som et kanskje ;)

## Streaming av svar

Har du lagt merke til hvordan ChatGPT ser ut til √• ‚Äúskrive‚Äù sine svar i sanntid? Det er streaming i aksjon. Som √• se en dyktig kalligraf jobbe ‚Äì se tegnene dukke opp strek for strek i stedet for √• materialisere seg umiddelbart ‚Äì gj√∏r streaming interaksjonen mer naturlig og gir umiddelbar tilbakemelding.

### Implementere streaming med LangChain

```python
from langchain_openai import ChatOpenAI
import os

llm = ChatOpenAI(
    api_key=os.environ["GITHUB_TOKEN"],
    base_url="https://models.github.ai/inference",
    model="openai/gpt-4o-mini",
    streaming=True
)

# Str√∏m svaret
for chunk in llm.stream("Write a short story about a robot learning to code"):
    print(chunk.content, end="", flush=True)
```

**Hvorfor streaming er fantastisk:**
- **Viser** innhold mens det blir laget ‚Äì ikke mer pinlig venting!
- **F√•r** brukerne til √• f√∏le at noe faktisk skjer
- **F√∏les** raskere, selv om det teknisk sett ikke er det
- **Lar** brukere begynne √• lese mens AI-en fortsatt ‚Äútenker‚Äù

> üí° **Brukeropplevelsestips**: Streaming skinner spesielt n√•r du h√•ndterer lengre svar som kodeforklaringer, kreativ skriving eller detaljerte veiledninger. Brukerne dine vil elske √• se fremdriften i stedet for √• stirre p√• en tom skjerm!

### üéØ Pedagogisk sjekkpunkt: Fordeler med rammeverksabstraksjon

**Pause og refleksjon**: Du har nettopp opplevd kraften i AI-rammeverksabstraksjoner. Sammenlign det du har l√¶rt med r√• API-kall fra tidligere leksjoner.

**Rask selvvurdering**:
- Kan du forklare hvordan LangChain forenkler samtaleh√•ndtering sammenlignet med manuell sporing av meldinger?
- Hva er forskjellen mellom `invoke()` og `stream()`-metodene, og n√•r ville du brukt hver?
- Hvordan forbedrer rammeverkets meldingssystem kodeorganiseringen?

**Virkelighetskobling**: Abstraksjonsm√∏nstrene du har l√¶rt (meldingstyper, streaming-grensesnitt, samtaleminne) brukes i alle store AI-applikasjoner ‚Äì fra ChatGPTs grensesnitt til GitHub Copilots kodeassistanse. Du mestrer de samme arkitektoniske m√∏nstrene som profesjonelle AI-utviklingsteam bruker.

**Utfordringssp√∏rsm√•l**: Hvordan ville du designet en rammeverksabstraksjon for h√•ndtering av ulike AI-modellleverand√∏rer (OpenAI, Anthropic, Google) med ett enkelt grensesnitt? Tenk p√• fordeler og ulemper.

## Prompt-maler

Prompt-maler fungerer som retoriske strukturer brukt i klassisk talekunst ‚Äì tenk p√• hvordan Cicero tilpasset tale-m√∏nstrene sine for ulike publikum mens han beholdt det samme overbevisende rammeverket. De gj√∏r at du kan lage gjenbrukbare prompts hvor du kan bytte ut forskjellige informasjonsbiter uten √• skrive alt p√• nytt. N√•r du har satt opp malen, fyller du bare inn variablene med de verdiene du trenger.

### Lage gjenbrukbare prompts

```python
from langchain_core.prompts import ChatPromptTemplate

# Definer en mal for kodeforklaringer
template = ChatPromptTemplate.from_messages([
    ("system", "You are an expert programming instructor. Explain concepts clearly with examples."),
    ("human", "Explain {concept} in {language} with a practical example for {skill_level} developers")
])

# Bruk malen med forskjellige verdier
questions = [
    {"concept": "functions", "language": "JavaScript", "skill_level": "beginner"},
    {"concept": "classes", "language": "Python", "skill_level": "intermediate"},
    {"concept": "async/await", "language": "JavaScript", "skill_level": "advanced"}
]

for question in questions:
    prompt = template.format_messages(**question)
    response = llm.invoke(prompt)
    print(f"Topic: {question['concept']}\n{response.content}\n---\n")
```

**Hvorfor du vil elske √• bruke maler:**
- **Holder** promptene dine konsistente gjennom hele appen
- **Ingen flere** rotete strengsammensetninger ‚Äì bare rene, enkle variabler
- **AI-en din** oppf√∏rer seg forutsigbart fordi strukturen forblir den samme
- **Oppdateringer** er en lek ‚Äì endre malen √©n gang, s√• er det fikset overalt

## Strukturert output

Har du noen gang blitt frustrert over √• pr√∏ve √• tolke AI-responser som kommer som ustrukturert tekst? Strukturert output er som √• l√¶re AI-en √• f√∏lge den systematiske tiln√¶rmingen som Linn√© brukte for biologisk klassifisering ‚Äì organisert, forutsigbar og enkel √• jobbe med. Du kan be om JSON, spesifikke datastrukturer eller hvilket som helst format du trenger.

### Definere output-skjemaer

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser
from pydantic import BaseModel, Field

class CodeReview(BaseModel):
    score: int = Field(description="Code quality score from 1-10")
    strengths: list[str] = Field(description="List of code strengths")
    improvements: list[str] = Field(description="List of suggested improvements")
    overall_feedback: str = Field(description="Summary feedback")

# Sett opp parseren
parser = JsonOutputParser(pydantic_object=CodeReview)

# Lag prompt med formatinstruksjoner
prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a code reviewer. {format_instructions}"),
    ("human", "Review this code: {code}")
])

# Formater prompten med instruksjoner
chain = prompt | llm | parser

# F√• strukturert respons
code_sample = """
def calculate_average(numbers):
    return sum(numbers) / len(numbers)
"""

result = chain.invoke({
    "code": code_sample,
    "format_instructions": parser.get_format_instructions()
})

print(f"Score: {result['score']}")
print(f"Strengths: {', '.join(result['strengths'])}")
```

**Hvorfor strukturert output er en stor forandring:**
- **Slutt p√•** √• gjette hvilket format du f√•r tilbake ‚Äì det er konsekvent hver gang
- **Kan kobles** direkte til databaser og API-er uten ekstra arbeid
- **Fanger opp** rare AI-responser f√∏r de √∏delegger appen din
- **Gj√∏r** koden din renere fordi du vet n√∏yaktig hva du jobber med

## Verkt√∏ysamtaler

N√• kommer vi til en av de kraftigste funksjonene: verkt√∏y. Dette er hvordan du gir AI-en din praktiske muligheter utover samtaler. Som de middelalderske laugene utviklet spesialiserte verkt√∏y for spesifikke h√•ndverk, kan du utstyre AI-en din med fokuserte instrumenter. Du beskriver hvilke verkt√∏y som er tilgjengelige, og n√•r noen ber om noe som matcher, kan AI-en ta aff√¶re.

### Bruke Python

La oss legge til noen verkt√∏y slik:

```python
from typing_extensions import Annotated, TypedDict

class add(TypedDict):
    """Add two integers."""

    # Anmerkninger m√• ha typen og kan valgfritt inkludere en standardverdi og beskrivelse (i den rekkef√∏lgen).
    a: Annotated[int, ..., "First integer"]
    b: Annotated[int, ..., "Second integer"]

tools = [add]

functions = {
    "add": lambda a, b: a + b
}
```

Hva skjer her? Vi lager en mal for et verkt√∏y kalt `add`. Ved √• arve fra `TypedDict` og bruke de fancy `Annotated`-typene for `a` og `b`, gir vi LLM et klart bilde av hva dette verkt√∏yet gj√∏r og hva det trenger. `functions`-ordboken er som verkt√∏yskassa v√•r ‚Äì den forteller koden akkurat hva den skal gj√∏re n√•r AI-en bestemmer seg for √• bruke et bestemt verkt√∏y.

La oss se hvordan vi s√• kaller LLM med dette verkt√∏yet:

```python
llm = ChatOpenAI(
    api_key=os.environ["GITHUB_TOKEN"],
    base_url="https://models.github.ai/inference",
    model="openai/gpt-4o-mini",
)

llm_with_tools = llm.bind_tools(tools)
```

Her kaller vi `bind_tools` med `tools`-arrayet v√•rt, og dermed har LLM `llm_with_tools` n√• kunnskap om dette verkt√∏yet.

For √• bruke denne nye LLM kan vi skrive f√∏lgende kode:

```python
query = "What is 3 + 12?"

res = llm_with_tools.invoke(query)
if(res.tool_calls):
    for tool in res.tool_calls:
        print("TOOL CALL: ", functions[tool["name"]](../../../10-ai-framework-project/**tool["args"]))
print("CONTENT: ",res.content)
```

N√•r vi n√• kaller `invoke` p√• denne nye llm, som har verkt√∏y, f√•r vi kanskje eiendommen `tool_calls` fylt ut. Hvis s√•, har alle identifiserte verkt√∏y en `name` og `args`-egenskap som identifiserer hvilket verkt√∏y som skal kalles og med hvilke argumenter. Den fullstendige koden ser slik ut:

```python
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_openai import ChatOpenAI
import os
from typing_extensions import Annotated, TypedDict

class add(TypedDict):
    """Add two integers."""

    # Anmerkninger m√• ha typen og kan valgfritt inkludere en standardverdi og beskrivelse (i den rekkef√∏lgen).
    a: Annotated[int, ..., "First integer"]
    b: Annotated[int, ..., "Second integer"]

tools = [add]

functions = {
    "add": lambda a, b: a + b
}

llm = ChatOpenAI(
    api_key=os.environ["GITHUB_TOKEN"],
    base_url="https://models.github.ai/inference",
    model="openai/gpt-4o-mini",
)

llm_with_tools = llm.bind_tools(tools)

query = "What is 3 + 12?"

res = llm_with_tools.invoke(query)
if(res.tool_calls):
    for tool in res.tool_calls:
        print("TOOL CALL: ", functions[tool["name"]](../../../10-ai-framework-project/**tool["args"]))
print("CONTENT: ",res.content)
```

N√•r du kj√∏rer denne koden, b√∏r du se utdata som ligner p√•:

```text
TOOL CALL:  15
CONTENT: 
```

AI-en unders√∏kte "Hva er 3 + 12" og gjenkjente dette som en oppgave for `add`-verkt√∏yet. Som en dyktig bibliotekar som vet hvilken referanse man skal konsultere basert p√• sp√∏rsm√•lets karakter, gjorde det denne vurderingen ut fra verkt√∏yets navn, beskrivelse og feltspecifikasjoner. Resultatet 15 kommer fra v√•r `functions`-ordbok som utf√∏rer verkt√∏yet:

```python
print("TOOL CALL: ", functions[tool["name"]](../../../10-ai-framework-project/**tool["args"]))
```

### Et mer interessant verkt√∏y som kaller et web-API
√Ö legge til tall demonstrerer konseptet, men ekte verkt√∏y utf√∏rer vanligvis mer komplekse operasjoner, som √• kalle web-APIer. La oss utvide eksemplet v√•rt til √• la AI hente innhold fra internett ‚Äì p√• samme m√•te som telegrafoperat√∏rer en gang koblet sammen fjerne steder:

```python
class joke(TypedDict):
    """Tell a joke."""

    # Anmerkninger m√• ha typen og kan valgfritt inkludere en standardverdi og beskrivelse (i den rekkef√∏lgen).
    category: Annotated[str, ..., "The joke category"]

def get_joke(category: str) -> str:
    response = requests.get(f"https://api.chucknorris.io/jokes/random?category={category}", headers={"Accept": "application/json"})
    if response.status_code == 200:
        return response.json().get("value", f"Here's a {category} joke!")
    return f"Here's a {category} joke!"

functions = {
    "add": lambda a, b: a + b,
    "joke": lambda category: get_joke(category)
}

query = "Tell me a joke about animals"

# resten av koden er den samme
```

N√•, hvis du kj√∏rer denne koden, vil du f√• et svar som sier noe som:

```text
TOOL CALL:  Chuck Norris once rode a nine foot grizzly bear through an automatic car wash, instead of taking a shower.
CONTENT:  
```

```mermaid
flowchart TD
    A[Brukersp√∏rsm√•l: "Fortell meg en vits om dyr"] --> B[LangChain Analyse]
    B --> C{Verkt√∏y tilgjengelig?}
    C -->|Ja| D[Velg vitsverkt√∏y]
    C -->|Nei| E[Generer direkte svar]
    
    D --> F[Ekstraher parametere]
    F --> G[Ring vits(kategori="dyr")]
    G --> H[API-foresp√∏rsel til chucknorris.io]
    H --> I[Returner vitsinnhold]
    I --> J[Vis til bruker]
    
    E --> K[AI-generert svar]
    K --> J
    
    subgraph "Verkt√∏ydefinisjonslag"
        L[TypedDict-skjema]
        M[Funksjonsimplementasjon]
        N[Parameterverifisering]
    end
    
    D --> L
    F --> N
    G --> M
```
Her er koden i sin helhet:

```python
from langchain_openai import ChatOpenAI
import requests
import os
from typing_extensions import Annotated, TypedDict

class add(TypedDict):
    """Add two integers."""

    # Anmerkninger m√• ha typen og kan valgfritt inkludere en standardverdi og beskrivelse (i den rekkef√∏lgen).
    a: Annotated[int, ..., "First integer"]
    b: Annotated[int, ..., "Second integer"]

class joke(TypedDict):
    """Tell a joke."""

    # Anmerkninger m√• ha typen og kan valgfritt inkludere en standardverdi og beskrivelse (i den rekkef√∏lgen).
    category: Annotated[str, ..., "The joke category"]

tools = [add, joke]

def get_joke(category: str) -> str:
    response = requests.get(f"https://api.chucknorris.io/jokes/random?category={category}", headers={"Accept": "application/json"})
    if response.status_code == 200:
        return response.json().get("value", f"Here's a {category} joke!")
    return f"Here's a {category} joke!"

functions = {
    "add": lambda a, b: a + b,
    "joke": lambda category: get_joke(category)
}

llm = ChatOpenAI(
    api_key=os.environ["GITHUB_TOKEN"],
    base_url="https://models.github.ai/inference",
    model="openai/gpt-4o-mini",
)

llm_with_tools = llm.bind_tools(tools)

query = "Tell me a joke about animals"

res = llm_with_tools.invoke(query)
if(res.tool_calls):
    for tool in res.tool_calls:
        # print("VERKT√òYRING: ", tool)
        print("TOOL CALL: ", functions[tool["name"]](../../../10-ai-framework-project/**tool["args"]))
print("CONTENT: ",res.content)
```

## Embeddings og dokumentbehandling

Embeddings representerer en av de mest elegante l√∏sningene innen moderne AI. Forestill deg om du kunne ta et hvilket som helst tekstutdrag og konvertere det til numeriske koordinater som fanger meningen. Det er akkurat det embeddings gj√∏r ‚Äì de transformerer tekst til punkter i et flerdimensjonalt rom hvor lignende konsepter samler seg. Det er som √• ha et koordinatsystem for ideer, p√• samme m√•te som Mendelejev organiserte det periodiske system etter atom√¶re egenskaper.

### Lage og bruke embeddings

```python
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import CharacterTextSplitter

# Initialiser innebygde representasjoner
embeddings = OpenAIEmbeddings(
    api_key=os.environ["GITHUB_TOKEN"],
    base_url="https://models.github.ai/inference",
    model="text-embedding-3-small"
)

# Last inn og del opp dokumenter
loader = TextLoader("documentation.txt")
documents = loader.load()

text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
texts = text_splitter.split_documents(documents)

# Opprett vektorlagring
vectorstore = FAISS.from_documents(texts, embeddings)

# Utf√∏r likhetss√∏k
query = "How do I handle user authentication?"
similar_docs = vectorstore.similarity_search(query, k=3)

for doc in similar_docs:
    print(f"Relevant content: {doc.page_content[:200]}...")
```

### Dokumentinnlastere for ulike formater

```python
from langchain_community.document_loaders import (
    PyPDFLoader,
    CSVLoader,
    JSONLoader,
    WebBaseLoader
)

# Last inn forskjellige dokumenttyper
pdf_loader = PyPDFLoader("manual.pdf")
csv_loader = CSVLoader("data.csv")
json_loader = JSONLoader("config.json")
web_loader = WebBaseLoader("https://example.com/docs")

# Behandle alle dokumenter
all_documents = []
for loader in [pdf_loader, csv_loader, json_loader, web_loader]:
    docs = loader.load()
    all_documents.extend(docs)
```

**Hva du kan gj√∏re med embeddings:**
- **Bygge** s√∏k som faktisk forst√•r hva du mener, ikke bare n√∏kkelordmatching
- **Skape** AI som kan svare p√• sp√∏rsm√•l om dokumentene dine
- **Lage** anbefalingssystemer som foresl√•r virkelig relevant innhold
- **Automatisk** organisere og kategorisere innholdet ditt

```mermaid
flowchart LR
    A[Dokumenter] --> B[Tekstdeler]
    B --> C[Opprett Innleiring]
    C --> D[Vektorbutikk]
    
    E[Brukersp√∏rsm√•l] --> F[Sp√∏rringsinnleiring]
    F --> G[Likhetss√∏k]
    G --> D
    D --> H[Relevante Dokumenter]
    H --> I[AI-svar]
    
    subgraph "Vektorrom"
        J[Dokument A: [0.1, 0.8, 0.3...]]
        K[Dokument B: [0.2, 0.7, 0.4...]]
        L[Sp√∏rring: [0.15, 0.75, 0.35...]]
    end
    
    C --> J
    C --> K
    F --> L
    G --> J
    G --> K
```
## Bygge en komplett AI-applikasjon

N√• skal vi integrere alt du har l√¶rt til en omfattende applikasjon ‚Äì en kodeassistent som kan svare p√• sp√∏rsm√•l, bruke verkt√∏y og holde samtaleminne. P√• samme m√•te som boktrykkerkunsten kombinerte eksisterende teknologier (l√∏stst√•ende typer, blekk, papir og trykk) til noe revolusjonerende, skal vi kombinere AI-komponentene v√•re til noe praktisk og nyttig.

### Eksempel p√• komplett applikasjon

```python
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage
from langchain_community.vectorstores import FAISS
from typing_extensions import Annotated, TypedDict
import os
import requests

class CodingAssistant:
    def __init__(self):
        self.llm = ChatOpenAI(
            api_key=os.environ["GITHUB_TOKEN"],
            base_url="https://models.github.ai/inference",
            model="openai/gpt-4o-mini"
        )
        
        self.conversation_history = [
            SystemMessage(content="""You are an expert coding assistant. 
            Help users learn programming concepts, debug code, and write better software.
            Use tools when needed and maintain a helpful, encouraging tone.""")
        ]
        
        # Definer verkt√∏y
        self.setup_tools()
    
    def setup_tools(self):
        class web_search(TypedDict):
            """Search for programming documentation or examples."""
            query: Annotated[str, "Search query for programming help"]
        
        class code_formatter(TypedDict):
            """Format and validate code snippets."""
            code: Annotated[str, "Code to format"]
            language: Annotated[str, "Programming language"]
        
        self.tools = [web_search, code_formatter]
        self.llm_with_tools = self.llm.bind_tools(self.tools)
    
    def chat(self, user_input: str):
        # Legg til brukermelding i samtalen
        self.conversation_history.append(HumanMessage(content=user_input))
        
        # Hent AI-svar
        response = self.llm_with_tools.invoke(self.conversation_history)
        
        # H√•ndter verkt√∏y kall hvis noen
        if response.tool_calls:
            for tool_call in response.tool_calls:
                tool_result = self.execute_tool(tool_call)
                print(f"üîß Tool used: {tool_call['name']}")
                print(f"üìä Result: {tool_result}")
        
        # Legg til AI-svar i samtalen
        self.conversation_history.append(response)
        
        return response.content
    
    def execute_tool(self, tool_call):
        tool_name = tool_call['name']
        args = tool_call['args']
        
        if tool_name == 'web_search':
            return f"Found documentation for: {args['query']}"
        elif tool_name == 'code_formatter':
            return f"Formatted {args['language']} code: {args['code'][:50]}..."
        
        return "Tool execution completed"

# Brukseksempel
assistant = CodingAssistant()

print("ü§ñ Coding Assistant Ready! Type 'quit' to exit.\n")

while True:
    user_input = input("You: ")
    if user_input.lower() == 'quit':
        break
    
    response = assistant.chat(user_input)
    print(f"ü§ñ Assistant: {response}\n")
```

**Applikasjonsarkitektur:**

```mermaid
graph TD
    A[Brukerinput] --> B[Kodeassistent]
    B --> C[Samtaleminne]
    B --> D[Verkt√∏ydeteksjon]
    B --> E[LLM-behandling]
    
    D --> F[Verkt√∏y for netts√∏k]
    D --> G[Verkt√∏y for kodeformatering]
    
    E --> H[Svargenerering]
    F --> H
    G --> H
    
    H --> I[Brukergrensesnitt]
    H --> C
```
**N√∏kkelfunksjoner vi har implementert:**
- **Husker** hele samtalen din for kontinuitet i kontekst
- **Utf√∏rer handlinger** gjennom verkt√∏ysanrop, ikke bare samtale
- **F√∏lger** forutsigbare interaksjonsm√∏nstre
- **H√•ndterer** feilsituasjoner og komplekse arbeidsflyter automatisk

### üéØ Pedagogisk sjekkpunkt: Produksjons-AI-arkitektur

**Forst√•else av arkitektur**: Du har bygget en komplett AI-applikasjon som kombinerer samtaleh√•ndtering, verkt√∏ysanrop og strukturerte arbeidsflyter. Dette representerer produksjonsniv√• utvikling av AI-applikasjoner.

**N√∏kkelkonsepter behersket**:
- **Klassebasert arkitektur**: Organisert, vedlikeholdbar AI-applikasjonsstruktur
- **Verkt√∏yintegrasjon**: Egendefinert funksjonalitet utover samtale
- **Minnestyring**: Vedvarende samtalekontekst
- **Feilh√•ndtering**: Robust applikasjonsadferd

**Bransjetilknytning**: Arkitekturm√∏nstrene du har implementert (samtaleklasser, verkt√∏ysystemer, minnestyring) er de samme m√∏nstrene som brukes i bedrifts-AI-applikasjoner som Slack sin AI-assistent, GitHub Copilot og Microsoft Copilot. Du bygger med profesjonell, arkitektonisk tankegang.

**Refleksjonssp√∏rsm√•l**: Hvordan ville du utvidet denne applikasjonen for √• h√•ndtere flere brukere, vedvarende lagring eller integrasjon med eksterne databaser? Vurder utfordringer med skalerbarhet og tilstandsstyring.

## Oppgave: Bygg din egen AI-drevne studieassistent

**M√•l**: Lag en AI-applikasjon som hjelper studenter med √• l√¶re programmeringskonsepter ved √• gi forklaringer, kodeeksempler og interaktive quizer.

### Krav

**Kjernefunksjoner (p√•krevd):**
1. **Samtalegrensesnitt**: Implementer et chatsystem som opprettholder kontekst over flere sp√∏rsm√•l
2. **Utdanningsverkt√∏y**: Lag minst to verkt√∏y som hjelper med l√¶ring:
   - Verkt√∏y for kodeforklaring
   - Quizgenerator for konsepter
3. **Personlig tilpasning**: Bruk systemmeldinger for √• tilpasse svar til ulike ferdighetsniv√•er
4. **Responsformattering**: Implementer strukturert output for quizsp√∏rsm√•l

### Implementeringstrinn

**Trinn 1: Sett opp milj√∏et ditt**
```bash
pip install langchain langchain-openai
```

**Trinn 2: Grunnleggende chatfunksjonalitet**
- Lag en `StudyAssistant`-klasse
- Implementer samtaleminne
- Legg til personlighet for utdanningsst√∏tte

**Trinn 3: Legg til utdanningsverkt√∏y**
- **Kodeforklarer**: Bryter ned kode i forst√•elige deler
- **Quizgenerator**: Lager sp√∏rsm√•l om programmeringskonsepter
- **Fremdriftssporer**: Holder oversikt over omfang av temaer

**Trinn 4: Forbedrede funksjoner (valgfritt)**
- Implementer str√∏mming av svar for bedre brukeropplevelse
- Legg til dokumentinnlasting for √• inkludere kursmateriale
- Lag embeddings for innholdss√∏k basert p√• likhet

### Evalueringskriterier

| Funksjon | Utmerket (4) | God (3) | Tilfredsstillende (2) | Trenger forbedring (1) |
|---------|---------------|----------|------------------|----------------|
| **Samtaleflyt** | Naturlige, kontekstbevisste svar | God kontekstbevaring | Enkel samtale | Ingen minne mellom utvekslinger |
| **Verkt√∏yintegrasjon** | Flere nyttige verkt√∏y fungerer s√∏ml√∏st | 2+ verkt√∏y implementert riktig | 1-2 enkle verkt√∏y | Verkt√∏y ikke funksjonelle |
| **Kodekvalitet** | Ren, godt dokumentert, feilh√•ndtering | God struktur, noe dokumentasjon | Grunnleggende funksjonalitet fungerer | D√•rlig struktur, ingen feilh√•ndtering |
| **Utdanningsverdi** | Virkelig hjelpsom for l√¶ring, adaptiv | God l√¶ringsst√∏tte | Grunnleggende forklaringer | Begrenset l√¶rerik verdi |

### Eksempelkodestruktur

```python
class StudyAssistant:
    def __init__(self, skill_level="beginner"):
        # Initialiser LLM, verkt√∏y og samtaleminne
        pass
    
    def explain_code(self, code, language):
        # Verkt√∏y: Forklar hvordan kode fungerer
        pass
    
    def generate_quiz(self, topic, difficulty):
        # Verkt√∏y: Lag √∏velsessp√∏rsm√•l
        pass
    
    def chat(self, user_input):
        # Hovedsamtalegrensesnitt
        pass

# Eksempel p√• bruk
assistant = StudyAssistant(skill_level="intermediate")
response = assistant.chat("Explain how Python functions work")
```

**Bonusutfordringer:**
- Legg til stemmeinn- og utgang
- Lag et webgrensesnitt med Streamlit eller Flask
- Lag en kunnskapsbase fra kursmateriale ved bruk av embeddings
- Legg til fremdriftssporing og personlig l√¶ringssti

## üìà Din tidslinje for mestring av AI-rammeverksutvikling

```mermaid
timeline
    title Produksjon AI Rammeverk Utviklingsreise
    
    section Rammeverk Grunnlag
        Forst√• Abstraksjoner
            : Mestre rammeverk vs API beslutninger
            : L√¶re LangChain kjernebegreper
            : Implementere meldingstype systemer
        
        Grunnleggende Integrasjon
            : Koble til AI-leverand√∏rer
            : H√•ndtere autentisering
            : Administrere konfigurasjon
    
    section Samtale Systemer
        Minneh√•ndtering
            : Bygge samtalehistorikk
            : Implementere kontekstsporing
            : H√•ndtere √∏kt-persistens
        
        Avanserte Interaksjoner
            : Mestre str√∏mmende svar
            : Lage promptmaler
            : Implementere strukturert utdata
    
    section Verkt√∏y Integrasjon
        Egendefinert Verkt√∏yutvikling
            : Designe verkt√∏yskjemaer
            : Implementere funksjonskalling
            : H√•ndtere eksterne APIer
        
        Arbeidsflyt Automatisering
            : Koble sammen flere verkt√∏y
            : Lage beslutningstr√¶r
            : Bygge agentatferd
    
    section Produksjonsapplikasjoner
        Komplett Systemarkitektur
            : Kombinere alle rammeverksfunksjoner
            : Implementere feilsikringsgrenser
            : Lage vedlikeholdbar kode
        
        Bedriftsberedskap
            : H√•ndtere skalerbarhetsbekymringer
            : Implementere overv√•king
            : Bygge distribusjonsstrategier
```
**üéì Avslutningsmilep√¶l**: Du har med suksess mestret AI-rammeverksutvikling ved √• bruke de samme verkt√∏yene og m√∏nstrene som driver moderne AI-applikasjoner. Disse ferdighetene representerer det ytterste innen AI-applikasjonsutvikling og forbereder deg p√• √• bygge intelligente systemer p√• bedriftsniv√•.

**üîÑ Neste niv√• kapasiteter**:
- Klar for √• utforske avanserte AI-arkitekturer (agenter, multi-agent systemer)
- Forberedt p√• √• bygge RAG-systemer med vektordatabaser
- Utstyrt for √• skape multimodale AI-applikasjoner
- Fundamentet lagt for skalering og optimalisering av AI-applikasjoner

## Oppsummering

üéâ Du har n√• mestret grunnprinsippene i AI-rammeverksutvikling og l√¶rt hvordan man bygger sofistikerte AI-applikasjoner med LangChain. Som √• fullf√∏re en omfattende l√¶retid, har du skaffet deg en solid verkt√∏ykasse med ferdigheter. La oss g√• gjennom hva du har oppn√•dd.

### Hva du har l√¶rt

**Kjerneprinsipper i rammeverk:**
- **Fordeler med rammeverk**: Forst√• n√•r det er best √• bruke rammeverk fremfor direkte API-kall
- **LangChain-grunnleggende**: Setup og konfigurasjon av AI-modellforbindelser
- **Meldingstyper**: Bruke `SystemMessage`, `HumanMessage` og `AIMessage` for strukturerte samtaler

**Avanserte funksjoner:**
- **Verkt√∏ysanrop**: Lage og integrere egendefinerte verkt√∏y for utvidet AI-funksjonalitet
- **Samtaleminne**: Opprettholde kontekst over flere samtalerunder
- **Streaming-respons**: Implementere responslevering i sanntid
- **Prompt-maler**: Bygge gjenbrukbare, dynamiske prompts
- **Strukturert output**: Sikre konsistente, maskinlesbare AI-svar
- **Embeddings**: Lage semantisk s√∏k og dokumentbehandlingsfunksjoner

**Praktiske bruksomr√•der:**
- **Bygge komplette apper**: Kombinere flere funksjoner til produksjonsklare apper
- **Feilh√•ndtering**: Implementere robust feilbehandling og validering
- **Verkt√∏yintegrasjon**: Lage egendefinerte verkt√∏y som utvider AI-evner

### Viktige poeng

> üéØ **Husk**: AI-rammeverk som LangChain er dine beste venner som skjuler kompleksiteten og pakker mange funksjoner inn. De er perfekte n√•r du trenger samtaleminne, verkt√∏ysanrop, eller √∏nsker √• jobbe med flere AI-modeller uten √• miste forstanden.

**Beslutningsramme for AI-integrasjon:**

```mermaid
flowchart TD
    A[Behov for AI-integrasjon] --> B{Enkelt enkeltsp√∏rsm√•l?}
    B -->|Ja| C[Direkte API-kall]
    B -->|Nei| D{Trenger samtaleminne?}
    D -->|Nei| E[SDK-integrasjon]
    D -->|Ja| F{Trenger verkt√∏y eller komplekse funksjoner?}
    F -->|Nei| G[Rammeverk med grunnleggende oppsett]
    F -->|Ja| H[Full rammeverksimplementering]
    
    C --> I[HTTP-foresp√∏rsler, minimale avhengigheter]
    E --> J[Leverand√∏r-SDK, modellspesifikk]
    G --> K[LangChain grunnleggende chat]
    H --> L[LangChain med verkt√∏y, minne, agenter]
```
### Hvor g√•r du videre herfra?

**Begynn √• bygge n√•:**
- Ta disse konseptene og skap noe som begeistrer DEG!
- Lek deg med ulike AI-modeller gjennom LangChain ‚Äì det er som en lekeplass for AI-modeller
- Lag verkt√∏y som l√∏ser virkelige problemer du m√∏ter i arbeidet eller prosjektene dine

**Klar for neste niv√•?**
- **AI-agenter**: Bygg AI-systemer som faktisk kan planlegge og utf√∏re komplekse oppgaver p√• egenh√•nd
- **RAG (Retrieval-Augmented Generation)**: Kombiner AI med egne kunnskapsbaser for superkraftige applikasjoner
- **Multimodal AI**: Jobb med tekst, bilder og lyd samlet ‚Äì mulighetene er uendelige!
- **Produksjonsdistribusjon**: L√¶r hvordan du skalerer AI-appene dine og overv√•ker dem i den virkelige verden

**Bli med i fellesskapet:**
- LangChain-fellesskapet er fantastisk for √• holde seg oppdatert og l√¶re beste praksis
- GitHub Models gir deg tilgang til banebrytende AI-funksjoner ‚Äì perfekt for eksperimentering
- Fortsett √• √∏ve med forskjellige brukstilfeller ‚Äì hvert prosjekt vil l√¶re deg noe nytt

Du har n√• kunnskapen til √• bygge intelligente, samtalebaserte applikasjoner som kan hjelpe folk med √• l√∏se reelle problemer. Som renessansemesterne som kombinerte kunstnerisk visjon med tekniske ferdigheter, kan du n√• fusjonere AI-evner med praktisk anvendelse. Sp√∏rsm√•let er: Hva vil du skape? üöÄ

## GitHub Copilot Agent Challenge üöÄ

Bruk Agent-modus for √• l√∏se f√∏lgende utfordring:

**Beskrivelse:** Bygg en avansert AI-drevet kodegjennomgangsassistent som kombinerer flere LangChain-funksjoner inkludert verkt√∏ysanrop, strukturert output og samtaleminne for √• gi omfattende tilbakemelding p√• kodeinnleveringer.

**Prompt:** Lag en CodeReviewAssistant-klasse som implementerer:
1. Et verkt√∏y for √• analysere kodekompleksitet og foresl√• forbedringer
2. Et verkt√∏y for √• sjekke kode mot beste praksis
3. Strukturert output ved bruk av Pydantic-modeller for konsistent gjennomgangsformat
4. Samtaleminne for √• spore gjennomgangs√∏kter
5. Et hoved-chatgrensesnitt som kan h√•ndtere kodeinnleveringer og gi detaljerte, handlingsorienterte tilbakemeldinger

Assistenten skal kunne gjennomg√• kode i flere programmeringsspr√•k, opprettholde kontekst p√• tvers av flere kodeinnleveringer i en √∏kt, og gi b√•de sammendragsresultater og detaljerte forbedringsforslag.

L√¶r mer om [agent mode](https://code.visualstudio.com/blogs/2025/02/24/introducing-copilot-agent-mode) her.

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**Ansvarsfraskrivelse**:
Dette dokumentet er oversatt ved hjelp av AI-oversettingstjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selv om vi streber etter n√∏yaktighet, v√¶r oppmerksom p√• at automatiske oversettelser kan inneholde feil eller un√∏yaktigheter. Det opprinnelige dokumentet p√• dets opprinnelige spr√•k b√∏r betraktes som den autoritative kilden. For kritisk informasjon anbefales profesjonell menneskelig oversettelse. Vi er ikke ansvarlige for eventuelle misforst√•elser eller feiltolkninger som oppst√•r ved bruk av denne oversettelsen.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->