# Marco de IA

¬øAlguna vez te has sentido abrumado intentando construir aplicaciones de IA desde cero? ¬°No est√°s solo! Los marcos de IA son como tener una navaja suiza para el desarrollo de IA: son herramientas poderosas que pueden ahorrarte tiempo y dolores de cabeza al crear aplicaciones inteligentes. Piensa en un marco de IA como una biblioteca bien organizada: proporciona componentes preconstruidos, API estandarizadas y abstracciones inteligentes para que puedas concentrarte en resolver problemas en lugar de lidiar con detalles de implementaci√≥n.

En esta lecci√≥n, exploraremos c√≥mo marcos como LangChain pueden convertir lo que antes eran tareas complejas de integraci√≥n de IA en c√≥digo limpio y legible. Descubrir√°s c√≥mo abordar desaf√≠os del mundo real como llevar el registro de conversaciones, implementar llamadas a herramientas y manejar diferentes modelos de IA a trav√©s de una interfaz unificada.

Para cuando terminemos, sabr√°s cu√°ndo recurrir a marcos en lugar de llamadas directas a API, c√≥mo usar sus abstracciones eficazmente y c√≥mo construir aplicaciones de IA listas para uso real. Vamos a explorar lo que los marcos de IA pueden hacer por tus proyectos.

## ‚ö° Lo que puedes hacer en los pr√≥ximos 5 minutos

**Camino de Inicio R√°pido para Desarrolladores Ocupados**

```mermaid
flowchart LR
    A[‚ö° 5 minutos] --> B[Instalar LangChain]
    B --> C[Crear cliente ChatOpenAI]
    C --> D[Enviar primer mensaje]
    D --> E[Ver el poder del framework]
```
- **Minuto 1**: Instala LangChain: `pip install langchain langchain-openai`
- **Minuto 2**: Configura tu token de GitHub e importa el cliente ChatOpenAI
- **Minuto 3**: Crea una conversaci√≥n sencilla con mensajes del sistema y del humano
- **Minuto 4**: A√±ade una herramienta b√°sica (como una funci√≥n de suma) y observa la llamada a herramientas IA
- **Minuto 5**: Experimenta la diferencia entre llamadas directas a API y abstracci√≥n del marco

**C√≥digo de prueba r√°pido**:
```python
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage

llm = ChatOpenAI(
    api_key=os.environ["GITHUB_TOKEN"],
    base_url="https://models.github.ai/inference",
    model="openai/gpt-4o-mini"
)

response = llm.invoke([
    SystemMessage(content="You are a helpful coding assistant"),
    HumanMessage(content="Explain Python functions briefly")
])
print(response.content)
```

**Por qu√© esto importa**: En 5 minutos, experimentar√°s c√≥mo los marcos de IA transforman integraciones complejas en llamadas simples a m√©todos. Esta es la base que impulsa aplicaciones de IA en producci√≥n.

## ¬øPor qu√© elegir un marco?

Entonces, est√°s listo para construir una app de IA, ¬°genial! Pero aqu√≠ est√° el asunto: tienes varias rutas que puedes tomar, y cada una tiene sus pros y sus contras. Es como elegir entre caminar, andar en bici o conducir para llegar a un lugar: todos te llevar√°n, pero la experiencia (y el esfuerzo) ser√° totalmente diferente.

Desglosamos las tres principales formas de integrar IA en tus proyectos:

| Enfoque | Ventajas | Ideal para | Consideraciones |
|----------|------------|----------|--------------|
| **Peticiones HTTP directas** | Control total, sin dependencias | Consultas simples, aprender fundamentos | C√≥digo m√°s verboso, manejo manual de errores |
| **Integraci√≥n con SDK** | Menos c√≥digo repetitivo, optimizaci√≥n espec√≠fica por modelo | Aplicaciones con un solo modelo | Limitado a proveedores espec√≠ficos |
| **Marcos de IA** | API unificada, abstracciones incorporadas | Aplicaciones multi-modelo, flujos de trabajo complejos | Curva de aprendizaje, posible sobreabstracci√≥n |

### Beneficios de los marcos en la pr√°ctica

```mermaid
graph TD
    A[Su Aplicaci√≥n] --> B[Marco de IA]
    B --> C[OpenAI GPT]
    B --> D[Anthropic Claude]
    B --> E[Modelos de GitHub]
    B --> F[Modelos Locales]
    
    B --> G[Herramientas Integradas]
    G --> H[Gesti√≥n de Memoria]
    G --> I[Historial de Conversaci√≥n]
    G --> J[Llamado de Funciones]
    G --> K[Manejo de Errores]
```
**Por qu√© importan los marcos:**
- **Unifica** m√∫ltiples proveedores de IA bajo una sola interfaz
- **Maneja** autom√°ticamente la memoria de las conversaciones
- **Proporciona** herramientas listas para tareas comunes como embeddings y llamadas a funciones
- **Gestiona** el manejo de errores y l√≥gica de reintentos
- **Convierte** flujos de trabajo complejos en llamadas a m√©todos legibles

> üí° **Consejo profesional**: Usa marcos al cambiar entre diferentes modelos de IA o construyendo caracter√≠sticas complejas como agentes, memoria o llamadas a herramientas. Usa APIs directas cuando aprendas lo b√°sico o construyas aplicaciones simples y focalizadas.

**En resumen**: como elegir entre herramientas especializadas de un artesano y un taller completo, se trata de adaptar la herramienta a la tarea. Los marcos destacan en aplicaciones complejas y ricas en funciones, mientras que las APIs directas funcionan bien para casos simples.

## üó∫Ô∏è Tu viaje de aprendizaje hacia el dominio de marcos de IA

```mermaid
journey
    title De APIs en crudo a aplicaciones de IA en producci√≥n
    section Fundamentos del marco
      Comprender beneficios de la abstracci√≥n: 4: You
      Dominar los conceptos b√°sicos de LangChain: 6: You
      Comparar enfoques: 7: You
    section Sistemas de conversaci√≥n
      Construir interfaces de chat: 5: You
      Implementar patrones de memoria: 7: You
      Manejar respuestas en streaming: 8: You
    section Funciones avanzadas
      Crear herramientas personalizadas: 6: You
      Dominar salida estructurada: 8: You
      Construir sistemas de documentos: 8: You
    section Aplicaciones de producci√≥n
      Combinar todas las funciones: 7: You
      Manejar escenarios de error: 8: You
      Desplegar sistemas completos: 9: You
```
**Destino de tu viaje**: Al final de esta lecci√≥n, habr√°s dominado el desarrollo con marcos de IA y podr√°s construir aplicaciones sofisticadas y listas para producci√≥n que compiten con asistentes comerciales de IA.

## Introducci√≥n

En esta lecci√≥n aprenderemos a:

- Usar un marco com√∫n de IA.
- Abordar problemas comunes como conversaciones de chat, uso de herramientas, memoria y contexto.
- Aprovechar esto para construir aplicaciones de IA.

## üß† Ecosistema de desarrollo de marcos de IA

```mermaid
mindmap
  root((AI Frameworks))
    Beneficios de la Abstracci√≥n
      Simplificaci√≥n del C√≥digo
        APIs Unificadas
        Manejo de Errores Integrado
        Patrones Consistentes
        Reducci√≥n de C√≥digo Repetitivo
      Soporte Multi-Modelo
        Independiente del Proveedor
        Cambio F√°cil
        Opciones de Respaldo
        Optimizaci√≥n de Costos
    Componentes Principales
      Gesti√≥n de Conversaciones
        Tipos de Mensajes
        Sistemas de Memoria
        Seguimiento de Contexto
        Persistencia de Historial
      Integraci√≥n de Herramientas
        Llamadas a Funciones
        Conexiones API
        Herramientas Personalizadas
        Automatizaci√≥n de Flujos de Trabajo
    Funciones Avanzadas
      Salida Estructurada
        Modelos Pydantic
        Esquemas JSON
        Seguridad de Tipos
        Reglas de Validaci√≥n
      Procesamiento de Documentos
        Embeddings
        Almacenes Vectoriales
        B√∫squeda por Similitud
        Sistemas RAG
    Patrones de Producci√≥n
      Arquitectura de Aplicaciones
        Dise√±o Modular
        L√≠mites de Error
        Operaciones As√≠ncronas
        Gesti√≥n de Estado
      Estrategias de Despliegue
        Escalabilidad
        Monitoreo
        Rendimiento
        Seguridad
```
**Principio central**: Los marcos de IA abstraen la complejidad mientras proveen potentes abstracciones para gesti√≥n de conversaciones, integraci√≥n de herramientas y procesamiento de documentos, permitiendo a los desarrolladores construir aplicaciones sofisticadas de IA con c√≥digo limpio y mantenible.

## Tu primer prompt de IA

Comencemos con los fundamentos creando tu primera aplicaci√≥n de IA que env√≠a una pregunta y recibe una respuesta. Como Arqu√≠medes descubriendo el principio de desplazamiento en su ba√±o, a veces las observaciones m√°s simples conducen a las ideas m√°s poderosas, y los marcos hacen que estas ideas sean accesibles.

### Configurando LangChain con GitHub Models

Vamos a usar LangChain para conectarnos a GitHub Models, lo cual es genial porque te da acceso gratuito a varios modelos de IA. ¬øLo mejor? Solo necesitas unos pocos par√°metros de configuraci√≥n para comenzar:

```python
from langchain_openai import ChatOpenAI
import os

llm = ChatOpenAI(
    api_key=os.environ["GITHUB_TOKEN"],
    base_url="https://models.github.ai/inference",
    model="openai/gpt-4o-mini",
)

# Enviar un mensaje simple
response = llm.invoke("What's the capital of France?")
print(response.content)
```

**Analicemos lo que sucede aqu√≠:**
- **Crea** un cliente LangChain usando la clase `ChatOpenAI`: ¬°es tu puerta de entrada a la IA!
- **Configura** la conexi√≥n a GitHub Models con tu token de autenticaci√≥n
- **Especifica** qu√© modelo de IA usar (`gpt-4o-mini`) ‚Äî piensa en esto como elegir tu asistente de IA
- **Env√≠a** tu pregunta mediante el m√©todo `invoke()` ‚Äî aqu√≠ ocurre la magia
- **Extrae** y muestra la respuesta ‚Äî ¬°y voil√†, est√°s chateando con IA!

> üîß **Nota de configuraci√≥n**: Si usas GitHub Codespaces, tienes suerte: ¬°el `GITHUB_TOKEN` ya est√° configurado para ti! ¬øTrabajando localmente? No te preocupes, solo necesitas crear un token de acceso personal con los permisos adecuados.

**Salida esperada:**
```text
The capital of France is Paris.
```

```mermaid
sequenceDiagram
    participant App as Tu App de Python
    participant LC as LangChain
    participant GM as GitHub Models
    participant AI as GPT-4o-mini
    
    App->>LC: llm.invoke("¬øCu√°l es la capital de Francia?")
    LC->>GM: solicitud HTTP con prompt
    GM->>AI: Procesar prompt
    AI->>GM: Respuesta generada
    GM->>LC: Devolver respuesta
    LC->>App: response.content
```
## Construyendo IA conversacional

Ese primer ejemplo muestra lo b√°sico, pero es solo un intercambio: haces una pregunta, recibes una respuesta y eso es todo. En aplicaciones reales, quieres que tu IA recuerde lo que han estado discutiendo, como Watson y Holmes constru√≠an sus conversaciones investigativas a lo largo del tiempo.

Aqu√≠ es donde LangChain se vuelve especialmente √∫til. Proporciona diferentes tipos de mensajes que ayudan a estructurar las conversaciones y permiten darle a tu IA una personalidad. Construir√°s experiencias de chat que mantienen contexto y car√°cter.

### Comprendiendo los tipos de mensajes

Piensa en estos tipos de mensajes como diferentes "sombreros" que usan los participantes en una conversaci√≥n. LangChain usa distintas clases de mensajes para registrar qui√©n dice qu√©:

| Tipo de mensaje | Prop√≥sito | Caso de uso |
|--------------|---------|------------------|
| `SystemMessage` | Define la personalidad y comportamiento de la IA | "Eres un asistente de codificaci√≥n √∫til" |
| `HumanMessage` | Representa la entrada del usuario | "Explica c√≥mo funcionan las funciones" |
| `AIMessage` | Guarda las respuestas de la IA | Respuestas previas de IA en la conversaci√≥n |

### Creando tu primera conversaci√≥n

Creamos una conversaci√≥n donde nuestra IA asume un rol espec√≠fico. Le haremos encarnar al Capit√°n Picard, personaje conocido por su sabidur√≠a diplom√°tica y liderazgo:

```python
messages = [
    SystemMessage(content="You are Captain Picard of the Starship Enterprise"),
    HumanMessage(content="Tell me about you"),
]
```

**Desglosando esta configuraci√≥n de conversaci√≥n:**
- **Establece** el rol y la personalidad de la IA mediante `SystemMessage`
- **Proporciona** la consulta inicial del usuario v√≠a `HumanMessage`
- **Crea** una base para una conversaci√≥n de m√∫ltiples turnos

El c√≥digo completo de este ejemplo es as√≠:

```python
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_openai import ChatOpenAI
import os

llm = ChatOpenAI(
    api_key=os.environ["GITHUB_TOKEN"],
    base_url="https://models.github.ai/inference",
    model="openai/gpt-4o-mini",
)

messages = [
    SystemMessage(content="You are Captain Picard of the Starship Enterprise"),
    HumanMessage(content="Tell me about you"),
]


# funciona
response  = llm.invoke(messages)
print(response.content)
```

Deber√≠as ver un resultado similar a:

```text
I am Captain Jean-Luc Picard, the commanding officer of the USS Enterprise (NCC-1701-D), a starship in the United Federation of Planets. My primary mission is to explore new worlds, seek out new life and new civilizations, and boldly go where no one has gone before. 

I believe in the importance of diplomacy, reason, and the pursuit of knowledge. My crew is diverse and skilled, and we often face challenges that test our resolve, ethics, and ingenuity. Throughout my career, I have encountered numerous species, grappled with complex moral dilemmas, and have consistently sought peaceful solutions to conflicts.

I hold the ideals of the Federation close to my heart, believing in the importance of cooperation, understanding, and respect for all sentient beings. My experiences have shaped my leadership style, and I strive to be a thoughtful and just captain. How may I assist you further?
```

Para mantener la continuidad de la conversaci√≥n (en lugar de reiniciar el contexto cada vez), necesitas seguir agregando respuestas a tu lista de mensajes. Como las tradiciones orales que preservaron historias a trav√©s de generaciones, este enfoque construye una memoria duradera:

```python
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_openai import ChatOpenAI
import os

llm = ChatOpenAI(
    api_key=os.environ["GITHUB_TOKEN"],
    base_url="https://models.github.ai/inference",
    model="openai/gpt-4o-mini",
)

messages = [
    SystemMessage(content="You are Captain Picard of the Starship Enterprise"),
    HumanMessage(content="Tell me about you"),
]


# funciona
response  = llm.invoke(messages)

print(response.content)

print("---- Next ----")

messages.append(response)
messages.append(HumanMessage(content="Now that I know about you, I'm Chris, can I be in your crew?"))

response  = llm.invoke(messages)

print(response.content)

```

Bastante genial, ¬øverdad? Lo que sucede aqu√≠ es que llamamos al LLM dos veces: primero solo con nuestros dos mensajes iniciales, pero luego de nuevo con el historial completo de la conversaci√≥n. ¬°Es como si la IA realmente estuviera siguiendo nuestro chat!

Al ejecutar este c√≥digo obtendr√°s una segunda respuesta que suena algo as√≠:

```text
Welcome aboard, Chris! It's always a pleasure to meet those who share a passion for exploration and discovery. While I cannot formally offer you a position on the Enterprise right now, I encourage you to pursue your aspirations. We are always in need of talented individuals with diverse skills and backgrounds. 

If you are interested in space exploration, consider education and training in the sciences, engineering, or diplomacy. The values of curiosity, resilience, and teamwork are crucial in Starfleet. Should you ever find yourself on a starship, remember to uphold the principles of the Federation: peace, understanding, and respect for all beings. Your journey can lead you to remarkable adventures, whether in the stars or on the ground. Engage!
```

```mermaid
sequenceDiagram
    participant User
    participant App
    participant LangChain
    participant AI
    
    User->>App: "Cu√©ntame sobre ti"
    App->>LangChain: [SystemMessage, HumanMessage]
    LangChain->>AI: Conversaci√≥n formateada
    AI->>LangChain: Respuesta del Capit√°n Picard
    LangChain->>App: Objeto AIMessage
    App->>User: Mostrar respuesta
    
    Note over App: A√±adir AIMessage a la conversaci√≥n
    
    User->>App: "¬øPuedo unirme a tu tripulaci√≥n?"
    App->>LangChain: [SystemMessage, HumanMessage, AIMessage, HumanMessage]
    LangChain->>AI: Contexto completo de la conversaci√≥n
    AI->>LangChain: Respuesta contextual
    LangChain->>App: Nuevo AIMessage
    App->>User: Mostrar respuesta contextual
```
Tomar√© eso como un tal vez ;)

## Respuestas en streaming

¬øHas notado que ChatGPT parece "escribir" sus respuestas en tiempo real? Eso es streaming en acci√≥n. Como ver a un cal√≠grafo experimentado trabajar ‚Äîviendo los caracteres aparecer trazo a trazo en vez de materializarse instant√°neamente‚Äî el streaming hace que la interacci√≥n se sienta m√°s natural y ofrece retroalimentaci√≥n inmediata.

### Implementando streaming con LangChain

```python
from langchain_openai import ChatOpenAI
import os

llm = ChatOpenAI(
    api_key=os.environ["GITHUB_TOKEN"],
    base_url="https://models.github.ai/inference",
    model="openai/gpt-4o-mini",
    streaming=True
)

# Transmitir la respuesta
for chunk in llm.stream("Write a short story about a robot learning to code"):
    print(chunk.content, end="", flush=True)
```

**Por qu√© el streaming es genial:**
- **Muestra** el contenido mientras se crea ‚Äî¬°nada de esperas inc√≥modas!
- **Hace** que los usuarios sientan que algo est√° ocurriendo realmente
- **Da** la sensaci√≥n de mayor velocidad, aunque t√©cnicamente no lo sea
- **Permite** que los usuarios empiecen a leer mientras la IA a√∫n est√° "pensando"

> üí° **Consejo de experiencia de usuario**: El streaming brilla especialmente con respuestas largas como explicaciones de c√≥digo, escritura creativa o tutoriales detallados. ¬°Tus usuarios amar√°n ver el progreso en lugar de mirar una pantalla en blanco!

### üéØ Reflexi√≥n pedag√≥gica: Beneficios de la abstracci√≥n del marco

**Pausa y reflexiona**: Acabas de experimentar el poder de las abstracciones en marcos de IA. Compara lo aprendido con las llamadas directas a APIs de lecciones previas.

**Autodiagn√≥stico r√°pido**:
- ¬øPuedes explicar c√≥mo LangChain simplifica la gesti√≥n de conversaciones en comparaci√≥n con el seguimiento manual de mensajes?
- ¬øCu√°l es la diferencia entre los m√©todos `invoke()` y `stream()`, y cu√°ndo usar√≠as cada uno?
- ¬øC√≥mo mejora el sistema de tipos de mensajes del marco la organizaci√≥n del c√≥digo?

**Conexi√≥n con el mundo real**: Los patrones de abstracci√≥n que aprendiste (tipos de mensajes, interfaces de streaming, memoria de conversaciones) se usan en todas las grandes aplicaciones de IA, desde la interfaz de ChatGPT hasta la asistencia de c√≥digo de GitHub Copilot. Est√°s dominando los mismos patrones arquitect√≥nicos que usan los equipos profesionales de desarrollo de IA.

**Pregunta de desaf√≠o**: ¬øC√≥mo dise√±ar√≠as una abstracci√≥n de marco para manejar distintos proveedores de modelos IA (OpenAI, Anthropic, Google) con una sola interfaz? Considera beneficios y compensaciones.

## Plantillas de prompt

Las plantillas de prompt funcionan como las estructuras ret√≥ricas usadas en la oratoria cl√°sica ‚Äîpiensa en c√≥mo Cicer√≥n adaptaba sus patrones de discurso para distintas audiencias manteniendo el mismo marco persuasivo. Te permiten crear prompts reutilizables donde puedes cambiar piezas de informaci√≥n sin reescribir todo desde cero. Una vez configurada la plantilla, solo llenas las variables con los valores que necesites.

### Creando prompts reutilizables

```python
from langchain_core.prompts import ChatPromptTemplate

# Define una plantilla para explicaciones de c√≥digo
template = ChatPromptTemplate.from_messages([
    ("system", "You are an expert programming instructor. Explain concepts clearly with examples."),
    ("human", "Explain {concept} in {language} with a practical example for {skill_level} developers")
])

# Usa la plantilla con diferentes valores
questions = [
    {"concept": "functions", "language": "JavaScript", "skill_level": "beginner"},
    {"concept": "classes", "language": "Python", "skill_level": "intermediate"},
    {"concept": "async/await", "language": "JavaScript", "skill_level": "advanced"}
]

for question in questions:
    prompt = template.format_messages(**question)
    response = llm.invoke(prompt)
    print(f"Topic: {question['concept']}\n{response.content}\n---\n")
```

**Por qu√© te encantar√°n las plantillas:**
- **Mantienen** tu prompt consistente en toda tu app
- **Nada m√°s** de concatenaciones complicadas ‚Äî solo variables limpias y simples
- **Tu IA** se comporta de manera predecible porque la estructura se mantiene igual
- **Actualizar** es muy f√°cil ‚Äî cambias la plantilla una vez y queda fija en todos lados

## Salida estructurada

¬øAlguna vez te frustraste intentando parsear respuestas de IA que vienen como texto sin estructura? La salida estructurada es como ense√±ar a tu IA a seguir el enfoque sistem√°tico que Linneo us√≥ para la clasificaci√≥n biol√≥gica ‚Äîorganizado, predecible y f√°cil de manejar. Puedes pedir JSON, estructuras de datos espec√≠ficas o cualquier formato que necesites.

### Definiendo esquemas de salida

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser
from pydantic import BaseModel, Field

class CodeReview(BaseModel):
    score: int = Field(description="Code quality score from 1-10")
    strengths: list[str] = Field(description="List of code strengths")
    improvements: list[str] = Field(description="List of suggested improvements")
    overall_feedback: str = Field(description="Summary feedback")

# Configurar el analizador
parser = JsonOutputParser(pydantic_object=CodeReview)

# Crear indicaci√≥n con instrucciones de formato
prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a code reviewer. {format_instructions}"),
    ("human", "Review this code: {code}")
])

# Formatear la indicaci√≥n con instrucciones
chain = prompt | llm | parser

# Obtener respuesta estructurada
code_sample = """
def calculate_average(numbers):
    return sum(numbers) / len(numbers)
"""

result = chain.invoke({
    "code": code_sample,
    "format_instructions": parser.get_format_instructions()
})

print(f"Score: {result['score']}")
print(f"Strengths: {', '.join(result['strengths'])}")
```

**Por qu√© la salida estructurada es revolucionaria:**
- **Nunca m√°s** adivinar√°s el formato que recibir√°s ‚Äî es consistente siempre
- **Se conecta** directamente a tus bases de datos y APIs sin trabajo adicional
- **Detecta** respuestas extra√±as antes de que rompan tu app
- **Limpia** tu c√≥digo porque sabes exactamente con qu√© est√°s trabajando

## Llamadas a herramientas

Ahora llegamos a una de las caracter√≠sticas m√°s poderosas: las herramientas. As√≠ le das a tu IA capacidades pr√°cticas m√°s all√° de la conversaci√≥n. Como los gremios medievales desarrollaban herramientas especializadas para oficios espec√≠ficos, puedes equipar a tu IA con instrumentos enfocados. Describes qu√© herramientas est√°n disponibles y cuando alguien solicita algo que coincide, tu IA puede actuar.

### Usando Python

Vamos a a√±adir algunas herramientas as√≠:

```python
from typing_extensions import Annotated, TypedDict

class add(TypedDict):
    """Add two integers."""

    # Las anotaciones deben tener el tipo y pueden incluir opcionalmente un valor predeterminado y una descripci√≥n (en ese orden).
    a: Annotated[int, ..., "First integer"]
    b: Annotated[int, ..., "Second integer"]

tools = [add]

functions = {
    "add": lambda a, b: a + b
}
```

¬øQu√© est√° pasando aqu√≠? Estamos creando un plano para una herramienta llamada `add`. Al heredar de `TypedDict` y usar esos tipos `Annotated` para `a` y `b`, le damos al LLM una idea clara de lo que hace esta herramienta y qu√© necesita. El diccionario `functions` es como nuestra caja de herramientas: le dice a nuestro c√≥digo exactamente qu√© hacer cuando la IA decide usar una herramienta espec√≠fica.

Veamos c√≥mo llamamos al LLM con esta herramienta a continuaci√≥n:

```python
llm = ChatOpenAI(
    api_key=os.environ["GITHUB_TOKEN"],
    base_url="https://models.github.ai/inference",
    model="openai/gpt-4o-mini",
)

llm_with_tools = llm.bind_tools(tools)
```

Aqu√≠ llamamos a `bind_tools` con nuestro arreglo `tools` y as√≠ el LLM `llm_with_tools` ahora tiene conocimiento de esta herramienta.

Para usar este nuevo LLM, podemos escribir el siguiente c√≥digo:

```python
query = "What is 3 + 12?"

res = llm_with_tools.invoke(query)
if(res.tool_calls):
    for tool in res.tool_calls:
        print("TOOL CALL: ", functions[tool["name"]](../../../10-ai-framework-project/**tool["args"]))
print("CONTENT: ",res.content)
```

Ahora que llamamos a `invoke` en este nuevo llm, que tiene herramientas, quiz√° la propiedad `tool_calls` est√© poblada. Si es as√≠, cualquier herramienta identificada tiene una propiedad `name` y `args` que indica qu√© herramienta se debe llamar y con qu√© argumentos. El c√≥digo completo es as√≠:

```python
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_openai import ChatOpenAI
import os
from typing_extensions import Annotated, TypedDict

class add(TypedDict):
    """Add two integers."""

    # Las anotaciones deben tener el tipo y opcionalmente pueden incluir un valor predeterminado y una descripci√≥n (en ese orden).
    a: Annotated[int, ..., "First integer"]
    b: Annotated[int, ..., "Second integer"]

tools = [add]

functions = {
    "add": lambda a, b: a + b
}

llm = ChatOpenAI(
    api_key=os.environ["GITHUB_TOKEN"],
    base_url="https://models.github.ai/inference",
    model="openai/gpt-4o-mini",
)

llm_with_tools = llm.bind_tools(tools)

query = "What is 3 + 12?"

res = llm_with_tools.invoke(query)
if(res.tool_calls):
    for tool in res.tool_calls:
        print("TOOL CALL: ", functions[tool["name"]](../../../10-ai-framework-project/**tool["args"]))
print("CONTENT: ",res.content)
```

Al ejecutar este c√≥digo, deber√≠as ver una salida similar a:

```text
TOOL CALL:  15
CONTENT: 
```

La IA examin√≥ "¬øQu√© es 3 + 12?" y reconoci√≥ esta tarea para la herramienta `add`. Como un bibliotecario experto que sabe qu√© referencia consultar seg√∫n el tipo de pregunta, hizo esta determinaci√≥n a partir del nombre, descripci√≥n y especificaciones de campo de la herramienta. El resultado de 15 proviene de nuestro diccionario `functions` ejecutando la herramienta:

```python
print("TOOL CALL: ", functions[tool["name"]](../../../10-ai-framework-project/**tool["args"]))
```

### Una herramienta m√°s interesante que llama a una API web
Agregar n√∫meros demuestra el concepto, pero las herramientas reales t√≠picamente realizan operaciones m√°s complejas, como llamar a APIs web. Ampliemos nuestro ejemplo para que la IA obtenga contenido de internet, similar a c√≥mo los operadores de tel√©grafo conectaban ubicaciones distantes:

```python
class joke(TypedDict):
    """Tell a joke."""

    # Las anotaciones deben tener el tipo y opcionalmente pueden incluir un valor predeterminado y una descripci√≥n (en ese orden).
    category: Annotated[str, ..., "The joke category"]

def get_joke(category: str) -> str:
    response = requests.get(f"https://api.chucknorris.io/jokes/random?category={category}", headers={"Accept": "application/json"})
    if response.status_code == 200:
        return response.json().get("value", f"Here's a {category} joke!")
    return f"Here's a {category} joke!"

functions = {
    "add": lambda a, b: a + b,
    "joke": lambda category: get_joke(category)
}

query = "Tell me a joke about animals"

# el resto del c√≥digo es el mismo
```

Ahora, si ejecutas este c√≥digo, obtendr√°s una respuesta que dice algo como:

```text
TOOL CALL:  Chuck Norris once rode a nine foot grizzly bear through an automatic car wash, instead of taking a shower.
CONTENT:  
```

```mermaid
flowchart TD
    A[Consulta del Usuario: "Cu√©ntame un chiste sobre animales"] --> B[An√°lisis LangChain]
    B --> C{¬øHerramienta disponible?}
    C -->|S√≠| D[Seleccionar herramienta de chistes]
    C -->|No| E[Generar respuesta directa]
    
    D --> F[Extraer Par√°metros]
    F --> G[Llamar a chiste(categor√≠a="animales")]
    G --> H[Solicitud API a chucknorris.io]
    H --> I[Devolver contenido del chiste]
    I --> J[Mostrar al usuario]
    
    E --> K[Respuesta generada por IA]
    K --> J
    
    subgraph "Capa de Definici√≥n de Herramienta"
        L[Esquema TypedDict]
        M[Implementaci√≥n de Funci√≥n]
        N[Validaci√≥n de Par√°metros]
    end
    
    D --> L
    F --> N
    G --> M
```
Aqu√≠ est√° el c√≥digo en su totalidad:

```python
from langchain_openai import ChatOpenAI
import requests
import os
from typing_extensions import Annotated, TypedDict

class add(TypedDict):
    """Add two integers."""

    # Las anotaciones deben tener el tipo y opcionalmente pueden incluir un valor predeterminado y una descripci√≥n (en ese orden).
    a: Annotated[int, ..., "First integer"]
    b: Annotated[int, ..., "Second integer"]

class joke(TypedDict):
    """Tell a joke."""

    # Las anotaciones deben tener el tipo y opcionalmente pueden incluir un valor predeterminado y una descripci√≥n (en ese orden).
    category: Annotated[str, ..., "The joke category"]

tools = [add, joke]

def get_joke(category: str) -> str:
    response = requests.get(f"https://api.chucknorris.io/jokes/random?category={category}", headers={"Accept": "application/json"})
    if response.status_code == 200:
        return response.json().get("value", f"Here's a {category} joke!")
    return f"Here's a {category} joke!"

functions = {
    "add": lambda a, b: a + b,
    "joke": lambda category: get_joke(category)
}

llm = ChatOpenAI(
    api_key=os.environ["GITHUB_TOKEN"],
    base_url="https://models.github.ai/inference",
    model="openai/gpt-4o-mini",
)

llm_with_tools = llm.bind_tools(tools)

query = "Tell me a joke about animals"

res = llm_with_tools.invoke(query)
if(res.tool_calls):
    for tool in res.tool_calls:
        # print("LLAMADA A HERRAMIENTA: ", tool)
        print("TOOL CALL: ", functions[tool["name"]](../../../10-ai-framework-project/**tool["args"]))
print("CONTENT: ",res.content)
```

## Embeddings y procesamiento de documentos

Los embeddings representan una de las soluciones m√°s elegantes en la IA moderna. Imagina si pudieras tomar cualquier fragmento de texto y convertirlo en coordenadas num√©ricas que capturen su significado. Eso es exactamente lo que hacen los embeddings: transforman texto en puntos en un espacio multidimensional donde los conceptos similares se agrupan. Es como tener un sistema de coordenadas para ideas, recordando c√≥mo Mendel√©yev organiz√≥ la tabla peri√≥dica seg√∫n propiedades at√≥micas.

### Creaci√≥n y uso de embeddings

```python
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import CharacterTextSplitter

# Inicializar incrustaciones
embeddings = OpenAIEmbeddings(
    api_key=os.environ["GITHUB_TOKEN"],
    base_url="https://models.github.ai/inference",
    model="text-embedding-3-small"
)

# Cargar y dividir documentos
loader = TextLoader("documentation.txt")
documents = loader.load()

text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
texts = text_splitter.split_documents(documents)

# Crear almac√©n vectorial
vectorstore = FAISS.from_documents(texts, embeddings)

# Realizar b√∫squeda de similitud
query = "How do I handle user authentication?"
similar_docs = vectorstore.similarity_search(query, k=3)

for doc in similar_docs:
    print(f"Relevant content: {doc.page_content[:200]}...")
```

### Cargadores de documentos para varios formatos

```python
from langchain_community.document_loaders import (
    PyPDFLoader,
    CSVLoader,
    JSONLoader,
    WebBaseLoader
)

# Cargar diferentes tipos de documentos
pdf_loader = PyPDFLoader("manual.pdf")
csv_loader = CSVLoader("data.csv")
json_loader = JSONLoader("config.json")
web_loader = WebBaseLoader("https://example.com/docs")

# Procesar todos los documentos
all_documents = []
for loader in [pdf_loader, csv_loader, json_loader, web_loader]:
    docs = loader.load()
    all_documents.extend(docs)
```

**Lo que puedes hacer con los embeddings:**
- **Construir** b√∫squedas que realmente entiendan lo que quieres decir, no solo coincidan por palabras clave
- **Crear** IA que pueda responder preguntas sobre tus documentos
- **Hacer** sistemas de recomendaci√≥n que sugieran contenido verdaderamente relevante
- **Organizar y categorizar** autom√°ticamente tu contenido

```mermaid
flowchart LR
    A[Documentos] --> B[Divisor de Texto]
    B --> C[Crear Embeddings]
    C --> D[Almac√©n Vectorial]
    
    E[Consulta del Usuario] --> F[Embedding de Consulta]
    F --> G[B√∫squeda por Similitud]
    G --> D
    D --> H[Documentos Relevantes]
    H --> I[Respuesta de IA]
    
    subgraph "Espacio Vectorial"
        J[Documento A: [0.1, 0.8, 0.3...]]
        K[Documento B: [0.2, 0.7, 0.4...]]
        L[Consulta: [0.15, 0.75, 0.35...]]
    end
    
    C --> J
    C --> K
    F --> L
    G --> J
    G --> K
```
## Construyendo una aplicaci√≥n de IA completa

Ahora integraremos todo lo que has aprendido en una aplicaci√≥n integral: un asistente de codificaci√≥n que puede responder preguntas, usar herramientas y mantener la memoria de la conversaci√≥n. As√≠ como la imprenta combin√≥ tecnolog√≠as existentes (tipograf√≠a m√≥vil, tinta, papel y presi√≥n) en algo transformador, combinaremos nuestros componentes de IA en algo pr√°ctico y √∫til.

### Ejemplo de aplicaci√≥n completa

```python
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage
from langchain_community.vectorstores import FAISS
from typing_extensions import Annotated, TypedDict
import os
import requests

class CodingAssistant:
    def __init__(self):
        self.llm = ChatOpenAI(
            api_key=os.environ["GITHUB_TOKEN"],
            base_url="https://models.github.ai/inference",
            model="openai/gpt-4o-mini"
        )
        
        self.conversation_history = [
            SystemMessage(content="""You are an expert coding assistant. 
            Help users learn programming concepts, debug code, and write better software.
            Use tools when needed and maintain a helpful, encouraging tone.""")
        ]
        
        # Definir herramientas
        self.setup_tools()
    
    def setup_tools(self):
        class web_search(TypedDict):
            """Search for programming documentation or examples."""
            query: Annotated[str, "Search query for programming help"]
        
        class code_formatter(TypedDict):
            """Format and validate code snippets."""
            code: Annotated[str, "Code to format"]
            language: Annotated[str, "Programming language"]
        
        self.tools = [web_search, code_formatter]
        self.llm_with_tools = self.llm.bind_tools(self.tools)
    
    def chat(self, user_input: str):
        # Agregar mensaje del usuario a la conversaci√≥n
        self.conversation_history.append(HumanMessage(content=user_input))
        
        # Obtener respuesta de la IA
        response = self.llm_with_tools.invoke(self.conversation_history)
        
        # Manejar llamadas a herramientas si las hay
        if response.tool_calls:
            for tool_call in response.tool_calls:
                tool_result = self.execute_tool(tool_call)
                print(f"üîß Tool used: {tool_call['name']}")
                print(f"üìä Result: {tool_result}")
        
        # Agregar respuesta de la IA a la conversaci√≥n
        self.conversation_history.append(response)
        
        return response.content
    
    def execute_tool(self, tool_call):
        tool_name = tool_call['name']
        args = tool_call['args']
        
        if tool_name == 'web_search':
            return f"Found documentation for: {args['query']}"
        elif tool_name == 'code_formatter':
            return f"Formatted {args['language']} code: {args['code'][:50]}..."
        
        return "Tool execution completed"

# Ejemplo de uso
assistant = CodingAssistant()

print("ü§ñ Coding Assistant Ready! Type 'quit' to exit.\n")

while True:
    user_input = input("You: ")
    if user_input.lower() == 'quit':
        break
    
    response = assistant.chat(user_input)
    print(f"ü§ñ Assistant: {response}\n")
```

**Arquitectura de la aplicaci√≥n:**

```mermaid
graph TD
    A[Entrada del usuario] --> B[Asistente de codificaci√≥n]
    B --> C[Memoria de conversaci√≥n]
    B --> D[Detecci√≥n de herramientas]
    B --> E[Procesamiento LLM]
    
    D --> F[Herramienta de b√∫squeda web]
    D --> G[Herramienta de formateador de c√≥digo]
    
    E --> H[Generaci√≥n de respuesta]
    F --> H
    G --> H
    
    H --> I[Interfaz de usuario]
    H --> C
```
**Caracter√≠sticas clave que hemos implementado:**
- **Recuerda** toda tu conversaci√≥n para continuidad de contexto
- **Realiza acciones** mediante llamadas a herramientas, no solo conversaci√≥n
- **Sigue** patrones de interacci√≥n predecibles
- **Gestiona** el manejo de errores y flujos de trabajo complejos autom√°ticamente

### üéØ Revisi√≥n pedag√≥gica: Arquitectura de IA para producci√≥n

**Comprensi√≥n de la arquitectura**: Has construido una aplicaci√≥n de IA completa que combina gesti√≥n de la conversaci√≥n, llamadas a herramientas y flujos de trabajo estructurados. Esto representa un desarrollo de aplicaciones de IA a nivel de producci√≥n.

**Conceptos clave dominados**:
- **Arquitectura basada en clases**: estructura organizativa y mantenible para aplicaciones de IA
- **Integraci√≥n de herramientas**: funcionalidad personalizada m√°s all√° de la conversaci√≥n
- **Gesti√≥n de memoria**: contexto persistente de la conversaci√≥n
- **Manejo de errores**: comportamiento robusto de la aplicaci√≥n

**Conexi√≥n con la industria**: Los patrones arquitect√≥nicos que has implementado (clases de conversaci√≥n, sistemas de herramientas, gesti√≥n de memoria) son los mismos patrones usados en aplicaciones empresariales de IA como el asistente de IA de Slack, GitHub Copilot y Microsoft Copilot. Est√°s construyendo con un pensamiento arquitect√≥nico de nivel profesional.

**Pregunta de reflexi√≥n**: ¬øC√≥mo extender√≠as esta aplicaci√≥n para manejar m√∫ltiples usuarios, almacenamiento persistente o integraci√≥n con bases de datos externas? Considera desaf√≠os de escalabilidad y gesti√≥n de estado.

## Tarea: Construye tu propio asistente de estudio potenciado por IA

**Objetivo**: Crear una aplicaci√≥n de IA que ayude a estudiantes a aprender conceptos de programaci√≥n proporcionando explicaciones, ejemplos de c√≥digo y cuestionarios interactivos.

### Requisitos

**Caracter√≠sticas principales (Obligatorias):**
1. **Interfaz conversacional**: Implementar un sistema de chat que mantenga el contexto a trav√©s de m√∫ltiples preguntas
2. **Herramientas educativas**: Crear al menos dos herramientas que ayuden con el aprendizaje:
   - Herramienta de explicaci√≥n de c√≥digo
   - Generador de cuestionarios de conceptos
3. **Aprendizaje personalizado**: Usar mensajes del sistema para adaptar respuestas a diferentes niveles de habilidad
4. **Formato de respuesta**: Implementar salida estructurada para preguntas de cuestionarios

### Pasos para la implementaci√≥n

**Paso 1: Configura tu entorno**
```bash
pip install langchain langchain-openai
```

**Paso 2: Funcionalidad b√°sica de chat**
- Crear una clase `StudyAssistant`
- Implementar memoria de conversaci√≥n
- A√±adir configuraci√≥n de personalidad para soporte educativo

**Paso 3: A√±adir herramientas educativas**
- **Explicador de c√≥digo**: Desglosa c√≥digo en partes comprensibles
- **Generador de cuestionarios**: Crea preguntas sobre conceptos de programaci√≥n
- **Rastreador de progreso**: Lleva control de los temas cubiertos

**Paso 4: Caracter√≠sticas mejoradas (Opcional)**
- Implementar respuestas en streaming para mejor experiencia de usuario
- A√±adir carga de documentos para incorporar materiales del curso
- Crear embeddings para recuperaci√≥n de contenido basada en similitud

### Criterios de evaluaci√≥n

| Caracter√≠stica | Excelente (4) | Bueno (3) | Satisfactorio (2) | Necesita Mejorar (1) |
|----------------|---------------|-----------|-------------------|----------------------|
| **Flujo de conversaci√≥n** | Respuestas naturales y conscientes del contexto | Buena retenci√≥n de contexto | Conversaci√≥n b√°sica | Sin memoria entre intercambios |
| **Integraci√≥n de herramientas** | Varias herramientas √∫tiles funcionando sin problemas | 2+ herramientas implementadas correctamente | 1-2 herramientas b√°sicas | Herramientas no funcionales |
| **Calidad del c√≥digo** | C√≥digo limpio, bien documentado, manejo de errores | Buena estructura, algo de documentaci√≥n | Funcionalidad b√°sica | Mala estructura, sin manejo de errores |
| **Valor educativo** | Realmente √∫til para el aprendizaje, adaptativo | Buen soporte para aprendizaje | Explicaciones b√°sicas | Beneficio educativo limitado |

### Estructura de c√≥digo de ejemplo

```python
class StudyAssistant:
    def __init__(self, skill_level="beginner"):
        # Inicializar LLM, herramientas y memoria de conversaci√≥n
        pass
    
    def explain_code(self, code, language):
        # Herramienta: Explicar c√≥mo funciona el c√≥digo
        pass
    
    def generate_quiz(self, topic, difficulty):
        # Herramienta: Crear preguntas de pr√°ctica
        pass
    
    def chat(self, user_input):
        # Interfaz principal de conversaci√≥n
        pass

# Ejemplo de uso
assistant = StudyAssistant(skill_level="intermediate")
response = assistant.chat("Explain how Python functions work")
```

**Desaf√≠os adicionales:**
- A√±adir capacidades de entrada/salida de voz
- Implementar una interfaz web usando Streamlit o Flask
- Crear una base de conocimiento a partir de materiales del curso usando embeddings
- A√±adir seguimiento de progreso y caminos de aprendizaje personalizados

## üìà Tu l√≠nea de tiempo para el dominio del desarrollo de frameworks de IA

```mermaid
timeline
    title Viaje de Desarrollo del Marco de IA de Producci√≥n
    
    section Fundamentos del Marco
        Comprensi√≥n de Abstracciones
            : Decisiones entre marco maestro vs API
            : Aprender conceptos clave de LangChain
            : Implementar sistemas de tipos de mensajes
        
        Integraci√≥n B√°sica
            : Conectar con proveedores de IA
            : Manejar la autenticaci√≥n
            : Gestionar la configuraci√≥n
    
    section Sistemas de Conversaci√≥n
        Gesti√≥n de Memoria
            : Construir historial de conversaci√≥n
            : Implementar seguimiento del contexto
            : Manejar la persistencia de sesi√≥n
        
        Interacciones Avanzadas
            : Dominar respuestas en streaming
            : Crear plantillas de indicaciones
            : Implementar salida estructurada
    
    section Integraci√≥n de Herramientas
        Desarrollo de Herramientas Personalizadas
            : Dise√±ar esquemas de herramientas
            : Implementar llamadas a funciones
            : Manejar APIs externas
        
        Automatizaci√≥n de Flujo de Trabajo
            : Encadenar m√∫ltiples herramientas
            : Crear √°rboles de decisi√≥n
            : Construir comportamientos de agentes
    
    section Aplicaciones en Producci√≥n
        Arquitectura Completa del Sistema
            : Combinar todas las caracter√≠sticas del marco
            : Implementar l√≠mites de errores
            : Crear c√≥digo mantenible
        
        Preparaci√≥n Empresarial
            : Manejar preocupaciones de escalabilidad
            : Implementar monitoreo
            : Construir estrategias de despliegue
```
**üéì Hito de graduaci√≥n**: Has dominado el desarrollo de frameworks de IA usando las mismas herramientas y patrones que impulsan las aplicaciones modernas de IA. Estas habilidades representan lo m√°s avanzado en desarrollo de aplicaciones de IA y te preparan para construir sistemas inteligentes empresariales de grado profesional.

**üîÑ Capacidades del siguiente nivel**:
- Listo para explorar arquitecturas avanzadas de IA (agentes, sistemas multiagente)
- Preparado para construir sistemas RAG con bases de datos vectoriales
- Equipado para crear aplicaciones de IA multimodales
- Fundaci√≥n s√≥lida para escalar y optimizar aplicaciones de IA

## Resumen

üéâ Ahora has dominado los fundamentos del desarrollo de frameworks de IA y aprendido c√≥mo construir aplicaciones sofisticadas usando LangChain. Como completar un aprendizaje integral, has adquirido un conjunto sustancial de habilidades. Repasemos lo que has logrado.

### Lo que has aprendido

**Conceptos centrales del framework:**
- **Beneficios del framework**: Entender cu√°ndo elegir frameworks en lugar de llamadas directas a APIs
- **Fundamentos de LangChain**: Configuraci√≥n y conexi√≥n con modelos de IA
- **Tipos de mensajes**: Uso de `SystemMessage`, `HumanMessage` y `AIMessage` para conversaciones estructuradas

**Funciones avanzadas:**
- **Llamada a herramientas**: Creaci√≥n e integraci√≥n de herramientas personalizadas para capacidades ampliadas de IA
- **Memoria de conversaci√≥n**: Mantener contexto a trav√©s de m√∫ltiples turnos de conversaci√≥n
- **Respuestas en streaming**: Implementar entrega de respuestas en tiempo real
- **Plantillas de prompts**: Construcci√≥n de prompts reutilizables y din√°micos
- **Salida estructurada**: Garantizar respuestas de IA consistentes y parseables
- **Embeddings**: Crear capacidades de b√∫squeda sem√°ntica y procesamiento de documentos

**Aplicaciones pr√°cticas:**
- **Construcci√≥n de aplicaciones completas**: Combinar m√∫ltiples caracter√≠sticas en aplicaciones listas para producci√≥n
- **Manejo de errores**: Implementar gesti√≥n robusta de errores y validaci√≥n
- **Integraci√≥n de herramientas**: Crear herramientas personalizadas que ampl√≠an las capacidades de la IA

### Puntos clave

> üéØ **Recuerda**: Frameworks de IA como LangChain son b√°sicamente tus mejores amigos que ocultan la complejidad y ofrecen muchas funcionalidades. Son perfectos cuando necesitas memoria de conversaci√≥n, llamadas a herramientas o trabajar con m√∫ltiples modelos de IA sin perder la cordura.

**Marco de decisi√≥n para la integraci√≥n de IA:**

```mermaid
flowchart TD
    A[Necesidad de integraci√≥n de IA] --> B{¬øConsulta √∫nica simple?}
    B -->|S√≠| C[Llamadas API directas]
    B -->|No| D{¬øNecesita memoria de conversaci√≥n?}
    D -->|No| E[Integraci√≥n SDK]
    D -->|S√≠| F{¬øNecesita herramientas o funciones complejas?}
    F -->|No| G[Framework con configuraci√≥n b√°sica]
    F -->|S√≠| H[Implementaci√≥n completa del framework]
    
    C --> I[Solicitudes HTTP, dependencias m√≠nimas]
    E --> J[SDK del proveedor, espec√≠fico del modelo]
    G --> K[Chat b√°sico de LangChain]
    H --> L[LangChain con herramientas, memoria, agentes]
```
### ¬øA d√≥nde vas desde aqu√≠?

**Comienza a construir ahora mismo:**
- Toma estos conceptos y crea algo que TE emocione
- Experimenta con diferentes modelos de IA a trav√©s de LangChain, es como tener un parque de juegos de modelos de IA
- Crea herramientas que resuelvan problemas reales que enfrentes en tu trabajo o proyectos

**¬øListo para el siguiente nivel?**
- **Agentes de IA**: Construye sistemas de IA que puedan planificar y ejecutar tareas complejas por s√≠ mismos
- **RAG (Generaci√≥n aumentada por recuperaci√≥n)**: Combina IA con tus propias bases de conocimiento para aplicaciones superpotentes
- **IA multimodal**: Trabaja con texto, im√°genes y audio juntos; ¬°las posibilidades son infinitas!
- **Despliegue en producci√≥n**: Aprende a escalar tus aplicaciones de IA y monitorearlas en el mundo real

**√önete a la comunidad:**
- La comunidad de LangChain es fant√°stica para mantenerte actualizado y aprender buenas pr√°cticas
- GitHub Models te da acceso a capacidades de IA de vanguardia, perfecto para experimentar
- Sigue practicando con diferentes casos de uso; cada proyecto te ense√±ar√° algo nuevo

Ahora tienes el conocimiento para construir aplicaciones inteligentes y conversacionales que pueden ayudar a las personas a resolver problemas reales. Como los artesanos del Renacimiento que combinaron visi√≥n art√≠stica con habilidad t√©cnica, ahora puedes fusionar capacidades de IA con aplicaci√≥n pr√°ctica. La pregunta es: ¬øqu√© crear√°s t√∫? üöÄ

## Desaf√≠o GitHub Copilot Agent üöÄ

Usa el modo Agente para completar el siguiente desaf√≠o:

**Descripci√≥n:** Construye un asistente avanzado de revisi√≥n de c√≥digo potenciado por IA que combine m√∫ltiples caracter√≠sticas de LangChain, incluyendo llamada a herramientas, salida estructurada y memoria de conversaci√≥n para proporcionar retroalimentaci√≥n completa sobre env√≠os de c√≥digo.

**Prompt:** Crea una clase CodeReviewAssistant que implemente:
1. Una herramienta para analizar la complejidad del c√≥digo y sugerir mejoras
2. Una herramienta para verificar el c√≥digo seg√∫n mejores pr√°cticas
3. Salida estructurada usando modelos Pydantic para formato consistente de revisi√≥n
4. Memoria de conversaci√≥n para rastrear sesiones de revisi√≥n
5. Una interfaz principal de chat que pueda manejar env√≠os de c√≥digo y proporcionar retroalimentaci√≥n detallada y accionable

El asistente debe poder revisar c√≥digo en m√∫ltiples lenguajes de programaci√≥n, mantener contexto a trav√©s de m√∫ltiples env√≠os de c√≥digo en una sesi√≥n, y proporcionar tanto puntuaciones resumen como sugerencias detalladas de mejora.

Aprende m√°s sobre [modo agente](https://code.visualstudio.com/blogs/2025/02/24/introducing-copilot-agent-mode) aqu√≠.

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**Descargo de responsabilidad**:
Este documento ha sido traducido utilizando el servicio de traducci√≥n autom√°tica [Co-op Translator](https://github.com/Azure/co-op-translator). Aunque nos esforzamos por la precisi√≥n, tenga en cuenta que las traducciones autom√°ticas pueden contener errores o imprecisiones. El documento original en su idioma nativo debe considerarse la fuente autorizada. Para informaci√≥n cr√≠tica, se recomienda una traducci√≥n profesional realizada por humanos. No nos hacemos responsables de ning√∫n malentendido o interpretaci√≥n err√≥nea que surja del uso de esta traducci√≥n.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->