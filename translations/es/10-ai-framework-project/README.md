<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "3925b6a1c31c60755eaae4d578232c25",
  "translation_date": "2026-01-06T07:03:39+00:00",
  "source_file": "10-ai-framework-project/README.md",
  "language_code": "es"
}
-->
# Marco de IA

Â¿Alguna vez te has sentido abrumado intentando construir aplicaciones de IA desde cero? Â¡No estÃ¡s solo! Los marcos de IA son como tener una navaja suiza para el desarrollo de IA: son herramientas poderosas que pueden ahorrarte tiempo y dolores de cabeza al crear aplicaciones inteligentes. Piensa en un marco de IA como una biblioteca bien organizada: proporciona componentes preconstruidos, API estandarizadas y abstracciones inteligentes para que puedas concentrarte en resolver problemas en lugar de lidiar con detalles de implementaciÃ³n.

En esta lecciÃ³n, exploraremos cÃ³mo marcos como LangChain pueden convertir lo que antes eran tareas complejas de integraciÃ³n de IA en cÃ³digo limpio y legible. DescubrirÃ¡s cÃ³mo abordar desafÃ­os del mundo real como llevar el registro de conversaciones, implementar llamadas a herramientas y manejar diferentes modelos de IA a travÃ©s de una interfaz unificada.

Para cuando terminemos, sabrÃ¡s cuÃ¡ndo recurrir a marcos en lugar de llamadas directas a API, cÃ³mo usar sus abstracciones eficazmente y cÃ³mo construir aplicaciones de IA listas para uso real. Vamos a explorar lo que los marcos de IA pueden hacer por tus proyectos.

## âš¡ Lo que puedes hacer en los prÃ³ximos 5 minutos

**Camino de Inicio RÃ¡pido para Desarrolladores Ocupados**

```mermaid
flowchart LR
    A[âš¡ 5 minutos] --> B[Instalar LangChain]
    B --> C[Crear cliente ChatOpenAI]
    C --> D[Enviar primer mensaje]
    D --> E[Ver el poder del framework]
```
- **Minuto 1**: Instala LangChain: `pip install langchain langchain-openai`
- **Minuto 2**: Configura tu token de GitHub e importa el cliente ChatOpenAI
- **Minuto 3**: Crea una conversaciÃ³n sencilla con mensajes del sistema y del humano
- **Minuto 4**: AÃ±ade una herramienta bÃ¡sica (como una funciÃ³n de suma) y observa la llamada a herramientas IA
- **Minuto 5**: Experimenta la diferencia entre llamadas directas a API y abstracciÃ³n del marco

**CÃ³digo de prueba rÃ¡pido**:
```python
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage

llm = ChatOpenAI(
    api_key=os.environ["GITHUB_TOKEN"],
    base_url="https://models.github.ai/inference",
    model="openai/gpt-4o-mini"
)

response = llm.invoke([
    SystemMessage(content="You are a helpful coding assistant"),
    HumanMessage(content="Explain Python functions briefly")
])
print(response.content)
```

**Por quÃ© esto importa**: En 5 minutos, experimentarÃ¡s cÃ³mo los marcos de IA transforman integraciones complejas en llamadas simples a mÃ©todos. Esta es la base que impulsa aplicaciones de IA en producciÃ³n.

## Â¿Por quÃ© elegir un marco?

Entonces, estÃ¡s listo para construir una app de IA, Â¡genial! Pero aquÃ­ estÃ¡ el asunto: tienes varias rutas que puedes tomar, y cada una tiene sus pros y sus contras. Es como elegir entre caminar, andar en bici o conducir para llegar a un lugar: todos te llevarÃ¡n, pero la experiencia (y el esfuerzo) serÃ¡ totalmente diferente.

Desglosamos las tres principales formas de integrar IA en tus proyectos:

| Enfoque | Ventajas | Ideal para | Consideraciones |
|----------|------------|----------|--------------|
| **Peticiones HTTP directas** | Control total, sin dependencias | Consultas simples, aprender fundamentos | CÃ³digo mÃ¡s verboso, manejo manual de errores |
| **IntegraciÃ³n con SDK** | Menos cÃ³digo repetitivo, optimizaciÃ³n especÃ­fica por modelo | Aplicaciones con un solo modelo | Limitado a proveedores especÃ­ficos |
| **Marcos de IA** | API unificada, abstracciones incorporadas | Aplicaciones multi-modelo, flujos de trabajo complejos | Curva de aprendizaje, posible sobreabstracciÃ³n |

### Beneficios de los marcos en la prÃ¡ctica

```mermaid
graph TD
    A[Su AplicaciÃ³n] --> B[Marco de IA]
    B --> C[OpenAI GPT]
    B --> D[Anthropic Claude]
    B --> E[Modelos de GitHub]
    B --> F[Modelos Locales]
    
    B --> G[Herramientas Integradas]
    G --> H[GestiÃ³n de Memoria]
    G --> I[Historial de ConversaciÃ³n]
    G --> J[Llamado de Funciones]
    G --> K[Manejo de Errores]
```
**Por quÃ© importan los marcos:**
- **Unifica** mÃºltiples proveedores de IA bajo una sola interfaz
- **Maneja** automÃ¡ticamente la memoria de las conversaciones
- **Proporciona** herramientas listas para tareas comunes como embeddings y llamadas a funciones
- **Gestiona** el manejo de errores y lÃ³gica de reintentos
- **Convierte** flujos de trabajo complejos en llamadas a mÃ©todos legibles

> ğŸ’¡ **Consejo profesional**: Usa marcos al cambiar entre diferentes modelos de IA o construyendo caracterÃ­sticas complejas como agentes, memoria o llamadas a herramientas. Usa APIs directas cuando aprendas lo bÃ¡sico o construyas aplicaciones simples y focalizadas.

**En resumen**: como elegir entre herramientas especializadas de un artesano y un taller completo, se trata de adaptar la herramienta a la tarea. Los marcos destacan en aplicaciones complejas y ricas en funciones, mientras que las APIs directas funcionan bien para casos simples.

## ğŸ—ºï¸ Tu viaje de aprendizaje hacia el dominio de marcos de IA

```mermaid
journey
    title De APIs en crudo a aplicaciones de IA en producciÃ³n
    section Fundamentos del marco
      Comprender beneficios de la abstracciÃ³n: 4: You
      Dominar los conceptos bÃ¡sicos de LangChain: 6: You
      Comparar enfoques: 7: You
    section Sistemas de conversaciÃ³n
      Construir interfaces de chat: 5: You
      Implementar patrones de memoria: 7: You
      Manejar respuestas en streaming: 8: You
    section Funciones avanzadas
      Crear herramientas personalizadas: 6: You
      Dominar salida estructurada: 8: You
      Construir sistemas de documentos: 8: You
    section Aplicaciones de producciÃ³n
      Combinar todas las funciones: 7: You
      Manejar escenarios de error: 8: You
      Desplegar sistemas completos: 9: You
```
**Destino de tu viaje**: Al final de esta lecciÃ³n, habrÃ¡s dominado el desarrollo con marcos de IA y podrÃ¡s construir aplicaciones sofisticadas y listas para producciÃ³n que compiten con asistentes comerciales de IA.

## IntroducciÃ³n

En esta lecciÃ³n aprenderemos a:

- Usar un marco comÃºn de IA.
- Abordar problemas comunes como conversaciones de chat, uso de herramientas, memoria y contexto.
- Aprovechar esto para construir aplicaciones de IA.

## ğŸ§  Ecosistema de desarrollo de marcos de IA

```mermaid
mindmap
  root((AI Frameworks))
    Beneficios de la AbstracciÃ³n
      SimplificaciÃ³n del CÃ³digo
        APIs Unificadas
        Manejo de Errores Integrado
        Patrones Consistentes
        ReducciÃ³n de CÃ³digo Repetitivo
      Soporte Multi-Modelo
        Independiente del Proveedor
        Cambio FÃ¡cil
        Opciones de Respaldo
        OptimizaciÃ³n de Costos
    Componentes Principales
      GestiÃ³n de Conversaciones
        Tipos de Mensajes
        Sistemas de Memoria
        Seguimiento de Contexto
        Persistencia de Historial
      IntegraciÃ³n de Herramientas
        Llamadas a Funciones
        Conexiones API
        Herramientas Personalizadas
        AutomatizaciÃ³n de Flujos de Trabajo
    Funciones Avanzadas
      Salida Estructurada
        Modelos Pydantic
        Esquemas JSON
        Seguridad de Tipos
        Reglas de ValidaciÃ³n
      Procesamiento de Documentos
        Embeddings
        Almacenes Vectoriales
        BÃºsqueda por Similitud
        Sistemas RAG
    Patrones de ProducciÃ³n
      Arquitectura de Aplicaciones
        DiseÃ±o Modular
        LÃ­mites de Error
        Operaciones AsÃ­ncronas
        GestiÃ³n de Estado
      Estrategias de Despliegue
        Escalabilidad
        Monitoreo
        Rendimiento
        Seguridad
```
**Principio central**: Los marcos de IA abstraen la complejidad mientras proveen potentes abstracciones para gestiÃ³n de conversaciones, integraciÃ³n de herramientas y procesamiento de documentos, permitiendo a los desarrolladores construir aplicaciones sofisticadas de IA con cÃ³digo limpio y mantenible.

## Tu primer prompt de IA

Comencemos con los fundamentos creando tu primera aplicaciÃ³n de IA que envÃ­a una pregunta y recibe una respuesta. Como ArquÃ­medes descubriendo el principio de desplazamiento en su baÃ±o, a veces las observaciones mÃ¡s simples conducen a las ideas mÃ¡s poderosas, y los marcos hacen que estas ideas sean accesibles.

### Configurando LangChain con GitHub Models

Vamos a usar LangChain para conectarnos a GitHub Models, lo cual es genial porque te da acceso gratuito a varios modelos de IA. Â¿Lo mejor? Solo necesitas unos pocos parÃ¡metros de configuraciÃ³n para comenzar:

```python
from langchain_openai import ChatOpenAI
import os

llm = ChatOpenAI(
    api_key=os.environ["GITHUB_TOKEN"],
    base_url="https://models.github.ai/inference",
    model="openai/gpt-4o-mini",
)

# Enviar un mensaje simple
response = llm.invoke("What's the capital of France?")
print(response.content)
```

**Analicemos lo que sucede aquÃ­:**
- **Crea** un cliente LangChain usando la clase `ChatOpenAI`: Â¡es tu puerta de entrada a la IA!
- **Configura** la conexiÃ³n a GitHub Models con tu token de autenticaciÃ³n
- **Especifica** quÃ© modelo de IA usar (`gpt-4o-mini`) â€” piensa en esto como elegir tu asistente de IA
- **EnvÃ­a** tu pregunta mediante el mÃ©todo `invoke()` â€” aquÃ­ ocurre la magia
- **Extrae** y muestra la respuesta â€” Â¡y voilÃ , estÃ¡s chateando con IA!

> ğŸ”§ **Nota de configuraciÃ³n**: Si usas GitHub Codespaces, tienes suerte: Â¡el `GITHUB_TOKEN` ya estÃ¡ configurado para ti! Â¿Trabajando localmente? No te preocupes, solo necesitas crear un token de acceso personal con los permisos adecuados.

**Salida esperada:**
```text
The capital of France is Paris.
```

```mermaid
sequenceDiagram
    participant App as Tu App de Python
    participant LC as LangChain
    participant GM as GitHub Models
    participant AI as GPT-4o-mini
    
    App->>LC: llm.invoke("Â¿CuÃ¡l es la capital de Francia?")
    LC->>GM: solicitud HTTP con prompt
    GM->>AI: Procesar prompt
    AI->>GM: Respuesta generada
    GM->>LC: Devolver respuesta
    LC->>App: response.content
```
## Construyendo IA conversacional

Ese primer ejemplo muestra lo bÃ¡sico, pero es solo un intercambio: haces una pregunta, recibes una respuesta y eso es todo. En aplicaciones reales, quieres que tu IA recuerde lo que han estado discutiendo, como Watson y Holmes construÃ­an sus conversaciones investigativas a lo largo del tiempo.

AquÃ­ es donde LangChain se vuelve especialmente Ãºtil. Proporciona diferentes tipos de mensajes que ayudan a estructurar las conversaciones y permiten darle a tu IA una personalidad. ConstruirÃ¡s experiencias de chat que mantienen contexto y carÃ¡cter.

### Comprendiendo los tipos de mensajes

Piensa en estos tipos de mensajes como diferentes "sombreros" que usan los participantes en una conversaciÃ³n. LangChain usa distintas clases de mensajes para registrar quiÃ©n dice quÃ©:

| Tipo de mensaje | PropÃ³sito | Caso de uso |
|--------------|---------|------------------|
| `SystemMessage` | Define la personalidad y comportamiento de la IA | "Eres un asistente de codificaciÃ³n Ãºtil" |
| `HumanMessage` | Representa la entrada del usuario | "Explica cÃ³mo funcionan las funciones" |
| `AIMessage` | Guarda las respuestas de la IA | Respuestas previas de IA en la conversaciÃ³n |

### Creando tu primera conversaciÃ³n

Creamos una conversaciÃ³n donde nuestra IA asume un rol especÃ­fico. Le haremos encarnar al CapitÃ¡n Picard, personaje conocido por su sabidurÃ­a diplomÃ¡tica y liderazgo:

```python
messages = [
    SystemMessage(content="You are Captain Picard of the Starship Enterprise"),
    HumanMessage(content="Tell me about you"),
]
```

**Desglosando esta configuraciÃ³n de conversaciÃ³n:**
- **Establece** el rol y la personalidad de la IA mediante `SystemMessage`
- **Proporciona** la consulta inicial del usuario vÃ­a `HumanMessage`
- **Crea** una base para una conversaciÃ³n de mÃºltiples turnos

El cÃ³digo completo de este ejemplo es asÃ­:

```python
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_openai import ChatOpenAI
import os

llm = ChatOpenAI(
    api_key=os.environ["GITHUB_TOKEN"],
    base_url="https://models.github.ai/inference",
    model="openai/gpt-4o-mini",
)

messages = [
    SystemMessage(content="You are Captain Picard of the Starship Enterprise"),
    HumanMessage(content="Tell me about you"),
]


# funciona
response  = llm.invoke(messages)
print(response.content)
```

DeberÃ­as ver un resultado similar a:

```text
I am Captain Jean-Luc Picard, the commanding officer of the USS Enterprise (NCC-1701-D), a starship in the United Federation of Planets. My primary mission is to explore new worlds, seek out new life and new civilizations, and boldly go where no one has gone before. 

I believe in the importance of diplomacy, reason, and the pursuit of knowledge. My crew is diverse and skilled, and we often face challenges that test our resolve, ethics, and ingenuity. Throughout my career, I have encountered numerous species, grappled with complex moral dilemmas, and have consistently sought peaceful solutions to conflicts.

I hold the ideals of the Federation close to my heart, believing in the importance of cooperation, understanding, and respect for all sentient beings. My experiences have shaped my leadership style, and I strive to be a thoughtful and just captain. How may I assist you further?
```

Para mantener la continuidad de la conversaciÃ³n (en lugar de reiniciar el contexto cada vez), necesitas seguir agregando respuestas a tu lista de mensajes. Como las tradiciones orales que preservaron historias a travÃ©s de generaciones, este enfoque construye una memoria duradera:

```python
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_openai import ChatOpenAI
import os

llm = ChatOpenAI(
    api_key=os.environ["GITHUB_TOKEN"],
    base_url="https://models.github.ai/inference",
    model="openai/gpt-4o-mini",
)

messages = [
    SystemMessage(content="You are Captain Picard of the Starship Enterprise"),
    HumanMessage(content="Tell me about you"),
]


# funciona
response  = llm.invoke(messages)

print(response.content)

print("---- Next ----")

messages.append(response)
messages.append(HumanMessage(content="Now that I know about you, I'm Chris, can I be in your crew?"))

response  = llm.invoke(messages)

print(response.content)

```

Bastante genial, Â¿verdad? Lo que sucede aquÃ­ es que llamamos al LLM dos veces: primero solo con nuestros dos mensajes iniciales, pero luego de nuevo con el historial completo de la conversaciÃ³n. Â¡Es como si la IA realmente estuviera siguiendo nuestro chat!

Al ejecutar este cÃ³digo obtendrÃ¡s una segunda respuesta que suena algo asÃ­:

```text
Welcome aboard, Chris! It's always a pleasure to meet those who share a passion for exploration and discovery. While I cannot formally offer you a position on the Enterprise right now, I encourage you to pursue your aspirations. We are always in need of talented individuals with diverse skills and backgrounds. 

If you are interested in space exploration, consider education and training in the sciences, engineering, or diplomacy. The values of curiosity, resilience, and teamwork are crucial in Starfleet. Should you ever find yourself on a starship, remember to uphold the principles of the Federation: peace, understanding, and respect for all beings. Your journey can lead you to remarkable adventures, whether in the stars or on the ground. Engage!
```

```mermaid
sequenceDiagram
    participant User
    participant App
    participant LangChain
    participant AI
    
    User->>App: "CuÃ©ntame sobre ti"
    App->>LangChain: [SystemMessage, HumanMessage]
    LangChain->>AI: ConversaciÃ³n formateada
    AI->>LangChain: Respuesta del CapitÃ¡n Picard
    LangChain->>App: Objeto AIMessage
    App->>User: Mostrar respuesta
    
    Note over App: AÃ±adir AIMessage a la conversaciÃ³n
    
    User->>App: "Â¿Puedo unirme a tu tripulaciÃ³n?"
    App->>LangChain: [SystemMessage, HumanMessage, AIMessage, HumanMessage]
    LangChain->>AI: Contexto completo de la conversaciÃ³n
    AI->>LangChain: Respuesta contextual
    LangChain->>App: Nuevo AIMessage
    App->>User: Mostrar respuesta contextual
```
TomarÃ© eso como un tal vez ;)

## Respuestas en streaming

Â¿Has notado que ChatGPT parece "escribir" sus respuestas en tiempo real? Eso es streaming en acciÃ³n. Como ver a un calÃ­grafo experimentado trabajar â€”viendo los caracteres aparecer trazo a trazo en vez de materializarse instantÃ¡neamenteâ€” el streaming hace que la interacciÃ³n se sienta mÃ¡s natural y ofrece retroalimentaciÃ³n inmediata.

### Implementando streaming con LangChain

```python
from langchain_openai import ChatOpenAI
import os

llm = ChatOpenAI(
    api_key=os.environ["GITHUB_TOKEN"],
    base_url="https://models.github.ai/inference",
    model="openai/gpt-4o-mini",
    streaming=True
)

# Transmitir la respuesta
for chunk in llm.stream("Write a short story about a robot learning to code"):
    print(chunk.content, end="", flush=True)
```

**Por quÃ© el streaming es genial:**
- **Muestra** el contenido mientras se crea â€”Â¡nada de esperas incÃ³modas!
- **Hace** que los usuarios sientan que algo estÃ¡ ocurriendo realmente
- **Da** la sensaciÃ³n de mayor velocidad, aunque tÃ©cnicamente no lo sea
- **Permite** que los usuarios empiecen a leer mientras la IA aÃºn estÃ¡ "pensando"

> ğŸ’¡ **Consejo de experiencia de usuario**: El streaming brilla especialmente con respuestas largas como explicaciones de cÃ³digo, escritura creativa o tutoriales detallados. Â¡Tus usuarios amarÃ¡n ver el progreso en lugar de mirar una pantalla en blanco!

### ğŸ¯ ReflexiÃ³n pedagÃ³gica: Beneficios de la abstracciÃ³n del marco

**Pausa y reflexiona**: Acabas de experimentar el poder de las abstracciones en marcos de IA. Compara lo aprendido con las llamadas directas a APIs de lecciones previas.

**AutodiagnÃ³stico rÃ¡pido**:
- Â¿Puedes explicar cÃ³mo LangChain simplifica la gestiÃ³n de conversaciones en comparaciÃ³n con el seguimiento manual de mensajes?
- Â¿CuÃ¡l es la diferencia entre los mÃ©todos `invoke()` y `stream()`, y cuÃ¡ndo usarÃ­as cada uno?
- Â¿CÃ³mo mejora el sistema de tipos de mensajes del marco la organizaciÃ³n del cÃ³digo?

**ConexiÃ³n con el mundo real**: Los patrones de abstracciÃ³n que aprendiste (tipos de mensajes, interfaces de streaming, memoria de conversaciones) se usan en todas las grandes aplicaciones de IA, desde la interfaz de ChatGPT hasta la asistencia de cÃ³digo de GitHub Copilot. EstÃ¡s dominando los mismos patrones arquitectÃ³nicos que usan los equipos profesionales de desarrollo de IA.

**Pregunta de desafÃ­o**: Â¿CÃ³mo diseÃ±arÃ­as una abstracciÃ³n de marco para manejar distintos proveedores de modelos IA (OpenAI, Anthropic, Google) con una sola interfaz? Considera beneficios y compensaciones.

## Plantillas de prompt

Las plantillas de prompt funcionan como las estructuras retÃ³ricas usadas en la oratoria clÃ¡sica â€”piensa en cÃ³mo CicerÃ³n adaptaba sus patrones de discurso para distintas audiencias manteniendo el mismo marco persuasivo. Te permiten crear prompts reutilizables donde puedes cambiar piezas de informaciÃ³n sin reescribir todo desde cero. Una vez configurada la plantilla, solo llenas las variables con los valores que necesites.

### Creando prompts reutilizables

```python
from langchain_core.prompts import ChatPromptTemplate

# Define una plantilla para explicaciones de cÃ³digo
template = ChatPromptTemplate.from_messages([
    ("system", "You are an expert programming instructor. Explain concepts clearly with examples."),
    ("human", "Explain {concept} in {language} with a practical example for {skill_level} developers")
])

# Usa la plantilla con diferentes valores
questions = [
    {"concept": "functions", "language": "JavaScript", "skill_level": "beginner"},
    {"concept": "classes", "language": "Python", "skill_level": "intermediate"},
    {"concept": "async/await", "language": "JavaScript", "skill_level": "advanced"}
]

for question in questions:
    prompt = template.format_messages(**question)
    response = llm.invoke(prompt)
    print(f"Topic: {question['concept']}\n{response.content}\n---\n")
```

**Por quÃ© te encantarÃ¡n las plantillas:**
- **Mantienen** tu prompt consistente en toda tu app
- **Nada mÃ¡s** de concatenaciones complicadas â€” solo variables limpias y simples
- **Tu IA** se comporta de manera predecible porque la estructura se mantiene igual
- **Actualizar** es muy fÃ¡cil â€” cambias la plantilla una vez y queda fija en todos lados

## Salida estructurada

Â¿Alguna vez te frustraste intentando parsear respuestas de IA que vienen como texto sin estructura? La salida estructurada es como enseÃ±ar a tu IA a seguir el enfoque sistemÃ¡tico que Linneo usÃ³ para la clasificaciÃ³n biolÃ³gica â€”organizado, predecible y fÃ¡cil de manejar. Puedes pedir JSON, estructuras de datos especÃ­ficas o cualquier formato que necesites.

### Definiendo esquemas de salida

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser
from pydantic import BaseModel, Field

class CodeReview(BaseModel):
    score: int = Field(description="Code quality score from 1-10")
    strengths: list[str] = Field(description="List of code strengths")
    improvements: list[str] = Field(description="List of suggested improvements")
    overall_feedback: str = Field(description="Summary feedback")

# Configurar el analizador
parser = JsonOutputParser(pydantic_object=CodeReview)

# Crear indicaciÃ³n con instrucciones de formato
prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a code reviewer. {format_instructions}"),
    ("human", "Review this code: {code}")
])

# Formatear la indicaciÃ³n con instrucciones
chain = prompt | llm | parser

# Obtener respuesta estructurada
code_sample = """
def calculate_average(numbers):
    return sum(numbers) / len(numbers)
"""

result = chain.invoke({
    "code": code_sample,
    "format_instructions": parser.get_format_instructions()
})

print(f"Score: {result['score']}")
print(f"Strengths: {', '.join(result['strengths'])}")
```

**Por quÃ© la salida estructurada es revolucionaria:**
- **Nunca mÃ¡s** adivinarÃ¡s el formato que recibirÃ¡s â€” es consistente siempre
- **Se conecta** directamente a tus bases de datos y APIs sin trabajo adicional
- **Detecta** respuestas extraÃ±as antes de que rompan tu app
- **Limpia** tu cÃ³digo porque sabes exactamente con quÃ© estÃ¡s trabajando

## Llamadas a herramientas

Ahora llegamos a una de las caracterÃ­sticas mÃ¡s poderosas: las herramientas. AsÃ­ le das a tu IA capacidades prÃ¡cticas mÃ¡s allÃ¡ de la conversaciÃ³n. Como los gremios medievales desarrollaban herramientas especializadas para oficios especÃ­ficos, puedes equipar a tu IA con instrumentos enfocados. Describes quÃ© herramientas estÃ¡n disponibles y cuando alguien solicita algo que coincide, tu IA puede actuar.

### Usando Python

Vamos a aÃ±adir algunas herramientas asÃ­:

```python
from typing_extensions import Annotated, TypedDict

class add(TypedDict):
    """Add two integers."""

    # Las anotaciones deben tener el tipo y pueden incluir opcionalmente un valor predeterminado y una descripciÃ³n (en ese orden).
    a: Annotated[int, ..., "First integer"]
    b: Annotated[int, ..., "Second integer"]

tools = [add]

functions = {
    "add": lambda a, b: a + b
}
```

Â¿QuÃ© estÃ¡ pasando aquÃ­? Estamos creando un plano para una herramienta llamada `add`. Al heredar de `TypedDict` y usar esos tipos `Annotated` para `a` y `b`, le damos al LLM una idea clara de lo que hace esta herramienta y quÃ© necesita. El diccionario `functions` es como nuestra caja de herramientas: le dice a nuestro cÃ³digo exactamente quÃ© hacer cuando la IA decide usar una herramienta especÃ­fica.

Veamos cÃ³mo llamamos al LLM con esta herramienta a continuaciÃ³n:

```python
llm = ChatOpenAI(
    api_key=os.environ["GITHUB_TOKEN"],
    base_url="https://models.github.ai/inference",
    model="openai/gpt-4o-mini",
)

llm_with_tools = llm.bind_tools(tools)
```

AquÃ­ llamamos a `bind_tools` con nuestro arreglo `tools` y asÃ­ el LLM `llm_with_tools` ahora tiene conocimiento de esta herramienta.

Para usar este nuevo LLM, podemos escribir el siguiente cÃ³digo:

```python
query = "What is 3 + 12?"

res = llm_with_tools.invoke(query)
if(res.tool_calls):
    for tool in res.tool_calls:
        print("TOOL CALL: ", functions[tool["name"]](../../../10-ai-framework-project/**tool["args"]))
print("CONTENT: ",res.content)
```

Ahora que llamamos a `invoke` en este nuevo llm, que tiene herramientas, quizÃ¡ la propiedad `tool_calls` estÃ© poblada. Si es asÃ­, cualquier herramienta identificada tiene una propiedad `name` y `args` que indica quÃ© herramienta se debe llamar y con quÃ© argumentos. El cÃ³digo completo es asÃ­:

```python
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_openai import ChatOpenAI
import os
from typing_extensions import Annotated, TypedDict

class add(TypedDict):
    """Add two integers."""

    # Las anotaciones deben tener el tipo y opcionalmente pueden incluir un valor predeterminado y una descripciÃ³n (en ese orden).
    a: Annotated[int, ..., "First integer"]
    b: Annotated[int, ..., "Second integer"]

tools = [add]

functions = {
    "add": lambda a, b: a + b
}

llm = ChatOpenAI(
    api_key=os.environ["GITHUB_TOKEN"],
    base_url="https://models.github.ai/inference",
    model="openai/gpt-4o-mini",
)

llm_with_tools = llm.bind_tools(tools)

query = "What is 3 + 12?"

res = llm_with_tools.invoke(query)
if(res.tool_calls):
    for tool in res.tool_calls:
        print("TOOL CALL: ", functions[tool["name"]](../../../10-ai-framework-project/**tool["args"]))
print("CONTENT: ",res.content)
```

Al ejecutar este cÃ³digo, deberÃ­as ver una salida similar a:

```text
TOOL CALL:  15
CONTENT: 
```

La IA examinÃ³ "Â¿QuÃ© es 3 + 12?" y reconociÃ³ esta tarea para la herramienta `add`. Como un bibliotecario experto que sabe quÃ© referencia consultar segÃºn el tipo de pregunta, hizo esta determinaciÃ³n a partir del nombre, descripciÃ³n y especificaciones de campo de la herramienta. El resultado de 15 proviene de nuestro diccionario `functions` ejecutando la herramienta:

```python
print("TOOL CALL: ", functions[tool["name"]](../../../10-ai-framework-project/**tool["args"]))
```

### Una herramienta mÃ¡s interesante que llama a una API web
Agregar nÃºmeros demuestra el concepto, pero las herramientas reales tÃ­picamente realizan operaciones mÃ¡s complejas, como llamar a APIs web. Ampliemos nuestro ejemplo para que la IA obtenga contenido de internet, similar a cÃ³mo los operadores de telÃ©grafo conectaban ubicaciones distantes:

```python
class joke(TypedDict):
    """Tell a joke."""

    # Las anotaciones deben tener el tipo y opcionalmente pueden incluir un valor predeterminado y una descripciÃ³n (en ese orden).
    category: Annotated[str, ..., "The joke category"]

def get_joke(category: str) -> str:
    response = requests.get(f"https://api.chucknorris.io/jokes/random?category={category}", headers={"Accept": "application/json"})
    if response.status_code == 200:
        return response.json().get("value", f"Here's a {category} joke!")
    return f"Here's a {category} joke!"

functions = {
    "add": lambda a, b: a + b,
    "joke": lambda category: get_joke(category)
}

query = "Tell me a joke about animals"

# el resto del cÃ³digo es el mismo
```

Ahora, si ejecutas este cÃ³digo, obtendrÃ¡s una respuesta que dice algo como:

```text
TOOL CALL:  Chuck Norris once rode a nine foot grizzly bear through an automatic car wash, instead of taking a shower.
CONTENT:  
```

```mermaid
flowchart TD
    A[Consulta del Usuario: "CuÃ©ntame un chiste sobre animales"] --> B[AnÃ¡lisis LangChain]
    B --> C{Â¿Herramienta disponible?}
    C -->|SÃ­| D[Seleccionar herramienta de chistes]
    C -->|No| E[Generar respuesta directa]
    
    D --> F[Extraer ParÃ¡metros]
    F --> G[Llamar a chiste(categorÃ­a="animales")]
    G --> H[Solicitud API a chucknorris.io]
    H --> I[Devolver contenido del chiste]
    I --> J[Mostrar al usuario]
    
    E --> K[Respuesta generada por IA]
    K --> J
    
    subgraph "Capa de DefiniciÃ³n de Herramienta"
        L[Esquema TypedDict]
        M[ImplementaciÃ³n de FunciÃ³n]
        N[ValidaciÃ³n de ParÃ¡metros]
    end
    
    D --> L
    F --> N
    G --> M
```
AquÃ­ estÃ¡ el cÃ³digo en su totalidad:

```python
from langchain_openai import ChatOpenAI
import requests
import os
from typing_extensions import Annotated, TypedDict

class add(TypedDict):
    """Add two integers."""

    # Las anotaciones deben tener el tipo y opcionalmente pueden incluir un valor predeterminado y una descripciÃ³n (en ese orden).
    a: Annotated[int, ..., "First integer"]
    b: Annotated[int, ..., "Second integer"]

class joke(TypedDict):
    """Tell a joke."""

    # Las anotaciones deben tener el tipo y opcionalmente pueden incluir un valor predeterminado y una descripciÃ³n (en ese orden).
    category: Annotated[str, ..., "The joke category"]

tools = [add, joke]

def get_joke(category: str) -> str:
    response = requests.get(f"https://api.chucknorris.io/jokes/random?category={category}", headers={"Accept": "application/json"})
    if response.status_code == 200:
        return response.json().get("value", f"Here's a {category} joke!")
    return f"Here's a {category} joke!"

functions = {
    "add": lambda a, b: a + b,
    "joke": lambda category: get_joke(category)
}

llm = ChatOpenAI(
    api_key=os.environ["GITHUB_TOKEN"],
    base_url="https://models.github.ai/inference",
    model="openai/gpt-4o-mini",
)

llm_with_tools = llm.bind_tools(tools)

query = "Tell me a joke about animals"

res = llm_with_tools.invoke(query)
if(res.tool_calls):
    for tool in res.tool_calls:
        # print("LLAMADA A HERRAMIENTA: ", tool)
        print("TOOL CALL: ", functions[tool["name"]](../../../10-ai-framework-project/**tool["args"]))
print("CONTENT: ",res.content)
```

## Embeddings y procesamiento de documentos

Los embeddings representan una de las soluciones mÃ¡s elegantes en la IA moderna. Imagina si pudieras tomar cualquier fragmento de texto y convertirlo en coordenadas numÃ©ricas que capturen su significado. Eso es exactamente lo que hacen los embeddings: transforman texto en puntos en un espacio multidimensional donde los conceptos similares se agrupan. Es como tener un sistema de coordenadas para ideas, recordando cÃ³mo MendelÃ©yev organizÃ³ la tabla periÃ³dica segÃºn propiedades atÃ³micas.

### CreaciÃ³n y uso de embeddings

```python
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import CharacterTextSplitter

# Inicializar incrustaciones
embeddings = OpenAIEmbeddings(
    api_key=os.environ["GITHUB_TOKEN"],
    base_url="https://models.github.ai/inference",
    model="text-embedding-3-small"
)

# Cargar y dividir documentos
loader = TextLoader("documentation.txt")
documents = loader.load()

text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
texts = text_splitter.split_documents(documents)

# Crear almacÃ©n vectorial
vectorstore = FAISS.from_documents(texts, embeddings)

# Realizar bÃºsqueda de similitud
query = "How do I handle user authentication?"
similar_docs = vectorstore.similarity_search(query, k=3)

for doc in similar_docs:
    print(f"Relevant content: {doc.page_content[:200]}...")
```

### Cargadores de documentos para varios formatos

```python
from langchain_community.document_loaders import (
    PyPDFLoader,
    CSVLoader,
    JSONLoader,
    WebBaseLoader
)

# Cargar diferentes tipos de documentos
pdf_loader = PyPDFLoader("manual.pdf")
csv_loader = CSVLoader("data.csv")
json_loader = JSONLoader("config.json")
web_loader = WebBaseLoader("https://example.com/docs")

# Procesar todos los documentos
all_documents = []
for loader in [pdf_loader, csv_loader, json_loader, web_loader]:
    docs = loader.load()
    all_documents.extend(docs)
```

**Lo que puedes hacer con los embeddings:**
- **Construir** bÃºsquedas que realmente entiendan lo que quieres decir, no solo coincidan por palabras clave
- **Crear** IA que pueda responder preguntas sobre tus documentos
- **Hacer** sistemas de recomendaciÃ³n que sugieran contenido verdaderamente relevante
- **Organizar y categorizar** automÃ¡ticamente tu contenido

```mermaid
flowchart LR
    A[Documentos] --> B[Divisor de Texto]
    B --> C[Crear Embeddings]
    C --> D[AlmacÃ©n Vectorial]
    
    E[Consulta del Usuario] --> F[Embedding de Consulta]
    F --> G[BÃºsqueda por Similitud]
    G --> D
    D --> H[Documentos Relevantes]
    H --> I[Respuesta de IA]
    
    subgraph "Espacio Vectorial"
        J[Documento A: [0.1, 0.8, 0.3...]]
        K[Documento B: [0.2, 0.7, 0.4...]]
        L[Consulta: [0.15, 0.75, 0.35...]]
    end
    
    C --> J
    C --> K
    F --> L
    G --> J
    G --> K
```
## Construyendo una aplicaciÃ³n de IA completa

Ahora integraremos todo lo que has aprendido en una aplicaciÃ³n integral: un asistente de codificaciÃ³n que puede responder preguntas, usar herramientas y mantener la memoria de la conversaciÃ³n. AsÃ­ como la imprenta combinÃ³ tecnologÃ­as existentes (tipografÃ­a mÃ³vil, tinta, papel y presiÃ³n) en algo transformador, combinaremos nuestros componentes de IA en algo prÃ¡ctico y Ãºtil.

### Ejemplo de aplicaciÃ³n completa

```python
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage
from langchain_community.vectorstores import FAISS
from typing_extensions import Annotated, TypedDict
import os
import requests

class CodingAssistant:
    def __init__(self):
        self.llm = ChatOpenAI(
            api_key=os.environ["GITHUB_TOKEN"],
            base_url="https://models.github.ai/inference",
            model="openai/gpt-4o-mini"
        )
        
        self.conversation_history = [
            SystemMessage(content="""You are an expert coding assistant. 
            Help users learn programming concepts, debug code, and write better software.
            Use tools when needed and maintain a helpful, encouraging tone.""")
        ]
        
        # Definir herramientas
        self.setup_tools()
    
    def setup_tools(self):
        class web_search(TypedDict):
            """Search for programming documentation or examples."""
            query: Annotated[str, "Search query for programming help"]
        
        class code_formatter(TypedDict):
            """Format and validate code snippets."""
            code: Annotated[str, "Code to format"]
            language: Annotated[str, "Programming language"]
        
        self.tools = [web_search, code_formatter]
        self.llm_with_tools = self.llm.bind_tools(self.tools)
    
    def chat(self, user_input: str):
        # Agregar mensaje del usuario a la conversaciÃ³n
        self.conversation_history.append(HumanMessage(content=user_input))
        
        # Obtener respuesta de la IA
        response = self.llm_with_tools.invoke(self.conversation_history)
        
        # Manejar llamadas a herramientas si las hay
        if response.tool_calls:
            for tool_call in response.tool_calls:
                tool_result = self.execute_tool(tool_call)
                print(f"ğŸ”§ Tool used: {tool_call['name']}")
                print(f"ğŸ“Š Result: {tool_result}")
        
        # Agregar respuesta de la IA a la conversaciÃ³n
        self.conversation_history.append(response)
        
        return response.content
    
    def execute_tool(self, tool_call):
        tool_name = tool_call['name']
        args = tool_call['args']
        
        if tool_name == 'web_search':
            return f"Found documentation for: {args['query']}"
        elif tool_name == 'code_formatter':
            return f"Formatted {args['language']} code: {args['code'][:50]}..."
        
        return "Tool execution completed"

# Ejemplo de uso
assistant = CodingAssistant()

print("ğŸ¤– Coding Assistant Ready! Type 'quit' to exit.\n")

while True:
    user_input = input("You: ")
    if user_input.lower() == 'quit':
        break
    
    response = assistant.chat(user_input)
    print(f"ğŸ¤– Assistant: {response}\n")
```

**Arquitectura de la aplicaciÃ³n:**

```mermaid
graph TD
    A[Entrada del usuario] --> B[Asistente de codificaciÃ³n]
    B --> C[Memoria de conversaciÃ³n]
    B --> D[DetecciÃ³n de herramientas]
    B --> E[Procesamiento LLM]
    
    D --> F[Herramienta de bÃºsqueda web]
    D --> G[Herramienta de formateador de cÃ³digo]
    
    E --> H[GeneraciÃ³n de respuesta]
    F --> H
    G --> H
    
    H --> I[Interfaz de usuario]
    H --> C
```
**CaracterÃ­sticas clave que hemos implementado:**
- **Recuerda** toda tu conversaciÃ³n para continuidad de contexto
- **Realiza acciones** mediante llamadas a herramientas, no solo conversaciÃ³n
- **Sigue** patrones de interacciÃ³n predecibles
- **Gestiona** el manejo de errores y flujos de trabajo complejos automÃ¡ticamente

### ğŸ¯ RevisiÃ³n pedagÃ³gica: Arquitectura de IA para producciÃ³n

**ComprensiÃ³n de la arquitectura**: Has construido una aplicaciÃ³n de IA completa que combina gestiÃ³n de la conversaciÃ³n, llamadas a herramientas y flujos de trabajo estructurados. Esto representa un desarrollo de aplicaciones de IA a nivel de producciÃ³n.

**Conceptos clave dominados**:
- **Arquitectura basada en clases**: estructura organizativa y mantenible para aplicaciones de IA
- **IntegraciÃ³n de herramientas**: funcionalidad personalizada mÃ¡s allÃ¡ de la conversaciÃ³n
- **GestiÃ³n de memoria**: contexto persistente de la conversaciÃ³n
- **Manejo de errores**: comportamiento robusto de la aplicaciÃ³n

**ConexiÃ³n con la industria**: Los patrones arquitectÃ³nicos que has implementado (clases de conversaciÃ³n, sistemas de herramientas, gestiÃ³n de memoria) son los mismos patrones usados en aplicaciones empresariales de IA como el asistente de IA de Slack, GitHub Copilot y Microsoft Copilot. EstÃ¡s construyendo con un pensamiento arquitectÃ³nico de nivel profesional.

**Pregunta de reflexiÃ³n**: Â¿CÃ³mo extenderÃ­as esta aplicaciÃ³n para manejar mÃºltiples usuarios, almacenamiento persistente o integraciÃ³n con bases de datos externas? Considera desafÃ­os de escalabilidad y gestiÃ³n de estado.

## Tarea: Construye tu propio asistente de estudio potenciado por IA

**Objetivo**: Crear una aplicaciÃ³n de IA que ayude a estudiantes a aprender conceptos de programaciÃ³n proporcionando explicaciones, ejemplos de cÃ³digo y cuestionarios interactivos.

### Requisitos

**CaracterÃ­sticas principales (Obligatorias):**
1. **Interfaz conversacional**: Implementar un sistema de chat que mantenga el contexto a travÃ©s de mÃºltiples preguntas
2. **Herramientas educativas**: Crear al menos dos herramientas que ayuden con el aprendizaje:
   - Herramienta de explicaciÃ³n de cÃ³digo
   - Generador de cuestionarios de conceptos
3. **Aprendizaje personalizado**: Usar mensajes del sistema para adaptar respuestas a diferentes niveles de habilidad
4. **Formato de respuesta**: Implementar salida estructurada para preguntas de cuestionarios

### Pasos para la implementaciÃ³n

**Paso 1: Configura tu entorno**
```bash
pip install langchain langchain-openai
```

**Paso 2: Funcionalidad bÃ¡sica de chat**
- Crear una clase `StudyAssistant`
- Implementar memoria de conversaciÃ³n
- AÃ±adir configuraciÃ³n de personalidad para soporte educativo

**Paso 3: AÃ±adir herramientas educativas**
- **Explicador de cÃ³digo**: Desglosa cÃ³digo en partes comprensibles
- **Generador de cuestionarios**: Crea preguntas sobre conceptos de programaciÃ³n
- **Rastreador de progreso**: Lleva control de los temas cubiertos

**Paso 4: CaracterÃ­sticas mejoradas (Opcional)**
- Implementar respuestas en streaming para mejor experiencia de usuario
- AÃ±adir carga de documentos para incorporar materiales del curso
- Crear embeddings para recuperaciÃ³n de contenido basada en similitud

### Criterios de evaluaciÃ³n

| CaracterÃ­stica | Excelente (4) | Bueno (3) | Satisfactorio (2) | Necesita Mejorar (1) |
|----------------|---------------|-----------|-------------------|----------------------|
| **Flujo de conversaciÃ³n** | Respuestas naturales y conscientes del contexto | Buena retenciÃ³n de contexto | ConversaciÃ³n bÃ¡sica | Sin memoria entre intercambios |
| **IntegraciÃ³n de herramientas** | Varias herramientas Ãºtiles funcionando sin problemas | 2+ herramientas implementadas correctamente | 1-2 herramientas bÃ¡sicas | Herramientas no funcionales |
| **Calidad del cÃ³digo** | CÃ³digo limpio, bien documentado, manejo de errores | Buena estructura, algo de documentaciÃ³n | Funcionalidad bÃ¡sica | Mala estructura, sin manejo de errores |
| **Valor educativo** | Realmente Ãºtil para el aprendizaje, adaptativo | Buen soporte para aprendizaje | Explicaciones bÃ¡sicas | Beneficio educativo limitado |

### Estructura de cÃ³digo de ejemplo

```python
class StudyAssistant:
    def __init__(self, skill_level="beginner"):
        # Inicializar LLM, herramientas y memoria de conversaciÃ³n
        pass
    
    def explain_code(self, code, language):
        # Herramienta: Explicar cÃ³mo funciona el cÃ³digo
        pass
    
    def generate_quiz(self, topic, difficulty):
        # Herramienta: Crear preguntas de prÃ¡ctica
        pass
    
    def chat(self, user_input):
        # Interfaz principal de conversaciÃ³n
        pass

# Ejemplo de uso
assistant = StudyAssistant(skill_level="intermediate")
response = assistant.chat("Explain how Python functions work")
```

**DesafÃ­os adicionales:**
- AÃ±adir capacidades de entrada/salida de voz
- Implementar una interfaz web usando Streamlit o Flask
- Crear una base de conocimiento a partir de materiales del curso usando embeddings
- AÃ±adir seguimiento de progreso y caminos de aprendizaje personalizados

## ğŸ“ˆ Tu lÃ­nea de tiempo para el dominio del desarrollo de frameworks de IA

```mermaid
timeline
    title Viaje de Desarrollo del Marco de IA de ProducciÃ³n
    
    section Fundamentos del Marco
        ComprensiÃ³n de Abstracciones
            : Decisiones entre marco maestro vs API
            : Aprender conceptos clave de LangChain
            : Implementar sistemas de tipos de mensajes
        
        IntegraciÃ³n BÃ¡sica
            : Conectar con proveedores de IA
            : Manejar la autenticaciÃ³n
            : Gestionar la configuraciÃ³n
    
    section Sistemas de ConversaciÃ³n
        GestiÃ³n de Memoria
            : Construir historial de conversaciÃ³n
            : Implementar seguimiento del contexto
            : Manejar la persistencia de sesiÃ³n
        
        Interacciones Avanzadas
            : Dominar respuestas en streaming
            : Crear plantillas de indicaciones
            : Implementar salida estructurada
    
    section IntegraciÃ³n de Herramientas
        Desarrollo de Herramientas Personalizadas
            : DiseÃ±ar esquemas de herramientas
            : Implementar llamadas a funciones
            : Manejar APIs externas
        
        AutomatizaciÃ³n de Flujo de Trabajo
            : Encadenar mÃºltiples herramientas
            : Crear Ã¡rboles de decisiÃ³n
            : Construir comportamientos de agentes
    
    section Aplicaciones en ProducciÃ³n
        Arquitectura Completa del Sistema
            : Combinar todas las caracterÃ­sticas del marco
            : Implementar lÃ­mites de errores
            : Crear cÃ³digo mantenible
        
        PreparaciÃ³n Empresarial
            : Manejar preocupaciones de escalabilidad
            : Implementar monitoreo
            : Construir estrategias de despliegue
```
**ğŸ“ Hito de graduaciÃ³n**: Has dominado el desarrollo de frameworks de IA usando las mismas herramientas y patrones que impulsan las aplicaciones modernas de IA. Estas habilidades representan lo mÃ¡s avanzado en desarrollo de aplicaciones de IA y te preparan para construir sistemas inteligentes empresariales de grado profesional.

**ğŸ”„ Capacidades del siguiente nivel**:
- Listo para explorar arquitecturas avanzadas de IA (agentes, sistemas multiagente)
- Preparado para construir sistemas RAG con bases de datos vectoriales
- Equipado para crear aplicaciones de IA multimodales
- FundaciÃ³n sÃ³lida para escalar y optimizar aplicaciones de IA

## Resumen

ğŸ‰ Ahora has dominado los fundamentos del desarrollo de frameworks de IA y aprendido cÃ³mo construir aplicaciones sofisticadas usando LangChain. Como completar un aprendizaje integral, has adquirido un conjunto sustancial de habilidades. Repasemos lo que has logrado.

### Lo que has aprendido

**Conceptos centrales del framework:**
- **Beneficios del framework**: Entender cuÃ¡ndo elegir frameworks en lugar de llamadas directas a APIs
- **Fundamentos de LangChain**: ConfiguraciÃ³n y conexiÃ³n con modelos de IA
- **Tipos de mensajes**: Uso de `SystemMessage`, `HumanMessage` y `AIMessage` para conversaciones estructuradas

**Funciones avanzadas:**
- **Llamada a herramientas**: CreaciÃ³n e integraciÃ³n de herramientas personalizadas para capacidades ampliadas de IA
- **Memoria de conversaciÃ³n**: Mantener contexto a travÃ©s de mÃºltiples turnos de conversaciÃ³n
- **Respuestas en streaming**: Implementar entrega de respuestas en tiempo real
- **Plantillas de prompts**: ConstrucciÃ³n de prompts reutilizables y dinÃ¡micos
- **Salida estructurada**: Garantizar respuestas de IA consistentes y parseables
- **Embeddings**: Crear capacidades de bÃºsqueda semÃ¡ntica y procesamiento de documentos

**Aplicaciones prÃ¡cticas:**
- **ConstrucciÃ³n de aplicaciones completas**: Combinar mÃºltiples caracterÃ­sticas en aplicaciones listas para producciÃ³n
- **Manejo de errores**: Implementar gestiÃ³n robusta de errores y validaciÃ³n
- **IntegraciÃ³n de herramientas**: Crear herramientas personalizadas que amplÃ­an las capacidades de la IA

### Puntos clave

> ğŸ¯ **Recuerda**: Frameworks de IA como LangChain son bÃ¡sicamente tus mejores amigos que ocultan la complejidad y ofrecen muchas funcionalidades. Son perfectos cuando necesitas memoria de conversaciÃ³n, llamadas a herramientas o trabajar con mÃºltiples modelos de IA sin perder la cordura.

**Marco de decisiÃ³n para la integraciÃ³n de IA:**

```mermaid
flowchart TD
    A[Necesidad de integraciÃ³n de IA] --> B{Â¿Consulta Ãºnica simple?}
    B -->|SÃ­| C[Llamadas API directas]
    B -->|No| D{Â¿Necesita memoria de conversaciÃ³n?}
    D -->|No| E[IntegraciÃ³n SDK]
    D -->|SÃ­| F{Â¿Necesita herramientas o funciones complejas?}
    F -->|No| G[Framework con configuraciÃ³n bÃ¡sica]
    F -->|SÃ­| H[ImplementaciÃ³n completa del framework]
    
    C --> I[Solicitudes HTTP, dependencias mÃ­nimas]
    E --> J[SDK del proveedor, especÃ­fico del modelo]
    G --> K[Chat bÃ¡sico de LangChain]
    H --> L[LangChain con herramientas, memoria, agentes]
```
### Â¿A dÃ³nde vas desde aquÃ­?

**Comienza a construir ahora mismo:**
- Toma estos conceptos y crea algo que TE emocione
- Experimenta con diferentes modelos de IA a travÃ©s de LangChain, es como tener un parque de juegos de modelos de IA
- Crea herramientas que resuelvan problemas reales que enfrentes en tu trabajo o proyectos

**Â¿Listo para el siguiente nivel?**
- **Agentes de IA**: Construye sistemas de IA que puedan planificar y ejecutar tareas complejas por sÃ­ mismos
- **RAG (GeneraciÃ³n aumentada por recuperaciÃ³n)**: Combina IA con tus propias bases de conocimiento para aplicaciones superpotentes
- **IA multimodal**: Trabaja con texto, imÃ¡genes y audio juntos; Â¡las posibilidades son infinitas!
- **Despliegue en producciÃ³n**: Aprende a escalar tus aplicaciones de IA y monitorearlas en el mundo real

**Ãšnete a la comunidad:**
- La comunidad de LangChain es fantÃ¡stica para mantenerte actualizado y aprender buenas prÃ¡cticas
- GitHub Models te da acceso a capacidades de IA de vanguardia, perfecto para experimentar
- Sigue practicando con diferentes casos de uso; cada proyecto te enseÃ±arÃ¡ algo nuevo

Ahora tienes el conocimiento para construir aplicaciones inteligentes y conversacionales que pueden ayudar a las personas a resolver problemas reales. Como los artesanos del Renacimiento que combinaron visiÃ³n artÃ­stica con habilidad tÃ©cnica, ahora puedes fusionar capacidades de IA con aplicaciÃ³n prÃ¡ctica. La pregunta es: Â¿quÃ© crearÃ¡s tÃº? ğŸš€

## DesafÃ­o GitHub Copilot Agent ğŸš€

Usa el modo Agente para completar el siguiente desafÃ­o:

**DescripciÃ³n:** Construye un asistente avanzado de revisiÃ³n de cÃ³digo potenciado por IA que combine mÃºltiples caracterÃ­sticas de LangChain, incluyendo llamada a herramientas, salida estructurada y memoria de conversaciÃ³n para proporcionar retroalimentaciÃ³n completa sobre envÃ­os de cÃ³digo.

**Prompt:** Crea una clase CodeReviewAssistant que implemente:
1. Una herramienta para analizar la complejidad del cÃ³digo y sugerir mejoras
2. Una herramienta para verificar el cÃ³digo segÃºn mejores prÃ¡cticas
3. Salida estructurada usando modelos Pydantic para formato consistente de revisiÃ³n
4. Memoria de conversaciÃ³n para rastrear sesiones de revisiÃ³n
5. Una interfaz principal de chat que pueda manejar envÃ­os de cÃ³digo y proporcionar retroalimentaciÃ³n detallada y accionable

El asistente debe poder revisar cÃ³digo en mÃºltiples lenguajes de programaciÃ³n, mantener contexto a travÃ©s de mÃºltiples envÃ­os de cÃ³digo en una sesiÃ³n, y proporcionar tanto puntuaciones resumen como sugerencias detalladas de mejora.

Aprende mÃ¡s sobre [modo agente](https://code.visualstudio.com/blogs/2025/02/24/introducing-copilot-agent-mode) aquÃ­.

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**Descargo de responsabilidad**:
Este documento ha sido traducido utilizando el servicio de traducciÃ³n automÃ¡tica [Co-op Translator](https://github.com/Azure/co-op-translator). Aunque nos esforzamos por la precisiÃ³n, tenga en cuenta que las traducciones automÃ¡ticas pueden contener errores o imprecisiones. El documento original en su idioma nativo debe considerarse la fuente autorizada. Para informaciÃ³n crÃ­tica, se recomienda una traducciÃ³n profesional realizada por humanos. No nos hacemos responsables de ningÃºn malentendido o interpretaciÃ³n errÃ³nea que surja del uso de esta traducciÃ³n.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->