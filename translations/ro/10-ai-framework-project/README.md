<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "e2c4ae5688e34b4b8b09d52aec56c79e",
  "translation_date": "2025-10-24T22:02:45+00:00",
  "source_file": "10-ai-framework-project/README.md",
  "language_code": "ro"
}
-->
# Cadru AI

Te-ai sim탵it vreodat캒 cople탳it 칥ncerc칙nd s캒 construie탳ti aplica탵ii AI de la zero? Nu e탳ti singur! Cadrele AI sunt ca un briceag elve탵ian pentru dezvoltarea AI - sunt instrumente puternice care te pot salva de timp 탳i dureri de cap atunci c칙nd construie탳ti aplica탵ii inteligente. G칙nde탳te-te la un cadru AI ca la o bibliotec캒 bine organizat캒: ofer캒 componente predefinite, API-uri standardizate 탳i abstrac탵ii inteligente, astfel 칥nc칙t s캒 te po탵i concentra pe rezolvarea problemelor, 칥n loc s캒 te lup탵i cu detaliile implement캒rii.

칉n aceast캒 lec탵ie, vom explora cum cadrele precum LangChain pot transforma sarcinile complexe de integrare AI 칥n cod curat 탳i u탳or de citit. Vei descoperi cum s캒 abordezi provoc캒ri reale, cum ar fi urm캒rirea conversa탵iilor, implementarea apelurilor de instrumente 탳i gestionarea diferitelor modele AI printr-o interfa탵캒 unificat캒.

P칙n캒 la final, vei 탳ti c칙nd s캒 alegi cadrele 칥n locul apelurilor API brute, cum s캒 folose탳ti eficient abstrac탵iile lor 탳i cum s캒 construie탳ti aplica탵ii AI preg캒tite pentru utilizarea 칥n lumea real캒. Haide s캒 explor캒m ce pot face cadrele AI pentru proiectele tale.

## De ce s캒 alegi un cadru?

E탳ti gata s캒 construie탳ti o aplica탵ie AI - grozav! Dar iat캒 problema: ai mai multe c캒i pe care le po탵i urma, iar fiecare are propriile avantaje 탳i dezavantaje. E ca 탳i cum ai alege 칥ntre a merge pe jos, a merge cu bicicleta sau a conduce pentru a ajunge undeva - toate te vor duce acolo, dar experien탵a (탳i efortul) vor fi complet diferite.

S캒 analiz캒m cele trei moduri principale prin care po탵i integra AI 칥n proiectele tale:

| Abordare | Avantaje | Cel mai potrivit pentru | Considera탵ii |
|----------|----------|--------------------------|--------------|
| **Cereri HTTP directe** | Control total, f캒r캒 dependen탵e | Interog캒ri simple, 칥nv캒탵area fundamentelor | Cod mai detaliat, gestionarea manual캒 a erorilor |
| **Integrarea SDK** | Mai pu탵in cod redundant, optimizare specific캒 modelului | Aplica탵ii cu un singur model | Limitat la furnizori specifici |
| **Cadre AI** | API unificat, abstrac탵ii integrate | Aplica탵ii multi-model, fluxuri de lucru complexe | Curb캒 de 칥nv캒탵are, posibil캒 supra-abstrac탵ie |

### Beneficiile cadrelor 칥n practic캒

```mermaid
graph TD
    A[Your Application] --> B[AI Framework]
    B --> C[OpenAI GPT]
    B --> D[Anthropic Claude]
    B --> E[GitHub Models]
    B --> F[Local Models]
    
    B --> G[Built-in Tools]
    G --> H[Memory Management]
    G --> I[Conversation History]
    G --> J[Function Calling]
    G --> K[Error Handling]
```

**De ce conteaz캒 cadrele:**
- **Unific캒** mai mul탵i furnizori AI sub o singur캒 interfa탵캒
- **Gestioneaz캒** automat memoria conversa탵iei
- **Ofer캒** instrumente gata f캒cute pentru sarcini comune, cum ar fi 칥ncorpor캒rile 탳i apelurile func탵iilor
- **Administreaz캒** gestionarea erorilor 탳i logica de re칥ncercare
- **Transform캒** fluxurile de lucru complexe 칥n apeluri de metode u탳or de citit

> 游눠 **Sfat Pro**: Folose탳te cadrele atunci c칙nd treci 칥ntre diferite modele AI sau construie탳ti func탵ii complexe, cum ar fi agen탵i, memorie sau apeluri de instrumente. R캒m칙i la API-uri directe c칙nd 칥nve탵i bazele sau construie탳ti aplica탵ii simple 탳i concentrate.

**Concluzie**: La fel ca alegerea 칥ntre unelte specializate ale unui me탳te탳ugar 탳i un atelier complet, este vorba despre potrivirea instrumentului cu sarcina. Cadrele exceleaz캒 pentru aplica탵ii complexe 탳i bogate 칥n func탵ii, 칥n timp ce API-urile directe func탵ioneaz캒 bine pentru cazuri de utilizare simple.

## Introducere

칉n aceast캒 lec탵ie, vom 칥nv캒탵a s캒:

- Folosim un cadru AI comun.
- Abord캒m probleme comune, cum ar fi conversa탵iile de chat, utilizarea instrumentelor, memoria 탳i contextul.
- Folosim aceste cuno탳tin탵e pentru a construi aplica탵ii AI.

## Primul t캒u prompt AI

S캒 칥ncepem cu elementele fundamentale, cre칙nd prima ta aplica탵ie AI care trimite o 칥ntrebare 탳i prime탳te un r캒spuns. La fel cum Arhimede a descoperit principiul deplas캒rii 칥n baie, uneori cele mai simple observa탵ii duc la cele mai puternice perspective - iar cadrele fac aceste perspective accesibile.

### Configurarea LangChain cu Modele GitHub

Vom folosi LangChain pentru a ne conecta la Modele GitHub, ceea ce este grozav, deoarece 칥탵i ofer캒 acces gratuit la diverse modele AI. Partea cea mai bun캒? Ai nevoie doar de c칙탵iva parametri de configurare simpli pentru a 칥ncepe:

```python
from langchain_openai import ChatOpenAI
import os

llm = ChatOpenAI(
    api_key=os.environ["GITHUB_TOKEN"],
    base_url="https://models.github.ai/inference",
    model="openai/gpt-4o-mini",
)

# Send a simple prompt
response = llm.invoke("What's the capital of France?")
print(response.content)
```

**S캒 descompunem ce se 칥nt칙mpl캒 aici:**
- **Creeaz캒** un client LangChain folosind clasa `ChatOpenAI` - aceasta este poarta ta c캒tre AI!
- **Configureaz캒** conexiunea la Modele GitHub cu tokenul t캒u de autentificare
- **Specific캒** ce model AI s캒 folose탳ti (`gpt-4o-mini`) - g칙nde탳te-te la asta ca la alegerea asistentului t캒u AI
- **Trimite** 칥ntrebarea ta folosind metoda `invoke()` - aici se 칥nt칙mpl캒 magia
- **Extrage** 탳i afi탳eaz캒 r캒spunsul - 탳i voil, vorbe탳ti cu AI!

> 游댢 **Not캒 de configurare**: Dac캒 folose탳ti GitHub Codespaces, e탳ti norocos - `GITHUB_TOKEN` este deja configurat pentru tine! Lucrezi local? Nicio problem캒, va trebui doar s캒 creezi un token de acces personal cu permisiunile corespunz캒toare.

**Rezultatul a탳teptat:**
```text
The capital of France is Paris.
```

```mermaid
sequenceDiagram
    participant App as Your Python App
    participant LC as LangChain
    participant GM as GitHub Models
    participant AI as GPT-4o-mini
    
    App->>LC: llm.invoke("What's the capital of France?")
    LC->>GM: HTTP request with prompt
    GM->>AI: Process prompt
    AI->>GM: Generated response
    GM->>LC: Return response
    LC->>App: response.content
```

## Construirea unui AI conversa탵ional

Primul exemplu demonstreaz캒 elementele de baz캒, dar este doar un schimb unic - pui o 칥ntrebare, prime탳ti un r캒spuns 탳i at칙t. 칉n aplica탵iile reale, vrei ca AI-ul t캒u s캒-탳i aminteasc캒 ce a탵i discutat, la fel cum Watson 탳i Holmes 칥탳i construiau conversa탵iile investigative de-a lungul timpului.

Aici LangChain devine deosebit de util. Ofer캒 diferite tipuri de mesaje care ajut캒 la structurarea conversa탵iilor 탳i 칥탵i permit s캒 dai personalitate AI-ului t캒u. Vei construi experien탵e de chat care men탵in contextul 탳i caracterul.

### 칉n탵elegerea tipurilor de mesaje

G칙nde탳te-te la aceste tipuri de mesaje ca la diferite "p캒l캒rii" pe care participan탵ii le poart캒 칥ntr-o conversa탵ie. LangChain folose탳te diferite clase de mesaje pentru a 탵ine eviden탵a cine spune ce:

| Tip de mesaj | Scop | Exemplu de utilizare |
|--------------|------|----------------------|
| `SystemMessage` | Define탳te personalitatea 탳i comportamentul AI | "E탳ti un asistent de codare util" |
| `HumanMessage` | Reprezint캒 inputul utilizatorului | "Explic캒 cum func탵ioneaz캒 func탵iile" |
| `AIMessage` | Stocheaz캒 r캒spunsurile AI | R캒spunsurile anterioare ale AI 칥n conversa탵ie |

### Crearea primei tale conversa탵ii

S캒 cre캒m o conversa탵ie 칥n care AI-ul nostru 칥탳i asum캒 un rol specific. 칉l vom face s캒 칥ntruchipeze pe C캒pitanul Picard - un personaj cunoscut pentru 칥n탵elepciunea sa diplomatic캒 탳i leadership:

```python
messages = [
    SystemMessage(content="You are Captain Picard of the Starship Enterprise"),
    HumanMessage(content="Tell me about you"),
]
```

**Descompunerea configur캒rii acestei conversa탵ii:**
- **Stabile탳te** rolul 탳i personalitatea AI-ului prin `SystemMessage`
- **Ofer캒** interogarea ini탵ial캒 a utilizatorului prin `HumanMessage`
- **Creeaz캒** o funda탵ie pentru conversa탵ii pe mai multe runde

Codul complet pentru acest exemplu arat캒 astfel:

```python
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_openai import ChatOpenAI
import os

llm = ChatOpenAI(
    api_key=os.environ["GITHUB_TOKEN"],
    base_url="https://models.github.ai/inference",
    model="openai/gpt-4o-mini",
)

messages = [
    SystemMessage(content="You are Captain Picard of the Starship Enterprise"),
    HumanMessage(content="Tell me about you"),
]


# works
response  = llm.invoke(messages)
print(response.content)
```

Ar trebui s캒 vezi un rezultat similar cu:

```text
I am Captain Jean-Luc Picard, the commanding officer of the USS Enterprise (NCC-1701-D), a starship in the United Federation of Planets. My primary mission is to explore new worlds, seek out new life and new civilizations, and boldly go where no one has gone before. 

I believe in the importance of diplomacy, reason, and the pursuit of knowledge. My crew is diverse and skilled, and we often face challenges that test our resolve, ethics, and ingenuity. Throughout my career, I have encountered numerous species, grappled with complex moral dilemmas, and have consistently sought peaceful solutions to conflicts.

I hold the ideals of the Federation close to my heart, believing in the importance of cooperation, understanding, and respect for all sentient beings. My experiences have shaped my leadership style, and I strive to be a thoughtful and just captain. How may I assist you further?
```

Pentru a men탵ine continuitatea conversa탵iei (칥n loc s캒 resetezi contextul de fiecare dat캒), trebuie s캒 continui s캒 adaugi r캒spunsuri la lista ta de mesaje. La fel ca tradi탵iile orale care au p캒strat pove탳tile de-a lungul genera탵iilor, aceast캒 abordare construie탳te o memorie durabil캒:

```python
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_openai import ChatOpenAI
import os

llm = ChatOpenAI(
    api_key=os.environ["GITHUB_TOKEN"],
    base_url="https://models.github.ai/inference",
    model="openai/gpt-4o-mini",
)

messages = [
    SystemMessage(content="You are Captain Picard of the Starship Enterprise"),
    HumanMessage(content="Tell me about you"),
]


# works
response  = llm.invoke(messages)

print(response.content)

print("---- Next ----")

messages.append(response)
messages.append(HumanMessage(content="Now that I know about you, I'm Chris, can I be in your crew?"))

response  = llm.invoke(messages)

print(response.content)

```

Destul de interesant, nu-i a탳a? Ce se 칥nt칙mpl캒 aici este c캒 apel캒m LLM de dou캒 ori - mai 칥nt칙i doar cu primele dou캒 mesaje, dar apoi din nou cu 칥ntreaga istorie a conversa탵iei. Este ca 탳i cum AI-ul ar urm캒ri cu adev캒rat conversa탵ia noastr캒!

C칙nd rulezi acest cod, vei ob탵ine un al doilea r캒spuns care sun캒 ceva de genul:

```text
Welcome aboard, Chris! It's always a pleasure to meet those who share a passion for exploration and discovery. While I cannot formally offer you a position on the Enterprise right now, I encourage you to pursue your aspirations. We are always in need of talented individuals with diverse skills and backgrounds. 

If you are interested in space exploration, consider education and training in the sciences, engineering, or diplomacy. The values of curiosity, resilience, and teamwork are crucial in Starfleet. Should you ever find yourself on a starship, remember to uphold the principles of the Federation: peace, understanding, and respect for all beings. Your journey can lead you to remarkable adventures, whether in the stars or on the ground. Engage!
```

O s캒 iau asta ca pe un poate ;)

## R캒spunsuri 칥n flux

Ai observat vreodat캒 cum ChatGPT pare s캒 "scrie" r캒spunsurile sale 칥n timp real? Asta este fluxul 칥n ac탵iune. La fel ca atunci c칙nd prive탳ti un caligraf priceput lucr칙nd - v캒z칙nd cum apar caracterele tr캒s캒tur캒 cu tr캒s캒tur캒, mai degrab캒 dec칙t materializ칙ndu-se instantaneu - fluxul face interac탵iunea s캒 par캒 mai natural캒 탳i ofer캒 feedback imediat.

### Implementarea fluxului cu LangChain

```python
from langchain_openai import ChatOpenAI
import os

llm = ChatOpenAI(
    api_key=os.environ["GITHUB_TOKEN"],
    base_url="https://models.github.ai/inference",
    model="openai/gpt-4o-mini",
    streaming=True
)

# Stream the response
for chunk in llm.stream("Write a short story about a robot learning to code"):
    print(chunk.content, end="", flush=True)
```

**De ce fluxul este grozav:**
- **Arat캒** con탵inutul pe m캒sur캒 ce este creat - nu mai trebuie s캒 a탳tep탵i st칙njenitor!
- **Face** utilizatorii s캒 simt캒 c캒 se 칥nt칙mpl캒 ceva
- **Pare** mai rapid, chiar dac캒 tehnic nu este
- **Permite** utilizatorilor s캒 칥nceap캒 s캒 citeasc캒 칥n timp ce AI-ul 칥nc캒 "g칙nde탳te"

> 游눠 **Sfat pentru experien탵a utilizatorului**: Fluxul str캒luce탳te cu adev캒rat atunci c칙nd ai de-a face cu r캒spunsuri mai lungi, cum ar fi explica탵ii de cod, scriere creativ캒 sau tutoriale detaliate. Utilizatorii t캒i vor iubi s캒 vad캒 progresul 칥n loc s캒 priveasc캒 un ecran gol!

## 탲abloane de prompturi

탲abloanele de prompturi func탵ioneaz캒 ca structurile retorice utilizate 칥n oratoria clasic캒 - g칙nde탳te-te cum Cicero 칥탳i adapta modelele de discurs pentru diferite audien탵e, men탵in칙nd 칥n acela탳i timp acela탳i cadru persuasiv. Ele 칥탵i permit s캒 creezi prompturi reutilizabile 칥n care po탵i 칥nlocui diferite piese de informa탵ie f캒r캒 s캒 rescrii totul de la zero. Odat캒 ce ai configurat 탳ablonul, doar completezi variabilele cu valorile necesare.

### Crearea de prompturi reutilizabile

```python
from langchain_core.prompts import ChatPromptTemplate

# Define a template for code explanations
template = ChatPromptTemplate.from_messages([
    ("system", "You are an expert programming instructor. Explain concepts clearly with examples."),
    ("human", "Explain {concept} in {language} with a practical example for {skill_level} developers")
])

# Use the template with different values
questions = [
    {"concept": "functions", "language": "JavaScript", "skill_level": "beginner"},
    {"concept": "classes", "language": "Python", "skill_level": "intermediate"},
    {"concept": "async/await", "language": "JavaScript", "skill_level": "advanced"}
]

for question in questions:
    prompt = template.format_messages(**question)
    response = llm.invoke(prompt)
    print(f"Topic: {question['concept']}\n{response.content}\n---\n")
```

**De ce vei iubi utilizarea 탳abloanelor:**
- **Men탵ine** prompturile consistente 칥n 칥ntreaga aplica탵ie
- **F캒r캒 mai mult** concatenare dezordonat캒 de 탳iruri - doar variabile curate 탳i simple
- **AI-ul t캒u** se comport캒 previzibil deoarece structura r캒m칙ne aceea탳i
- **Actualiz캒rile** sunt simple - modifici 탳ablonul o dat캒 탳i este rezolvat peste tot

## Output structurat

Te-ai frustrat vreodat캒 칥ncerc칙nd s캒 analizezi r캒spunsuri AI care vin sub form캒 de text nestructurat? Outputul structurat este ca 탳i cum ai 칥nv캒탵a AI-ul s캒 urmeze abordarea sistematic캒 pe care Linnaeus a folosit-o pentru clasificarea biologic캒 - organizat, previzibil 탳i u탳or de lucrat. Po탵i solicita JSON, structuri de date specifice sau orice format de care ai nevoie.

### Definirea schemelor de output

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser
from pydantic import BaseModel, Field

class CodeReview(BaseModel):
    score: int = Field(description="Code quality score from 1-10")
    strengths: list[str] = Field(description="List of code strengths")
    improvements: list[str] = Field(description="List of suggested improvements")
    overall_feedback: str = Field(description="Summary feedback")

# Set up the parser
parser = JsonOutputParser(pydantic_object=CodeReview)

# Create prompt with format instructions
prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a code reviewer. {format_instructions}"),
    ("human", "Review this code: {code}")
])

# Format the prompt with instructions
chain = prompt | llm | parser

# Get structured response
code_sample = """
def calculate_average(numbers):
    return sum(numbers) / len(numbers)
"""

result = chain.invoke({
    "code": code_sample,
    "format_instructions": parser.get_format_instructions()
})

print(f"Score: {result['score']}")
print(f"Strengths: {', '.join(result['strengths'])}")
```

**De ce outputul structurat este revolu탵ionar:**
- **Nu mai trebuie** s캒 ghice탳ti ce format vei primi - este consistent de fiecare dat캒
- **Se integreaz캒** direct 칥n bazele tale de date 탳i API-uri f캒r캒 munc캒 suplimentar캒
- **Detecteaz캒** r캒spunsuri ciudate ale AI 칥nainte s캒 칥탵i strice aplica탵ia
- **Face** codul t캒u mai curat, deoarece 탳tii exact cu ce lucrezi

## Apelarea instrumentelor

Acum ajungem la una dintre cele mai puternice func탵ii: instrumentele. A탳a 칥i oferi AI-ului t캒u capacit캒탵i practice dincolo de conversa탵ie. La fel cum breslele medievale au dezvoltat instrumente specializate pentru meserii specifice, po탵i echipa AI-ul t캒u cu instrumente focalizate. Descrii ce instrumente sunt disponibile, iar c칙nd cineva solicit캒 ceva care se potrive탳te, AI-ul t캒u poate ac탵iona.

### Utilizarea Python

S캒 ad캒ug캒m c칙teva instrumente astfel:

```python
from typing_extensions import Annotated, TypedDict

class add(TypedDict):
    """Add two integers."""

    # Annotations must have the type and can optionally include a default value and description (in that order).
    a: Annotated[int, ..., "First integer"]
    b: Annotated[int, ..., "Second integer"]

tools = [add]

functions = {
    "add": lambda a, b: a + b
}
```

Ce se 칥nt칙mpl캒 aici? Cre캒m un plan pentru un instrument numit `add`. Prin mo탳tenirea de la `TypedDict` 탳i utilizarea acelor tipuri elegante `Annotated` pentru `a` 탳i `b`, oferim LLM-ului o imagine clar캒 despre ce face acest instrument 탳i de ce are nevoie. Dic탵ionarul `functions` este ca o cutie de unelte - spune codului nostru exact ce s캒 fac캒 atunci c칙nd AI-ul decide s캒 foloseasc캒 un instrument specific.

S캒 vedem cum apel캒m LLM-ul cu acest instrument:

```python
llm = ChatOpenAI(
    api_key=os.environ["GITHUB_TOKEN"],
    base_url="https://models.github.ai/inference",
    model="openai/gpt-4o-mini",
)

llm_with_tools = llm.bind_tools(tools)
```

Aici apel캒m `bind_tools` cu array-ul nostru `tools`, iar astfel LLM-ul `llm_with_tools` are acum cuno탳tin탵캒 despre acest instrument.

Pentru a folosi acest nou LLM, putem scrie urm캒torul cod:

```python
query = "What is 3 + 12?"

res = llm_with_tools.invoke(query)
if(res.tool_calls):
    for tool in res.tool_calls:
        print("TOOL CALL: ", functions[tool["name"]](../../../10-ai-framework-project/**tool["args"]))
print("CONTENT: ",res.content)
```

Acum, c칙nd apel캒m `invoke` pe acest nou LLM, care are instrumente, proprietatea `tool_calls` poate fi populat캒. Dac캒 da, orice instrument identificat are o proprietate `name` 탳i `args` care identific캒 ce instrument ar trebui s캒 fie apelat 탳i cu ce argumente. Codul complet arat캒 astfel:

```python
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_openai import ChatOpenAI
import os
from typing_extensions import Annotated, TypedDict

class add(TypedDict):
    """Add two integers."""

    # Annotations must have the type and can optionally include a default value and description (in that order).
    a: Annotated[int, ..., "First integer"]
    b: Annotated[int, ..., "Second integer"]

tools = [add]

functions = {
    "add": lambda a, b: a + b
}

llm = ChatOpenAI(
    api_key=os.environ["GITHUB_TOKEN"],
    base_url="https://models.github.ai/inference",
    model="openai/gpt-4o-mini",
)

llm_with_tools = llm.bind_tools(tools)

query = "What is 3 + 12?"

res = llm_with_tools.invoke(query)
if(res.tool_calls):
    for tool in res.tool_calls:
        print("TOOL CALL: ", functions[tool["name"]](../../../10-ai-framework-project/**tool["args"]))
print("CONTENT: ",res.content)
```

Rul칙nd acest cod, ar trebui s캒 vezi un output similar cu:

```text
TOOL CALL:  15
CONTENT: 
```

AI-ul a examinat "Ce este 3 + 12" 탳i a recunoscut aceasta ca o sarcin캒 pentru instrumentul `add`. La fel cum un bibliotecar priceput 탳tie la ce referin탵캒 s캒 consulte 칥n func탵ie de tipul 칥ntreb캒rii, AI-ul a f캒cut aceast캒 determinare pe baza numelui instrumentului, descrierii 탳i specifica탵iilor c칙mpului. Rezultatul de 15 provine din dic탵ionarul nostru `functions` care execut캒 instrumentul:

```python
print("TOOL CALL: ", functions[tool["name"]](../../../10-ai-framework-project/**tool["args"]))
```

### Un instrument mai interesant care apeleaz캒 un API web

Ad캒ugarea numerelor demonstreaz캒 conceptul, dar instrumentele reale efectueaz캒 de obicei opera탵iuni mai complexe, cum ar fi apelarea API-urilor web. S캒 extindem exemplul nostru pentru ca AI-ul s캒 preia con탵inut de pe internet - similar cu modul 칥n care operatorii de telegraf conectau loca탵ii 칥ndep캒rtate:

```python
class joke(TypedDict):
    """Tell a joke."""

    # Annotations must have the type and can optionally include a default value and description (in that order).
    category: Annotated[str, ..., "The joke category"]

def get_joke(category: str) -> str:
    response = requests.get(f"https://api.chucknorris.io/jokes/random?category={category}", headers={"Accept": "application/json"})
    if response.status_code == 200:
        return response.json().get("value", f"Here's a {category} joke!")
    return f"Here's a {category} joke!"

functions = {
    "add": lambda a, b: a + b,
    "joke": lambda category: get_joke(category)
}

query = "Tell me a joke about animals"

# the rest of the code is the same
```

Acum, dac캒 rulezi acest cod, vei primi un r캒spuns care spune ceva de genul:

```text
TOOL CALL:  Chuck Norris once rode a nine foot grizzly bear through an automatic car wash, instead of taking a shower.
CONTENT:  
```

Iat캒 codul 칥n 칥ntregime:

```python
from langchain_openai import ChatOpenAI
import requests
import os
from typing_extensions import Annotated, TypedDict

class add(TypedDict):
    """Add two integers."""

    # Annotations must have the type and can optionally include a default value and description (in that order).
    a: Annotated[int, ..., "First integer"]
    b: Annotated[int, ..., "Second integer"]

class joke(TypedDict):
    """Tell a joke."""

    # Annotations must have the type and can optionally include a default value and description (in that order).
    category: Annotated[str, ..., "The joke category"]

tools = [add, joke]

def get_joke(category: str) -> str:
    response = requests.get(f"https://api.chucknorris.io/jokes/random?category={category}", headers={"Accept": "application/json"})
    if response.status_code == 200:
        return response.json().get("value", f"Here's a {category} joke!")
    return f"Here's a {category} joke!"

functions = {
    "add": lambda a, b: a + b,
    "joke": lambda category: get_joke(category)
}

llm = ChatOpenAI(
    api_key=os.environ["GITHUB_TOKEN"],
    base_url="https://models.github.ai/inference",
    model="openai/gpt-4o-mini",
)

llm_with_tools = llm.bind_tools(tools)

query = "Tell me a joke about animals"

res = llm_with_tools.invoke(query)
if(res.tool_calls):
    for tool in res.tool_calls:
        # print("TOOL CALL: ", tool)
        print("TOOL CALL: ", functions[tool["name"]](../../../10-ai-framework-project/**tool["args"]))
print("CONTENT: ",res.content)
```

## 칉ncorpor캒ri 탳i procesarea documentelor

칉ncorpor캒rile reprezint캒 una dintre cele mai elegante solu탵ii 칥n AI-ul modern. Imagineaz캒-탵i c캒 po탵i lua orice bucat캒 de text 탳i s캒 o transformi 칥n coordonate numerice care s캒-i capteze semnifica탵ia. Exact asta fac 칥ncorpor캒rile - transform캒 textul 칥n puncte 칥ntr-un spa탵iu multidimensional unde conceptele similare se grupeaz캒 칥mpreun캒. Este ca 탳i cum ai avea un sistem de coordonate pentru idei, asem캒n캒tor modului 칥n care Mendeleev a organizat tabelul periodic dup캒 propriet캒탵ile atomice.

### Crearea 탳i utilizarea 칥ncorpor캒rilor

```python
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import CharacterTextSplitter

# Initialize embeddings
embeddings = OpenAIEmbeddings(
    api_key=os.environ["GITHUB_TOKEN"],
    base_url="https://models.github.ai/inference",
    model="text-embedding-3-small"
)

# Load and split documents
loader = TextLoader("documentation.txt")
documents = loader.load()

text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
texts = text_splitter.split_documents(documents)

# Create vector store
vectorstore = FAISS.from_documents(texts, embeddings)

# Perform similarity search
query = "How do I handle user authentication?"
similar_docs = vectorstore.similarity_search(query, k=3)

for doc in similar_docs:
    print(f"Relevant content: {doc.page_content[:200]}...")
```

### 칉nc캒rc캒toare de documente pentru diverse formate

```python
from langchain_community.document_loaders import (
    PyPDFLoader,
    CSVLoader,
    JSONLoader,
    WebBaseLoader
)

# Load different document types
pdf_loader = PyPDFLoader("manual.pdf")
csv_loader = CSVLoader("data.csv")
json_loader = JSONLoader("config.json")
web_loader = WebBaseLoader("https://example.com/docs")

# Process all documents
all_documents = []
for loader in [pdf_loader, csv_loader, json_loader, web_loader]:
    docs = loader.load()
    all_documents.extend(docs)
```

**Ce po탵i face cu 칥ncorpor캒rile:**
- **Construie탳te** c캒ut캒ri care 칥n탵eleg cu adev캒rat ce vrei s캒 spui, nu doar potrivirea cuvintelor cheie
- **Creeaz캒** AI care poate r캒spunde la 칥ntreb캒ri despre documentele tale
- **Construie탳te** sisteme de recomandare care sugereaz캒 con탵inut cu adev캒rat relevant
- **Organizeaz캒** 탳i categorizeaz캒 automat con탵inutul t캒u

## Construirea unei aplica탵ii AI complete

Acum vom integra tot ce ai 칥nv캒탵at 칥ntr-o aplica탵ie cuprinz캒toare - un asistent de codare care poate r캒spunde la 칥ntreb캒ri, folosi instrumente 탳i men탵ine memoria conversa탵iei. La fel cum presa tipografic캒 a combinat tehnologii existente (tipar mobil, cerneal캒, h칙rtie 탳i presiune) 칥ntr-un ceva transformator, vom combina componentele AI 칥ntr-un ceva practic 탳i util.

### Exemplu de aplica탵ie complet캒

```python
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage
from langchain_community.vectorstores import FAISS
from typing_extensions import Annotated, TypedDict
import os
import requests

class CodingAssistant:
    def __init__(self):
        self.llm = ChatOpenAI(
            api_key=os.environ["GITHUB_TOKEN"],
            base_url="https://models.github.ai/inference",
            model="openai/gpt-4o-mini"
        )
        
        self.conversation_history = [
            SystemMessage(content="""You are an expert coding assistant. 
            Help users learn programming concepts, debug code, and write better software.
            Use tools when needed and maintain a helpful, encouraging tone.""")
        ]
        
        # Define tools
        self.setup_tools()
    
    def setup_tools(self):
        class web_search(TypedDict):
            """Search for programming documentation or examples."""
            query: Annotated[str, "Search query for programming help"]
        
        class code_formatter(TypedDict):
            """Format and validate code snippets."""
            code: Annotated[str, "Code to format"]
            language: Annotated[str, "Programming language"]
        
        self.tools = [web_search, code_formatter]
        self.llm_with_tools = self.llm.bind_tools(self.tools)
    
    def chat(self, user_input: str):
        # Add user message to conversation
        self.conversation_history.append(HumanMessage(content=user_input))
        
        # Get AI response
        response = self.llm_with_tools.invoke(self.conversation_history)
        
        # Handle tool calls if any
        if response.tool_calls:
            for tool_call in response.tool_calls:
                tool_result = self.execute_tool(tool_call)
                print(f"游댢 Tool used: {tool_call['name']}")
                print(f"游늵 Result: {tool_result}")
        
        # Add AI response to conversation
        self.conversation_history.append(response)
        
        return response.content
    
    def execute_tool(self, tool_call):
        tool_name = tool_call['name']
        args = tool_call['args']
        
        if tool_name == 'web_search':
            return f"Found documentation for: {args['query']}"
        elif tool_name == 'code_formatter':
            return f"Formatted {args['language']} code: {args['code'][:50]}..."
        
        return "Tool execution completed"

# Usage example
assistant = CodingAssistant()

print("游뱄 Coding Assistant Ready! Type 'quit' to exit.\n")

while True:
    user_input = input("You: ")
    if user_input.lower() == 'quit':
        break
    
    response = assistant.chat(user_input)
    print(f"游뱄 Assistant: {response}\n")
```

**Arhitectura aplica탵iei:**

```mermaid
graph TD
    A[User Input] --> B[Coding Assistant]
    B --> C[Conversation Memory]
    B --> D[Tool Detection]
    B --> E[LLM Processing]
    
    D --> F[Web Search Tool]
    D --> G[Code Formatter Tool]
    
    E --> H[Response Generation]
    F --> H
    G --> H
    
    H --> I[User Interface]
    H --> C
```

**Func탵ii cheie pe care le-am implementat:**
- **칉탳i aminte탳te** 칥ntreaga conversa탵ie pentru continuitatea contextului
- **Execut캒 ac탵iuni** prin apelarea instrumentelor, nu doar conversa탵ie
- **Urmeaz캒** modele de interac탵iune previzibile
- **Gestioneaz캒** automat erorile 탳i fluxurile de lucru complexe

## Tem캒: Construie탳te propriul t캒u asistent de studiu bazat pe AI

**Obiectiv**: Creeaz캒 o aplica탵ie AI care s캒 aj
3. **칉nv캒탵are personalizat캒**: Folose탳te mesaje de sistem pentru a adapta r캒spunsurile la diferite niveluri de competen탵캒  
4. **Formatarea r캒spunsurilor**: Implementeaz캒 ie탳iri structurate pentru 칥ntreb캒rile de tip quiz  

### Pa탳i de implementare  

**Pasul 1: Configurarea mediului de lucru**  
```bash
pip install langchain langchain-openai
```
  
**Pasul 2: Func탵ionalitate de baz캒 pentru chat**  
- Creeaz캒 o clas캒 `StudyAssistant`  
- Implementeaz캒 memoria conversa탵iei  
- Adaug캒 configurarea personalit캒탵ii pentru suport educa탵ional  

**Pasul 3: Adaug캒 instrumente educa탵ionale**  
- **Explicator de cod**: Descompune codul 칥n p캒r탵i u탳or de 칥n탵eles  
- **Generator de quiz-uri**: Creeaz캒 칥ntreb캒ri despre concepte de programare  
- **Tracker de progres**: Urm캒re탳te subiectele acoperite  

**Pasul 4: Func탵ionalit캒탵i avansate (Op탵ional)**  
- Implementeaz캒 r캒spunsuri 칥n flux pentru o experien탵캒 mai bun캒 a utilizatorului  
- Adaug캒 칥nc캒rcarea documentelor pentru a 칥ncorpora materiale de curs  
- Creeaz캒 embeddings pentru recuperarea con탵inutului bazat캒 pe similaritate  

### Criterii de evaluare  

| Func탵ionalitate | Excelent (4) | Bun (3) | Satisf캒c캒tor (2) | Necesit캒 칥mbun캒t캒탵iri (1) |  
|------------------|--------------|---------|-------------------|---------------------------|  
| **Fluxul conversa탵iei** | R캒spunsuri naturale, con탳tiente de context | Reten탵ie bun캒 a contextului | Conversa탵ie de baz캒 | F캒r캒 memorie 칥ntre schimburi |  
| **Integrarea instrumentelor** | Mai multe instrumente utile care func탵ioneaz캒 perfect | 2+ instrumente implementate corect | 1-2 instrumente de baz캒 | Instrumentele nu sunt func탵ionale |  
| **Calitatea codului** | Curat, bine documentat, gestionare a erorilor | Structur캒 bun캒, ceva documenta탵ie | Func탵ionalitate de baz캒 | Structur캒 slab캒, f캒r캒 gestionare a erorilor |  
| **Valoarea educa탵ional캒** | Foarte util pentru 칥nv캒탵are, adaptiv | Suport bun pentru 칥nv캒탵are | Explica탵ii de baz캒 | Beneficiu educa탵ional limitat |  

### Structura exemplului de cod  

```python
class StudyAssistant:
    def __init__(self, skill_level="beginner"):
        # Initialize LLM, tools, and conversation memory
        pass
    
    def explain_code(self, code, language):
        # Tool: Explain how code works
        pass
    
    def generate_quiz(self, topic, difficulty):
        # Tool: Create practice questions
        pass
    
    def chat(self, user_input):
        # Main conversation interface
        pass

# Example usage
assistant = StudyAssistant(skill_level="intermediate")
response = assistant.chat("Explain how Python functions work")
```
  
**Provoc캒ri bonus:**  
- Adaug캒 capabilit캒탵i de intrare/ie탳ire vocal캒  
- Implementeaz캒 o interfa탵캒 web folosind Streamlit sau Flask  
- Creeaz캒 o baz캒 de cuno탳tin탵e din materialele de curs folosind embeddings  
- Adaug캒 urm캒rirea progresului 탳i trasee de 칥nv캒탵are personalizate  

## Rezumat  

游꿀 Acum ai st캒p칙nit fundamentele dezvolt캒rii unui cadru AI 탳i ai 칥nv캒탵at cum s캒 construie탳ti aplica탵ii AI sofisticate folosind LangChain. La fel ca finalizarea unei ucenicii complete, ai dob칙ndit un set substan탵ial de abilit캒탵i. S캒 revizuim ce ai realizat.  

### Ce ai 칥nv캒탵at  

**Concepte de baz캒 ale cadrului:**  
- **Beneficiile cadrului**: 칉n탵elegerea momentului potrivit pentru a alege cadrele 칥n locul apelurilor directe API  
- **Bazele LangChain**: Configurarea 탳i configurarea conexiunilor modelului AI  
- **Tipuri de mesaje**: Utilizarea `SystemMessage`, `HumanMessage` 탳i `AIMessage` pentru conversa탵ii structurate  

**Func탵ionalit캒탵i avansate:**  
- **Apelarea instrumentelor**: Crearea 탳i integrarea instrumentelor personalizate pentru capabilit캒탵i AI 칥mbun캒t캒탵ite  
- **Memoria conversa탵iei**: Men탵inerea contextului pe parcursul mai multor schimburi de conversa탵ie  
- **R캒spunsuri 칥n flux**: Implementarea livr캒rii r캒spunsurilor 칥n timp real  
- **탲abloane de prompt**: Construirea de prompturi reutilizabile 탳i dinamice  
- **Ie탳ire structurat캒**: Asigurarea r캒spunsurilor AI consistente 탳i u탳or de analizat  
- **Embeddings**: Crearea de c캒ut캒ri semantice 탳i capabilit캒탵i de procesare a documentelor  

**Aplica탵ii practice:**  
- **Construirea aplica탵iilor complete**: Combinarea mai multor func탵ionalit캒탵i 칥n aplica탵ii gata de produc탵ie  
- **Gestionarea erorilor**: Implementarea unei gestion캒ri robuste a erorilor 탳i valid캒rii  
- **Integrarea instrumentelor**: Crearea de instrumente personalizate care extind capabilit캒탵ile AI  

### Concluzii cheie  

> 游꿢 **Aminte탳te-탵i**: Cadrele AI precum LangChain sunt practic prietenii t캒i care ascund complexitatea 탳i sunt pline de func탵ionalit캒탵i. Sunt perfecte atunci c칙nd ai nevoie de memorie conversa탵ional캒, apelarea instrumentelor sau vrei s캒 lucrezi cu mai multe modele AI f캒r캒 s캒-탵i pierzi min탵ile.  

**Cadru decizional pentru integrarea AI:**  

```mermaid
flowchart TD
    A[AI Integration Need] --> B{Simple single query?}
    B -->|Yes| C[Direct API calls]
    B -->|No| D{Need conversation memory?}
    D -->|No| E[SDK Integration]
    D -->|Yes| F{Need tools or complex features?}
    F -->|No| G[Framework with basic setup]
    F -->|Yes| H[Full framework implementation]
    
    C --> I[HTTP requests, minimal dependencies]
    E --> J[Provider SDK, model-specific]
    G --> K[LangChain basic chat]
    H --> L[LangChain with tools, memory, agents]
```
  
### Ce urmeaz캒?  

**칉ncepe s캒 construie탳ti chiar acum:**  
- Ia aceste concepte 탳i construie탳te ceva care te entuziasmeaz캒!  
- Joac캒-te cu diferite modele AI prin LangChain - e ca un teren de joac캒 al modelelor AI  
- Creeaz캒 instrumente care rezolv캒 probleme reale cu care te confrun탵i 칥n munc캒 sau proiecte  

**Preg캒tit pentru nivelul urm캒tor?**  
- **Agen탵i AI**: Construie탳te sisteme AI care pot planifica 탳i executa sarcini complexe pe cont propriu  
- **RAG (Generare augmentat캒 prin recuperare)**: Combin캒 AI cu propriile baze de cuno탳tin탵e pentru aplica탵ii super-puternice  
- **AI multi-modal**: Lucreaz캒 cu text, imagini 탳i audio 칥mpreun캒 - posibilit캒탵ile sunt nelimitate!  
- **Implementare 칥n produc탵ie**: 칉nva탵캒 cum s캒 scalezi aplica탵iile AI 탳i s캒 le monitorizezi 칥n lumea real캒  

**Al캒tur캒-te comunit캒탵ii:**  
- Comunitatea LangChain este fantastic캒 pentru a r캒m칙ne la curent 탳i a 칥nv캒탵a cele mai bune practici  
- Modelele GitHub 칥탵i ofer캒 acces la capabilit캒탵i AI de ultim캒 genera탵ie - perfect pentru experimentare  
- Continu캒 s캒 exersezi cu diferite cazuri de utilizare - fiecare proiect te va 칥nv캒탵a ceva nou  

Acum ai cuno탳tin탵ele necesare pentru a construi aplica탵ii conversa탵ionale inteligente care pot ajuta oamenii s캒 rezolve probleme reale. La fel ca me탳te탳ugarii Rena탳terii care combinau viziunea artistic캒 cu abilit캒탵ile tehnice, acum po탵i 칥mbina capabilit캒탵ile AI cu aplica탵iile practice. 칉ntrebarea este: ce vei crea? 游  

## Provocarea Agentului GitHub Copilot 游  

Folose탳te modul Agent pentru a finaliza urm캒toarea provocare:  

**Descriere:** Construie탳te un asistent avansat de revizuire a codului, alimentat de AI, care combin캒 multiple func탵ionalit캒탵i LangChain, inclusiv apelarea instrumentelor, ie탳irea structurat캒 탳i memoria conversa탵iei, pentru a oferi feedback cuprinz캒tor asupra codului trimis.  

**Prompt:** Creeaz캒 o clas캒 CodeReviewAssistant care implementeaz캒:  
1. Un instrument pentru analizarea complexit캒탵ii codului 탳i sugerarea 칥mbun캒t캒탵irilor  
2. Un instrument pentru verificarea codului conform celor mai bune practici  
3. Ie탳ire structurat캒 folosind modele Pydantic pentru un format consistent de revizuire  
4. Memoria conversa탵iei pentru a urm캒ri sesiunile de revizuire  
5. O interfa탵캒 principal캒 de chat care poate gestiona trimiterea codului 탳i oferi feedback detaliat 탳i ac탵ionabil  

Asistentul ar trebui s캒 poat캒 revizui codul 칥n mai multe limbaje de programare, s캒 men탵in캒 contextul pe parcursul mai multor trimiteri de cod 칥ntr-o sesiune 탳i s캒 ofere at칙t scoruri sumare, c칙t 탳i sugestii detaliate de 칥mbun캒t캒탵ire.  

Afl캒 mai multe despre [modul agent](https://code.visualstudio.com/blogs/2025/02/24/introducing-copilot-agent-mode) aici.  

---

**Declinare de responsabilitate**:  
Acest document a fost tradus folosind serviciul de traducere AI [Co-op Translator](https://github.com/Azure/co-op-translator). De탳i ne str캒duim s캒 asigur캒m acurate탵ea, v캒 rug캒m s캒 fi탵i con탳tien탵i c캒 traducerile automate pot con탵ine erori sau inexactit캒탵i. Documentul original 칥n limba sa natal캒 ar trebui considerat sursa autoritar캒. Pentru informa탵ii critice, se recomand캒 traducerea profesional캒 realizat캒 de oameni. Nu ne asum캒m responsabilitatea pentru ne칥n탵elegerile sau interpret캒rile gre탳ite care pot ap캒rea din utilizarea acestei traduceri.