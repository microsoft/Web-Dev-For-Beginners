# KI-Framework

Hast du dich jemals √ºberw√§ltigt gef√ºhlt, wenn du versucht hast, KI-Anwendungen von Grund auf neu zu erstellen? Du bist nicht allein! KI-Frameworks sind wie ein Schweizer Taschenmesser f√ºr die KI-Entwicklung ‚Äì sie sind leistungsstarke Werkzeuge, die dir Zeit und Nerven sparen k√∂nnen, wenn du intelligente Anwendungen baust. Stell dir ein KI-Framework wie eine gut organisierte Bibliothek vor: Es stellt vorgefertigte Komponenten, standardisierte APIs und clevere Abstraktionen bereit, sodass du dich auf das L√∂sen von Problemen konzentrieren kannst, anstatt dich mit Implementierungsdetails herumzuschlagen.

In dieser Lektion werden wir erkunden, wie Frameworks wie LangChain komplexe KI-Integrationsaufgaben in sauberen, lesbaren Code verwandeln k√∂nnen. Du wirst entdecken, wie du reale Herausforderungen meisterst, wie das Verfolgen von Unterhaltungen, das Implementieren von Tool-Aufrufen und das Jonglieren mit verschiedenen KI-Modellen √ºber eine einheitliche Schnittstelle.

Bis wir fertig sind, wirst du wissen, wann du Frameworks statt roher API-Aufrufe verwenden solltest, wie du deren Abstraktionen effektiv nutzt und wie du KI-Anwendungen baust, die bereit f√ºr den echten Einsatz sind. Lass uns entdecken, was KI-Frameworks f√ºr deine Projekte tun k√∂nnen.

## ‚ö° Was du in den n√§chsten 5 Minuten tun kannst

**Schnellstart-Pfad f√ºr vielbesch√§ftigte Entwickler**

```mermaid
flowchart LR
    A[‚ö° 5 Minuten] --> B[LangChain installieren]
    B --> C[ChatOpenAI-Client erstellen]
    C --> D[Erste Eingabe senden]
    D --> E[framework power sehen]
```
- **Minute 1**: Installiere LangChain: `pip install langchain langchain-openai`
- **Minute 2**: Richte dein GitHub-Token ein und importiere den ChatOpenAI-Client
- **Minute 3**: Erstelle eine einfache Unterhaltung mit System- und Nutzer-Nachrichten
- **Minute 4**: F√ºge ein einfaches Tool hinzu (wie eine Additionsfunktion) und sieh dir den AI-Tool-Aufruf an
- **Minute 5**: Erlebe den Unterschied zwischen rohen API-Aufrufen und Framework-Abstraktionen

**Schnelltest-Code**:
```python
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage

llm = ChatOpenAI(
    api_key=os.environ["GITHUB_TOKEN"],
    base_url="https://models.github.ai/inference",
    model="openai/gpt-4o-mini"
)

response = llm.invoke([
    SystemMessage(content="You are a helpful coding assistant"),
    HumanMessage(content="Explain Python functions briefly")
])
print(response.content)
```

**Warum das wichtig ist**: Innerhalb von 5 Minuten erlebst du, wie KI-Frameworks komplexe KI-Integration in einfache Methodenaufrufe verwandeln. Das ist die Grundlage, die produktive KI-Anwendungen antreibt.

## Warum ein Framework w√§hlen?

Du bist also bereit, eine KI-App zu bauen ‚Äì gro√üartig! Aber hier ist die Sache: Du hast mehrere verschiedene Wege, und jeder hat seine Vor- und Nachteile. Es ist ein bisschen so, als w√ºrdest du zwischen Gehen, Radfahren oder Autofahren w√§hlen ‚Äì alle bringen dich ans Ziel, aber das Erlebnis (und der Aufwand) ist v√∂llig unterschiedlich.

Lass uns die drei Hauptwege betrachten, wie du KI in deine Projekte integrieren kannst:

| Ansatz | Vorteile | Am besten geeignet f√ºr | √úberlegungen |
|----------|------------|----------|--------------|
| **Direkte HTTP-Anfragen** | Volle Kontrolle, keine Abh√§ngigkeiten | Einfache Anfragen, Grundlagen lernen | Ausf√ºhrlicherer Code, manuelle Fehlerbehandlung |
| **SDK-Integration** | Weniger Boilerplate, modell-spezifische Optimierungen | Anwendungen mit nur einem Modell | Auf bestimmte Anbieter beschr√§nkt |
| **KI-Frameworks** | Einheitliche API, eingebaute Abstraktionen | Multi-Modell-Anwendungen, komplexe Workflows | Lernkurve, potenzielle √úberabstraktion |

### Praxisnahe Vorteile von Frameworks

```mermaid
graph TD
    A[Deine Anwendung] --> B[KI-Framework]
    B --> C[OpenAI GPT]
    B --> D[Anthropic Claude]
    B --> E[GitHub-Modelle]
    B --> F[Lokal Modelle]
    
    B --> G[Eingebaute Werkzeuge]
    G --> H[Speicherverwaltung]
    G --> I[Konversationsverlauf]
    G --> J[Funktionsaufruf]
    G --> K[Fehlerbehandlung]
```
**Warum Frameworks wichtig sind:**
- **Vereinheitlicht** mehrere KI-Anbieter unter einer Schnittstelle
- **Handhabt** Konversationsspeicher automatisch
- **Bietet** vorgefertigte Tools f√ºr g√§ngige Aufgaben wie Einbettungen und Funktionsaufrufe
- **Verwaltet** Fehlerbehandlung und Wiederholungslogik
- **Verwandelt** komplexe Workflows in lesbare Methodenaufrufe

> üí° **Profi-Tipp**: Nutze Frameworks, wenn du zwischen verschiedenen KI-Modellen wechselst oder komplexe Features wie Agenten, Speicher oder Tool-Aufrufe baust. Bleibe bei direkten APIs, wenn du Grundlagen lernst oder einfache, fokussierte Anwendungen erstellst.

**Fazit**: √Ñhnlich wie bei der Wahl zwischen speziellen Werkzeugen eines Handwerkers und einer kompletten Werkstatt geht es darum, das richtige Werkzeug f√ºr die Aufgabe zu w√§hlen. Frameworks bieten Vorteile bei komplexen, funktionsreichen Anwendungen, w√§hrend direkte APIs bei einfachen Anwendungsf√§llen gut funktionieren.

## üó∫Ô∏è Deine Lernreise zum Meister von KI-Frameworks

```mermaid
journey
    title Von Roh-APIs bis zu Produktions-KI-Anwendungen
    section Grundlagen des Frameworks
      Vorteile der Abstraktion verstehen: 4: You
      LangChain-Grundlagen meistern: 6: You
      Ans√§tze vergleichen: 7: You
    section Gespr√§chssysteme
      Chat-Oberfl√§chen erstellen: 5: You
      Speicherungsmuster implementieren: 7: You
      Streaming-Antworten handhaben: 8: You
    section Erweiterte Funktionen
      Eigene Werkzeuge erstellen: 6: You
      Strukturierte Ausgaben meistern: 8: You
      Dokumentensysteme erstellen: 8: You
    section Produktionsanwendungen
      Alle Funktionen kombinieren: 7: You
      Fehlerf√§lle handhaben: 8: You
      Komplette Systeme bereitstellen: 9: You
```
**Dein Lernziel**: Am Ende dieser Lektion wirst du die Entwicklung mit KI-Frameworks gemeistert haben und in der Lage sein, anspruchsvolle, produktionsreife KI-Anwendungen zu erstellen, die mit kommerziellen KI-Assistenten mithalten.

## Einf√ºhrung

In dieser Lektion lernen wir:

- Ein g√§ngiges KI-Framework zu verwenden.
- H√§ufige Probleme wie Chat-Konversationen, Tool-Nutzung, Speicher und Kontext anzugehen.
- Das zu nutzen, um KI-Apps zu bauen.

## üß† √ñkosystem der KI-Framework-Entwicklung

```mermaid
mindmap
  root((KI-Frameworks))
    Abstraktionsvorteile
      Code-Vereinfachung
        Einheitliche APIs
        Eingebaute Fehlerbehandlung
        Konsistente Muster
        Reduzierter Boilerplate-Code
      Multi-Modell Unterst√ºtzung
        Anbieterunabh√§ngig
        Einfaches Umschalten
        Fallback-Optionen
        Kostenoptimierung
    Hauptkomponenten
      Gespr√§chsverwaltung
        Nachrichtentypen
        Speichersysteme
        Kontextverfolgung
        Verlaufsspeicherung
      Werkzeugintegration
        Funktionsaufrufe
        API-Verbindungen
        Benutzerdefinierte Werkzeuge
        Workflow-Automatisierung
    Erweiterte Funktionen
      Strukturierte Ausgabe
        Pydantic-Modelle
        JSON-Schemata
        Typsicherheit
        Validierungsregeln
      Dokumentenverarbeitung
        Embeddings
        Vektorspeicher
        √Ñhnlichkeitssuche
        RAG-Systeme
    Produktionsmuster
      Anwendungsarchitektur
        Modulares Design
        Fehlergrenzen
        Async-Operationen
        Statusverwaltung
      Deployment-Strategien
        Skalierbarkeit
        √úberwachung
        Leistung
        Sicherheit
```
**Kernprinzip**: KI-Frameworks abstrahieren Komplexit√§t und bieten dabei leistungsstarke Abstraktionen f√ºr Konversationsmanagement, Tool-Integration und Dokumentenverarbeitung, die es Entwicklern erm√∂glichen, anspruchsvolle KI-Anwendungen mit sauberem, wartbarem Code zu erstellen.

## Dein erster KI-Prompt

Fangen wir mit den Grundlagen an, indem wir deine erste KI-Anwendung erstellen, die eine Frage sendet und eine Antwort zur√ºckerh√§lt. Wie Archimedes, der im Bad das Prinzip der Verdr√§ngung entdeckte, f√ºhren manchmal die einfachsten Beobachtungen zu den m√§chtigsten Einsichten ‚Äì und Frameworks machen diese Einsichten zug√§nglich.

### LangChain mit GitHub-Modellen einrichten

Wir verwenden LangChain, um eine Verbindung zu GitHub Models herzustellen, was ziemlich cool ist, weil du so freien Zugriff auf verschiedene KI-Modelle bekommst. Das Beste daran? Du brauchst nur ein paar einfache Konfigurationsparameter, um loszulegen:

```python
from langchain_openai import ChatOpenAI
import os

llm = ChatOpenAI(
    api_key=os.environ["GITHUB_TOKEN"],
    base_url="https://models.github.ai/inference",
    model="openai/gpt-4o-mini",
)

# Senden Sie eine einfache Eingabeaufforderung
response = llm.invoke("What's the capital of France?")
print(response.content)
```

**Was hier passiert:**
- **Erstellt** einen LangChain-Client mit der Klasse `ChatOpenAI` ‚Äì dein Tor zur KI!
- **Konfiguriert** die Verbindung zu GitHub Models mit deinem Authentifizierungstoken
- **Legt fest**, welches KI-Modell verwendet wird (`gpt-4o-mini`) ‚Äì das ist wie die Wahl deines KI-Assistenten
- **Sendet** deine Frage mittels der Methode `invoke()` ‚Äì hier passiert die Magie
- **Extrahiert** und zeigt die Antwort an ‚Äì voil√†, du chattest mit KI!

> üîß **Einrichtungshinweis**: Wenn du GitHub Codespaces benutzt, hast du Gl√ºck ‚Äì das `GITHUB_TOKEN` ist bereits f√ºr dich eingerichtet! Arbeitet du lokal? Kein Problem, du musst lediglich einen pers√∂nlichen Zugriffstoken mit den richtigen Berechtigungen erstellen.

**Erwartete Ausgabe:**
```text
The capital of France is Paris.
```

```mermaid
sequenceDiagram
    participant App as Deine Python-App
    participant LC as LangChain
    participant GM as GitHub-Modelle
    participant AI as GPT-4o-mini
    
    App->>LC: llm.invoke("Was ist die Hauptstadt von Frankreich?")
    LC->>GM: HTTP-Anfrage mit Aufforderung
    GM->>AI: Aufforderung verarbeiten
    AI->>GM: Generierte Antwort
    GM->>LC: Antwort zur√ºckgeben
    LC->>App: response.content
```
## Aufbau konversationaler KI

Das erste Beispiel zeigt die Grundlagen, aber es ist nur ein einzelner Austausch ‚Äì du stellst eine Frage, bekommst eine Antwort, und das war‚Äôs. In realen Anwendungen m√∂chtest du, dass deine KI sich daran erinnert, was ihr besprochen habt, genau wie Watson und Holmes ihre Ermittlungsunterhaltungen im Laufe der Zeit aufbauten.

Hier wird LangChain besonders n√ºtzlich. Es bietet verschiedene Nachrichtentypen, die helfen, Unterhaltungen zu strukturieren und deiner KI eine Pers√∂nlichkeit zu verleihen. Du wirst Chat-Erlebnisse bauen, die Kontext und Charakter aufrechterhalten.

### Verstehen der Nachrichtentypen

Denk an diese Nachrichtentypen wie verschieden ‚ÄûH√ºte‚Äú, die Teilnehmer in einem Gespr√§ch tragen. LangChain verwendet unterschiedliche Nachrichtentypen, um nachzuvollziehen, wer was sagt:

| Nachrichtentyp | Zweck | Beispielanwendung |
|--------------|---------|------------------|
| `SystemMessage` | Definiert KI-Pers√∂nlichkeit und Verhalten | ‚ÄûDu bist ein hilfreicher Coding-Assistent‚Äú |
| `HumanMessage` | Repr√§sentiert Nutzereingaben | ‚ÄûErkl√§re, wie Funktionen funktionieren‚Äú |
| `AIMessage` | Speichert KI-Antworten | Vorherige KI-Antworten im Gespr√§ch |

### Deine erste Unterhaltung erstellen

Lass uns eine Unterhaltung erstellen, in der unsere KI eine bestimmte Rolle annimmt. Sie soll Captain Picard verk√∂rpern ‚Äì eine Figur, bekannt f√ºr diplomatische Weisheit und F√ºhrung:

```python
messages = [
    SystemMessage(content="You are Captain Picard of the Starship Enterprise"),
    HumanMessage(content="Tell me about you"),
]
```

**Was diese Unterhaltungseinrichtung bewirkt:**
- **Legt** die Rolle und Pers√∂nlichkeit der KI √ºber `SystemMessage` fest
- **Gibt** die erste Nutzeranfrage √ºber `HumanMessage` vor
- **Schafft** eine Grundlage f√ºr mehrstufige Gespr√§che

Der vollst√§ndige Code f√ºr dieses Beispiel sieht folgenderma√üen aus:

```python
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_openai import ChatOpenAI
import os

llm = ChatOpenAI(
    api_key=os.environ["GITHUB_TOKEN"],
    base_url="https://models.github.ai/inference",
    model="openai/gpt-4o-mini",
)

messages = [
    SystemMessage(content="You are Captain Picard of the Starship Enterprise"),
    HumanMessage(content="Tell me about you"),
]


# funktioniert
response  = llm.invoke(messages)
print(response.content)
```

Du solltest ein Ergebnis √§hnlich dem folgenden sehen:

```text
I am Captain Jean-Luc Picard, the commanding officer of the USS Enterprise (NCC-1701-D), a starship in the United Federation of Planets. My primary mission is to explore new worlds, seek out new life and new civilizations, and boldly go where no one has gone before. 

I believe in the importance of diplomacy, reason, and the pursuit of knowledge. My crew is diverse and skilled, and we often face challenges that test our resolve, ethics, and ingenuity. Throughout my career, I have encountered numerous species, grappled with complex moral dilemmas, and have consistently sought peaceful solutions to conflicts.

I hold the ideals of the Federation close to my heart, believing in the importance of cooperation, understanding, and respect for all sentient beings. My experiences have shaped my leadership style, and I strive to be a thoughtful and just captain. How may I assist you further?
```

Um die Gespr√§chskontinuit√§t zu wahren (anstatt den Kontext bei jeder Nachricht zur√ºckzusetzen), musst du die Antworten weiterhin zu deiner Nachrichtenliste hinzuf√ºgen. So wie m√ºndliche Traditionen Geschichten √ºber Generationen bewahren, baut dieser Ansatz ein dauerhaftes Ged√§chtnis auf:

```python
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_openai import ChatOpenAI
import os

llm = ChatOpenAI(
    api_key=os.environ["GITHUB_TOKEN"],
    base_url="https://models.github.ai/inference",
    model="openai/gpt-4o-mini",
)

messages = [
    SystemMessage(content="You are Captain Picard of the Starship Enterprise"),
    HumanMessage(content="Tell me about you"),
]


# funktioniert
response  = llm.invoke(messages)

print(response.content)

print("---- Next ----")

messages.append(response)
messages.append(HumanMessage(content="Now that I know about you, I'm Chris, can I be in your crew?"))

response  = llm.invoke(messages)

print(response.content)

```

Ziemlich genial, oder? Hier rufen wir das LLM zweimal auf ‚Äì zuerst nur mit unseren anf√§nglichen zwei Nachrichten, dann nochmals mit dem gesamten Gespr√§chsverlauf. Es ist, als w√ºrde die KI wirklich unserem Chat folgen!

Wenn du diesen Code ausf√ºhrst, erh√§ltst du eine zweite Antwort, die in etwa so klingt:

```text
Welcome aboard, Chris! It's always a pleasure to meet those who share a passion for exploration and discovery. While I cannot formally offer you a position on the Enterprise right now, I encourage you to pursue your aspirations. We are always in need of talented individuals with diverse skills and backgrounds. 

If you are interested in space exploration, consider education and training in the sciences, engineering, or diplomacy. The values of curiosity, resilience, and teamwork are crucial in Starfleet. Should you ever find yourself on a starship, remember to uphold the principles of the Federation: peace, understanding, and respect for all beings. Your journey can lead you to remarkable adventures, whether in the stars or on the ground. Engage!
```

```mermaid
sequenceDiagram
    participant User
    participant App
    participant LangChain
    participant AI
    
    User->>App: "Erz√§hl mir von dir"
    App->>LangChain: [SystemMessage, HumanMessage]
    LangChain->>AI: Formatierte Unterhaltung
    AI->>LangChain: Captain Picard Antwort
    LangChain->>App: AIMessage Objekt
    App->>User: Antwort anzeigen
    
    Note over App: AIMessage zur Unterhaltung hinzuf√ºgen
    
    User->>App: "Kann ich deiner Crew beitreten?"
    App->>LangChain: [SystemMessage, HumanMessage, AIMessage, HumanMessage]
    LangChain->>AI: Vollst√§ndiger Konversationskontext
    AI->>LangChain: Kontextbezogene Antwort
    LangChain->>App: Neue AIMessage
    App->>User: Kontextbezogene Antwort anzeigen
```
Das nehme ich mal als ein Vielleicht ;)

## Streaming-Antworten

Ist dir schon mal aufgefallen, wie ChatGPT seine Antworten quasi ‚Äûin Echtzeit tippt‚Äú? Das ist Streaming in Aktion. Wie einen erfahrenen Kalligraphen zu beobachten, der Buchstabe f√ºr Buchstabe schreibt, statt dass der Text sofort erscheint ‚Äì Streaming l√§sst die Interaktion nat√ºrlicher wirken und liefert sofortiges Feedback.

### Streaming mit LangChain umsetzen

```python
from langchain_openai import ChatOpenAI
import os

llm = ChatOpenAI(
    api_key=os.environ["GITHUB_TOKEN"],
    base_url="https://models.github.ai/inference",
    model="openai/gpt-4o-mini",
    streaming=True
)

# Die Antwort streamen
for chunk in llm.stream("Write a short story about a robot learning to code"):
    print(chunk.content, end="", flush=True)
```

**Warum Streaming gro√üartig ist:**
- **Zeigt** Inhalte w√§hrend der Erstellung ‚Äì kein unangenehmes Warten mehr!
- **L√§sst** Nutzer sp√ºren, dass etwas tats√§chlich geschieht
- **Wirkt** schneller, selbst wenn es technisch nicht so ist
- **Erm√∂glicht** Nutzern, schon mit dem Lesen zu beginnen, w√§hrend die KI noch ‚Äûdenkt‚Äú

> üí° **Benutzererfahrungstipp**: Streaming ist besonders toll bei l√§ngeren Antworten wie Code-Erkl√§rungen, kreativem Schreiben oder detaillierten Tutorials. Deine Nutzer werden es lieben, Fortschritte zu sehen, statt auf einem leeren Bildschirm zu warten!

### üéØ P√§dagogischer Check-in: Vorteile der Framework-Abstraktionen

**Pause und Nachdenken**: Du hast gerade die Kraft von KI-Framework-Abstraktionen erlebt. Vergleiche, was du gelernt hast, mit rohen API-Aufrufen aus fr√ºheren Lektionen.

**Kurze Selbstbewertung**:
- Kannst du erkl√§ren, wie LangChain das Konversationsmanagement im Vergleich zur manuellen Nachrichtenverfolgung vereinfacht?
- Was ist der Unterschied zwischen den Methoden `invoke()` und `stream()`, und wann w√ºrdest du welche verwenden?
- Wie verbessert das Nachrichtentyp-System des Frameworks die Code-Organisation?

**Bezug zur Praxis**: Die Abstraktionsmuster, die du gelernt hast (Nachrichtentypen, Streaming-Schnittstellen, Konversationsspeicher), werden in allen gro√üen KI-Anwendungen verwendet ‚Äì von der ChatGPT-Oberfl√§che bis zur Code-Unterst√ºtzung von GitHub Copilot. Du meisterst genau die architektonischen Muster, die von professionellen KI-Entwicklungsteams verwendet werden.

**Herausforderungsfrage**: Wie w√ºrdest du eine Framework-Abstraktion entwerfen, um verschiedene KI-Modell-Anbieter (OpenAI, Anthropic, Google) mit nur einer Schnittstelle zu verwalten? √úberlege dir Vor- und Nachteile.

## Prompt-Vorlagen

Prompt-Vorlagen funktionieren wie rhetorische Strukturen in der klassischen Rhetorik ‚Äì denk daran, wie Cicero seine Redemuster f√ºr unterschiedliche Zuh√∂rer anpasste, dabei aber den gleichen √ºberzeugenden Rahmen beibehielt. Sie erm√∂glichen es dir, wiederverwendbare Prompts zu erstellen, bei denen du verschiedene Informationsst√ºcke austauschen kannst, ohne alles neu schreiben zu m√ºssen. Sobald die Vorlage eingerichtet ist, f√ºllst du einfach die Variablen mit den gew√ºnschten Werten.

### Wiederverwendbare Prompts erstellen

```python
from langchain_core.prompts import ChatPromptTemplate

# Definieren Sie eine Vorlage f√ºr Codeerkl√§rungen
template = ChatPromptTemplate.from_messages([
    ("system", "You are an expert programming instructor. Explain concepts clearly with examples."),
    ("human", "Explain {concept} in {language} with a practical example for {skill_level} developers")
])

# Verwenden Sie die Vorlage mit verschiedenen Werten
questions = [
    {"concept": "functions", "language": "JavaScript", "skill_level": "beginner"},
    {"concept": "classes", "language": "Python", "skill_level": "intermediate"},
    {"concept": "async/await", "language": "JavaScript", "skill_level": "advanced"}
]

for question in questions:
    prompt = template.format_messages(**question)
    response = llm.invoke(prompt)
    print(f"Topic: {question['concept']}\n{response.content}\n---\n")
```

**Warum du Vorlagen lieben wirst:**
- **Halten** deine Prompts konsistent √ºber deine gesamte App
- **Kein** un√ºbersichtliches String-Verketten mehr ‚Äì nur noch saubere, einfache Variablen
- **Deine KI** verh√§lt sich vorhersehbar, weil die Struktur gleich bleibt
- **Updates** sind ein Kinderspiel ‚Äì Vorlage einmal √§ndern, und √ºberall ist es aktualisiert

## Strukturierte Ausgabe

Kennst du das Frustgef√ºhl, wenn du versuchen musst, unstrukturierte KI-Antworten zu parsen? Strukturierte Ausgabe ist wie das Lehren deiner KI, der systematischen Herangehensweise von Linn√© bei der biologischen Klassifikation zu folgen ‚Äì organisiert, vorhersehbar und einfach handhabbar. Du kannst JSON, bestimmte Datenstrukturen oder jedes gew√ºnschte Format anfordern.

### Ausgabe-Schemata definieren

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser
from pydantic import BaseModel, Field

class CodeReview(BaseModel):
    score: int = Field(description="Code quality score from 1-10")
    strengths: list[str] = Field(description="List of code strengths")
    improvements: list[str] = Field(description="List of suggested improvements")
    overall_feedback: str = Field(description="Summary feedback")

# Parser einrichten
parser = JsonOutputParser(pydantic_object=CodeReview)

# Eingabeaufforderung mit Formatierungsanweisungen erstellen
prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a code reviewer. {format_instructions}"),
    ("human", "Review this code: {code}")
])

# Die Eingabeaufforderung mit Anweisungen formatieren
chain = prompt | llm | parser

# Strukturierte Antwort erhalten
code_sample = """
def calculate_average(numbers):
    return sum(numbers) / len(numbers)
"""

result = chain.invoke({
    "code": code_sample,
    "format_instructions": parser.get_format_instructions()
})

print(f"Score: {result['score']}")
print(f"Strengths: {', '.join(result['strengths'])}")
```

**Warum strukturierte Ausgabe ein Game-Changer ist:**
- **Kein** R√§tselraten mehr, in welchem Format die Antwort kommt ‚Äì es ist jedes Mal konsistent
- **L√§sst** sich direkt in deine Datenbanken und APIs einbinden, ohne Zusatzaufwand
- **F√§ngt** merkw√ºrdige KI-Antworten ab, bevor sie deine App zerst√∂ren
- **Macht** deinen Code sauberer, weil du genau wei√üt, womit du arbeitest

## Tool-Aufrufe

Jetzt kommen wir zu einem der m√§chtigsten Features: Tools. Damit gibst du deiner KI praktische F√§higkeiten √ºber reine Konversation hinaus. So wie mittelalterliche Z√ºnfte spezialisierte Werkzeuge f√ºr bestimmte Gewerke entwickelten, kannst du deine KI mit gezielten Instrumenten ausstatten. Du beschreibst, welche Tools verf√ºgbar sind, und wenn jemand etwas anfragt, das passt, kann deine KI aktiv werden.

### Verwendung in Python

Lass uns einige Tools wie folgt hinzuf√ºgen:

```python
from typing_extensions import Annotated, TypedDict

class add(TypedDict):
    """Add two integers."""

    # Annotationen m√ºssen den Typ haben und k√∂nnen optional einen Standardwert und eine Beschreibung enthalten (in dieser Reihenfolge).
    a: Annotated[int, ..., "First integer"]
    b: Annotated[int, ..., "Second integer"]

tools = [add]

functions = {
    "add": lambda a, b: a + b
}
```

Was passiert hier? Wir erstellen eine Blaupause f√ºr ein Tool namens `add`. Indem wir von `TypedDict` erben und diese schicken `Annotated`-Typen f√ºr `a` und `b` verwenden, geben wir dem LLM ein klares Bild davon, was das Tool macht und was es braucht. Das `functions`-Dictionary ist wie unser Werkzeugkasten ‚Äì es sagt unserem Code genau, was zu tun ist, wenn die KI beschlie√üt, ein bestimmtes Tool zu verwenden.

Als n√§chstes sehen wir, wie wir das LLM mit diesem Tool aufrufen:

```python
llm = ChatOpenAI(
    api_key=os.environ["GITHUB_TOKEN"],
    base_url="https://models.github.ai/inference",
    model="openai/gpt-4o-mini",
)

llm_with_tools = llm.bind_tools(tools)
```

Hier rufen wir `bind_tools` mit unserem `tools`-Array auf, und dadurch wei√ü das LLM `llm_with_tools` jetzt √ºber dieses Tool Bescheid.

Um dieses neue LLM zu verwenden, k√∂nnen wir folgenden Code schreiben:

```python
query = "What is 3 + 12?"

res = llm_with_tools.invoke(query)
if(res.tool_calls):
    for tool in res.tool_calls:
        print("TOOL CALL: ", functions[tool["name"]](../../../10-ai-framework-project/**tool["args"]))
print("CONTENT: ",res.content)
```

Jetzt, wo wir `invoke` auf diesem neuen llm aufrufen, das Tools hat, ist m√∂glicherweise das Property `tool_calls` gef√ºllt. Falls ja, hat jedes identifizierte Tool eine `name`- und `args`-Eigenschaft, die angibt, welches Tool aufgerufen werden soll und mit welchen Argumenten. Der vollst√§ndige Code sieht so aus:

```python
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_openai import ChatOpenAI
import os
from typing_extensions import Annotated, TypedDict

class add(TypedDict):
    """Add two integers."""

    # Annotationen m√ºssen den Typ haben und k√∂nnen optional einen Standardwert und eine Beschreibung enthalten (in dieser Reihenfolge).
    a: Annotated[int, ..., "First integer"]
    b: Annotated[int, ..., "Second integer"]

tools = [add]

functions = {
    "add": lambda a, b: a + b
}

llm = ChatOpenAI(
    api_key=os.environ["GITHUB_TOKEN"],
    base_url="https://models.github.ai/inference",
    model="openai/gpt-4o-mini",
)

llm_with_tools = llm.bind_tools(tools)

query = "What is 3 + 12?"

res = llm_with_tools.invoke(query)
if(res.tool_calls):
    for tool in res.tool_calls:
        print("TOOL CALL: ", functions[tool["name"]](../../../10-ai-framework-project/**tool["args"]))
print("CONTENT: ",res.content)
```

Wenn du diesen Code ausf√ºhrst, solltest du eine Ausgabe √§hnlich dieser sehen:

```text
TOOL CALL:  15
CONTENT: 
```

Die KI hat die Anfrage ‚ÄûWas ist 3 + 12‚Äú gepr√ºft und erkannt, dass dies eine Aufgabe f√ºr das `add`-Tool ist. Wie ein erfahrener Bibliothekar, der je nach Art der Frage wei√ü, welche Referenz er zurate ziehen muss, hat die KI dies anhand des Namens, der Beschreibung und der Feldspezifikationen des Tools bestimmt. Das Ergebnis von 15 stammt aus der Ausf√ºhrung des Tools durch unser `functions`-Dictionary:

```python
print("TOOL CALL: ", functions[tool["name"]](../../../10-ai-framework-project/**tool["args"]))
```

### Ein interessanteres Tool, das eine Web-API aufruft


Zahlen hinzuf√ºgen veranschaulicht das Konzept, aber reale Werkzeuge f√ºhren typischerweise komplexere Operationen aus, wie das Aufrufen von Web-APIs. Erweitern wir unser Beispiel, sodass die KI Inhalte aus dem Internet abruft ‚Äì √§hnlich wie Telegrafisten fr√ºher entfernte Orte verbanden:

```python
class joke(TypedDict):
    """Tell a joke."""

    # Annotationen m√ºssen den Typ haben und k√∂nnen optional einen Standardwert und eine Beschreibung (in dieser Reihenfolge) enthalten.
    category: Annotated[str, ..., "The joke category"]

def get_joke(category: str) -> str:
    response = requests.get(f"https://api.chucknorris.io/jokes/random?category={category}", headers={"Accept": "application/json"})
    if response.status_code == 200:
        return response.json().get("value", f"Here's a {category} joke!")
    return f"Here's a {category} joke!"

functions = {
    "add": lambda a, b: a + b,
    "joke": lambda category: get_joke(category)
}

query = "Tell me a joke about animals"

# der Rest des Codes ist gleich
```

Wenn du diesen Code nun ausf√ºhrst, erh√§ltst du eine Antwort, die etwa Folgendes sagt:

```text
TOOL CALL:  Chuck Norris once rode a nine foot grizzly bear through an automatic car wash, instead of taking a shower.
CONTENT:  
```

```mermaid
flowchart TD
    A[Benutzeranfrage: "Erz√§hle mir einen Witz √ºber Tiere"] --> B[LangChain Analyse]
    B --> C{Werkzeug verf√ºgbar?}
    C -->|Ja| D[Witz-Werkzeug ausw√§hlen]
    C -->|Nein| E[Direkte Antwort generieren]
    
    D --> F[Parameter extrahieren]
    F --> G[Rufe Witz(category="animals") auf]
    G --> H[API-Anfrage an chucknorris.io]
    H --> I[Witzinhalt zur√ºckgeben]
    I --> J[Benutzer anzeigen]
    
    E --> K[KI-generierte Antwort]
    K --> J
    
    subgraph "Werkzeug-Definitionsebene"
        L[TypedDict Schema]
        M[Funktionsimplementierung]
        N[Parameter√ºberpr√ºfung]
    end
    
    D --> L
    F --> N
    G --> M
```
Hier ist der komplette Code:

```python
from langchain_openai import ChatOpenAI
import requests
import os
from typing_extensions import Annotated, TypedDict

class add(TypedDict):
    """Add two integers."""

    # Anmerkungen m√ºssen den Typ haben und k√∂nnen optional einen Standardwert und eine Beschreibung enthalten (in dieser Reihenfolge).
    a: Annotated[int, ..., "First integer"]
    b: Annotated[int, ..., "Second integer"]

class joke(TypedDict):
    """Tell a joke."""

    # Anmerkungen m√ºssen den Typ haben und k√∂nnen optional einen Standardwert und eine Beschreibung enthalten (in dieser Reihenfolge).
    category: Annotated[str, ..., "The joke category"]

tools = [add, joke]

def get_joke(category: str) -> str:
    response = requests.get(f"https://api.chucknorris.io/jokes/random?category={category}", headers={"Accept": "application/json"})
    if response.status_code == 200:
        return response.json().get("value", f"Here's a {category} joke!")
    return f"Here's a {category} joke!"

functions = {
    "add": lambda a, b: a + b,
    "joke": lambda category: get_joke(category)
}

llm = ChatOpenAI(
    api_key=os.environ["GITHUB_TOKEN"],
    base_url="https://models.github.ai/inference",
    model="openai/gpt-4o-mini",
)

llm_with_tools = llm.bind_tools(tools)

query = "Tell me a joke about animals"

res = llm_with_tools.invoke(query)
if(res.tool_calls):
    for tool in res.tool_calls:
        # print("TOOL AUFRUF: ", tool)
        print("TOOL CALL: ", functions[tool["name"]](../../../10-ai-framework-project/**tool["args"]))
print("CONTENT: ",res.content)
```

## Embeddings und Dokumentenverarbeitung

Embeddings stellen eine der elegantesten L√∂sungen in der modernen KI dar. Stell dir vor, du k√∂nntest jeden beliebigen Text in numerische Koordinaten umwandeln, die dessen Bedeutung erfassen. Genau das machen Embeddings ‚Äì sie transformieren Text in Punkte in einem mehrdimensionalen Raum, in dem sich √§hnliche Konzepte gruppieren. Es ist wie ein Koordinatensystem f√ºr Ideen, das an die Art erinnert, wie Mendelejew das Periodensystem nach atomaren Eigenschaften organisierte.

### Embeddings erstellen und verwenden

```python
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import CharacterTextSplitter

# Einbettungen initialisieren
embeddings = OpenAIEmbeddings(
    api_key=os.environ["GITHUB_TOKEN"],
    base_url="https://models.github.ai/inference",
    model="text-embedding-3-small"
)

# Dokumente laden und aufteilen
loader = TextLoader("documentation.txt")
documents = loader.load()

text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
texts = text_splitter.split_documents(documents)

# Vektorenspeicher erstellen
vectorstore = FAISS.from_documents(texts, embeddings)

# √Ñhnlichkeitssuche durchf√ºhren
query = "How do I handle user authentication?"
similar_docs = vectorstore.similarity_search(query, k=3)

for doc in similar_docs:
    print(f"Relevant content: {doc.page_content[:200]}...")
```

### Dokumentenladefunktionen f√ºr verschiedene Formate

```python
from langchain_community.document_loaders import (
    PyPDFLoader,
    CSVLoader,
    JSONLoader,
    WebBaseLoader
)

# Verschiedene Dokumenttypen laden
pdf_loader = PyPDFLoader("manual.pdf")
csv_loader = CSVLoader("data.csv")
json_loader = JSONLoader("config.json")
web_loader = WebBaseLoader("https://example.com/docs")

# Alle Dokumente verarbeiten
all_documents = []
for loader in [pdf_loader, csv_loader, json_loader, web_loader]:
    docs = loader.load()
    all_documents.extend(docs)
```

**Was du mit Embeddings machen kannst:**
- **Suchfunktionen** erstellen, die wirklich verstehen, was du meinst, nicht nur Stichwort√ºbereinstimmungen
- **KI entwickeln**, die Fragen zu deinen Dokumenten beantworten kann
- **Empfehlungssysteme bauen**, die wirklich relevante Inhalte vorschlagen
- **Inhalte automatisch** organisieren und kategorisieren

```mermaid
flowchart LR
    A[Dokumente] --> B[Textteiler]
    B --> C[Erstelle Einbettungen]
    C --> D[Vektorspeicher]
    
    E[Benutzeranfrage] --> F[Anfrage-Einbettung]
    F --> G[√Ñhnlichkeitssuche]
    G --> D
    D --> H[Relevante Dokumente]
    H --> I[KI-Antwort]
    
    subgraph "Vektorraum"
        J[Dokument A: [0.1, 0.8, 0.3...]]
        K[Dokument B: [0.2, 0.7, 0.4...]]
        L[Anfrage: [0.15, 0.75, 0.35...]]
    end
    
    C --> J
    C --> K
    F --> L
    G --> J
    G --> K
```
## Aufbau einer vollst√§ndigen KI-Anwendung

Jetzt integrieren wir alles, was du gelernt hast, in eine umfassende Anwendung ‚Äì einen Programmierassistenten, der Fragen beantworten, Werkzeuge nutzen und sich an Gespr√§che erinnern kann. So wie die Druckerpresse bestehende Technologien (bewegliche Lettern, Tinte, Papier und Druck) zu etwas Transformativem verband, kombinieren wir unsere KI-Komponenten zu einer praktikablen und n√ºtzlichen Anwendung.

### Beispiel f√ºr eine vollst√§ndige Anwendung

```python
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage
from langchain_community.vectorstores import FAISS
from typing_extensions import Annotated, TypedDict
import os
import requests

class CodingAssistant:
    def __init__(self):
        self.llm = ChatOpenAI(
            api_key=os.environ["GITHUB_TOKEN"],
            base_url="https://models.github.ai/inference",
            model="openai/gpt-4o-mini"
        )
        
        self.conversation_history = [
            SystemMessage(content="""You are an expert coding assistant. 
            Help users learn programming concepts, debug code, and write better software.
            Use tools when needed and maintain a helpful, encouraging tone.""")
        ]
        
        # Werkzeuge definieren
        self.setup_tools()
    
    def setup_tools(self):
        class web_search(TypedDict):
            """Search for programming documentation or examples."""
            query: Annotated[str, "Search query for programming help"]
        
        class code_formatter(TypedDict):
            """Format and validate code snippets."""
            code: Annotated[str, "Code to format"]
            language: Annotated[str, "Programming language"]
        
        self.tools = [web_search, code_formatter]
        self.llm_with_tools = self.llm.bind_tools(self.tools)
    
    def chat(self, user_input: str):
        # Benutzeranfrage zum Gespr√§ch hinzuf√ºgen
        self.conversation_history.append(HumanMessage(content=user_input))
        
        # KI-Antwort erhalten
        response = self.llm_with_tools.invoke(self.conversation_history)
        
        # Werkzeugaufrufe bearbeiten, falls vorhanden
        if response.tool_calls:
            for tool_call in response.tool_calls:
                tool_result = self.execute_tool(tool_call)
                print(f"üîß Tool used: {tool_call['name']}")
                print(f"üìä Result: {tool_result}")
        
        # KI-Antwort zum Gespr√§ch hinzuf√ºgen
        self.conversation_history.append(response)
        
        return response.content
    
    def execute_tool(self, tool_call):
        tool_name = tool_call['name']
        args = tool_call['args']
        
        if tool_name == 'web_search':
            return f"Found documentation for: {args['query']}"
        elif tool_name == 'code_formatter':
            return f"Formatted {args['language']} code: {args['code'][:50]}..."
        
        return "Tool execution completed"

# Anwendungsbeispiel
assistant = CodingAssistant()

print("ü§ñ Coding Assistant Ready! Type 'quit' to exit.\n")

while True:
    user_input = input("You: ")
    if user_input.lower() == 'quit':
        break
    
    response = assistant.chat(user_input)
    print(f"ü§ñ Assistant: {response}\n")
```

**Anwendungsarchitektur:**

```mermaid
graph TD
    A[Benutzereingabe] --> B[Programmierungsassistent]
    B --> C[Gespr√§chsspeicher]
    B --> D[Werkzeugerkennung]
    B --> E[LLM-Verarbeitung]
    
    D --> F[Web-Suchwerkzeug]
    D --> G[Code-Formatierer-Werkzeug]
    
    E --> H[Antwortgenerierung]
    F --> H
    G --> H
    
    H --> I[Benutzeroberfl√§che]
    H --> C
```
**Wesentliche Funktionen, die wir implementiert haben:**
- **Merkt sich** dein gesamtes Gespr√§ch f√ºr Kontextverst√§ndnis
- **F√ºhrt Aktionen aus** durch Werkzeugaufrufe, nicht nur √ºber Gespr√§che
- **Folgt** vorhersehbaren Interaktionsmustern
- **Handhabt** Fehler und komplexe Arbeitsabl√§ufe automatisch

### üéØ P√§dagogische Zwischenpr√ºfung: Produktions-KI-Architektur

**Architekturverst√§ndnis**: Du hast eine vollst√§ndige KI-Anwendung gebaut, die Gespr√§chsverwaltung, Werkzeugaufrufe und strukturierte Arbeitsabl√§ufe kombiniert. Das repr√§sentiert die Entwicklung von KI-Anwendungen auf Produktionsniveau.

**Beherrschte Kernkonzepte**:
- **Klassenbasierte Architektur**: Organisierte, wartbare KI-Anwendungsstruktur
- **Werkzeugintegration**: Eigene Funktionalit√§t √ºber Gespr√§che hinaus
- **Speichermanagement**: Persistenter Gespr√§chskontext
- **Fehlerbehandlung**: Robustes Anwendungsverhalten

**Branchenbezug**: Die Architekturmuster, die du implementiert hast (Gespr√§chsklassen, Werkzeugsysteme, Speichermanagement), sind dieselben Muster, die in Unternehmens-KI-Anwendungen wie dem AI-Assistenten von Slack, GitHub Copilot und Microsoft Copilot verwendet werden. Du baust mit professionellem Architekturdenken.

**Reflexionsfrage**: Wie w√ºrdest du diese Anwendung erweitern, um mehrere Benutzer, persistente Speicherung oder Integration mit externen Datenbanken zu unterst√ºtzen? Denk an Skalierbarkeit und Herausforderungen im Statusmanagement.

## Aufgabe: Baue deinen eigenen KI-gest√ºtzten Lernassistenten

**Ziel**: Erstelle eine KI-Anwendung, die Studierenden beim Lernen von Programmiersprachen hilft, indem sie Erkl√§rungen, Codebeispiele und interaktive Quiz bereitstellt.

### Anforderungen

**Kernfunktionen (Pflicht):**
1. **Konversationelle Schnittstelle**: Implementiere ein Chatsystem, das den Kontext mehrerer Fragen beibeh√§lt
2. **Bildungswerkzeuge**: Erstelle mindestens zwei Werkzeuge zur Unterst√ºtzung des Lernens:
   - Code-Erkl√§rer
   - Quizgenerator f√ºr Konzepte
3. **Personalisierte Lernunterst√ºtzung**: Nutze Systemnachrichten, um Antworten an unterschiedliche Kenntnisst√§nde anzupassen
4. **Antwortformatierung**: Implementiere strukturiertes Format f√ºr Quizfragen

### Implementierungsschritte

**Schritt 1: Umgebung einrichten**
```bash
pip install langchain langchain-openai
```

**Schritt 2: Grundlegende Chatfunktionalit√§t**
- Erstelle eine `StudyAssistant`-Klasse
- Implementiere Gespr√§chsspeicher
- F√ºge Pers√∂nlichkeitseinstellungen f√ºr Lernunterst√ºtzung hinzu

**Schritt 3: F√ºge Bildungswerkzeuge hinzu**
- **Code-Erkl√§rer**: Erkl√§rt Codeschritte verst√§ndlich
- **Quizgenerator**: Erstellt Fragen zu Programmierkonzepten
- **Fortschrittsverfolgung**: Verfolgt behandelte Themen

**Schritt 4: Erweiterte Funktionen (Optional)**
- Implementiere Streaming-Antworten f√ºr bessere Nutzererfahrung
- F√ºge Dokumentenladen hinzu, um Kursmaterial einzubinden
- Erstelle Embeddings f√ºr inhaltsbasierte Suche

### Bewertungskriterien

| Funktion | Ausgezeichnet (4) | Gut (3) | Zufriedenstellend (2) | Verbesserungsw√ºrdig (1) |
|----------|-------------------|---------|-----------------------|-------------------------|
| **Gespr√§chsverlauf** | Nat√ºrliche, kontextbewusste Antworten | Gute Kontextbeibehaltung | Grundlegende Konversation | Kein Ged√§chtnis zwischen den Austauschen |
| **Werkzeugintegration** | Mehrere hilfreiche Werkzeuge nahtlos integriert | 2+ Werkzeuge korrekt implementiert | 1-2 einfache Werkzeuge | Werkzeuge nicht funktionsf√§hig |
| **Codequalit√§t** | Sauber, gut dokumentiert, Fehlerbehandlung | Gute Struktur, teils Dokumentation | Grundfunktionalit√§t | Schlechte Struktur, keine Fehlerbehandlung |
| **Bildungswert** | Wirklich hilfsreich f√ºrs Lernen, adaptiv | Gute Lernunterst√ºtzung | Grundlegende Erkl√§rungen | Eingeschr√§nkter p√§dagogischer Nutzen |

### Beispielhafte Code-Struktur

```python
class StudyAssistant:
    def __init__(self, skill_level="beginner"):
        # Initialisiere LLM, Werkzeuge und Gespr√§chsspeicher
        pass
    
    def explain_code(self, code, language):
        # Werkzeug: Erkl√§re, wie der Code funktioniert
        pass
    
    def generate_quiz(self, topic, difficulty):
        # Werkzeug: Erstelle √úbungsfragen
        pass
    
    def chat(self, user_input):
        # Haupt-Gespr√§chsschnittstelle
        pass

# Beispielverwendung
assistant = StudyAssistant(skill_level="intermediate")
response = assistant.chat("Explain how Python functions work")
```

**Bonus-Herausforderungen:**
- Sprach-Ein- und Ausgabef√§higkeiten hinzuf√ºgen
- Webinterface mit Streamlit oder Flask implementieren
- Wissensdatenbank aus Kursmaterial mit Embeddings erstellen
- Fortschrittsverfolgung und personalisierte Lernpfade hinzuf√ºgen

## üìà Deine Timeline zum Meister der KI-Framework-Entwicklung

```mermaid
timeline
    title Entwicklungsreise des Produktions-KI-Frameworks
    
    section Grundlagen des Frameworks
        Verst√§ndnis von Abstraktionen
            : Entscheidungen zwischen Master-Framework und API
            : LangChain Kernkonzepte erlernen
            : Nachrichtentyp-Systeme implementieren
        
        Grundlagen der Integration
            : Verbindung zu KI-Anbietern herstellen
            : Authentifizierung verwalten
            : Konfiguration managen
    
    section Gespr√§chssysteme
        Speicherverwaltung
            : Gespr√§chshistorie aufbauen
            : Kontextverfolgung implementieren
            : Sitzungs-Persistenz handhaben
        
        Fortgeschrittene Interaktionen
            : Streaming-Antworten meistern
            : Prompt-Vorlagen erstellen
            : Strukturierte Ausgaben implementieren
    
    section Werkzeugintegration
        Entwicklung benutzerdefinierter Werkzeuge
            : Werkzeug-Schemata entwerfen
            : Funktionsaufrufe implementieren
            : Externe APIs handhaben
        
        Workflow-Automatisierung
            : Mehrere Werkzeuge verketten
            : Entscheidungsb√§ume erstellen
            : Agenten-Verhalten aufbauen
    
    section Produktionsanwendungen
        Komplette Systemarchitektur
            : Alle Framework-Funktionen kombinieren
            : Fehlergrenzen implementieren
            : Wartbaren Code erstellen
        
        Unternehmensbereitschaft
            : Skalierbarkeitsfragen handhaben
            : Monitoring implementieren
            : Einsatzstrategien entwickeln
```
**üéì Abschlussmeilenstein**: Du hast erfolgreich die Entwicklung von KI-Frameworks gemeistert und nutzt dabei dieselben Werkzeuge und Muster, die moderne KI-Anwendungen antreiben. Diese F√§higkeiten repr√§sentieren den neuesten Stand der KI-Anwendungsentwicklung und bereiten dich auf den Bau von Unternehmensl√∂sungen vor.

**üîÑ N√§chste Stufe F√§higkeiten**:
- Bereit, fortgeschrittene KI-Architekturen (Agenten, Multiagentensysteme) zu erkunden
- Bereit, RAG-Systeme mit Vektor-Datenbanken zu bauen
- Ausgestattet, multimodale KI-Anwendungen zu erstellen
- Fundament gelegt f√ºr Skalierung und Optimierung von KI-Anwendungen

## Zusammenfassung

üéâ Du hast nun die Grundlagen der KI-Framework-Entwicklung gemeistert und gelernt, wie man mit LangChain anspruchsvolle KI-Anwendungen baut. Wie nach einer umfassenden Ausbildung hast du einen gro√üen Werkzeugkasten an F√§higkeiten erlangt. Lass uns zusammenfassen, was du erreicht hast.

### Was du gelernt hast

**Kernkonzepte des Frameworks:**
- **Framework-Vorteile**: Wann Frameworks API-Aufrufen vorzuziehen sind
- **LangChain Grundlagen**: Einrichtung und Konfiguration von KI-Modellanbindungen
- **Nachrichtentypen**: Verwendung von `SystemMessage`, `HumanMessage` und `AIMessage` f√ºr strukturierte Konversationen

**Erweiterte Funktionen:**
- **Werkzeugaufrufe**: Erstellung und Integration eigener Werkzeuge f√ºr erweiterte KI-F√§higkeiten
- **Gespr√§chsspeicher**: Kontextbewahrung √ºber mehrere Gespr√§chsrunden hinweg
- **Streaming-Antworten**: Echtzeit-Ausgabe von Antworten
- **Prompt-Vorlagen**: Wiederverwendbare, dynamische Prompts bauen
- **Strukturierte Ausgabe**: Konsistente, parsbare KI-Antworten sicherstellen
- **Embeddings**: Semantische Suche und Dokumentenverarbeitung schaffen

**Praktische Anwendungen:**
- **Aufbau kompletter Apps**: Kombination mehrerer Funktionen zu produktionsreifen Anwendungen
- **Fehlerbehandlung**: Robustes Fehlermanagement und Validierung implementieren
- **Werkzeugintegration**: Eigene Werkzeuge erstellen, die KI erweitern

### Zentrale Erkenntnisse

> üéØ **Merke**: KI-Frameworks wie LangChain sind im Grunde deine Komplexit√§tsverstecker und feature-reichen besten Freunde. Sie sind perfekt, wenn du Gespr√§chsspeicher, Werkzeugaufrufe oder die Arbeit mit mehreren KI-Modellen brauchst, ohne den √úberblick zu verlieren.

**Entscheidungsrahmen f√ºr KI-Integration:**

```mermaid
flowchart TD
    A[KI-Integrationsbedarf] --> B{Einfache Einzelabfrage?}
    B -->|Ja| C[Direkte API-Aufrufe]
    B -->|Nein| D{Konversationsspeicher ben√∂tigt?}
    D -->|Nein| E[SDK-Integration]
    D -->|Ja| F{Werkzeuge oder komplexe Funktionen ben√∂tigt?}
    F -->|Nein| G[Framework mit Basissetup]
    F -->|Ja| H[Vollst√§ndige Framework-Implementierung]
    
    C --> I[HTTP-Anfragen, minimale Abh√§ngigkeiten]
    E --> J[Anbieter-SDK, modellbezogen]
    G --> K[LangChain Basis-Chat]
    H --> L[LangChain mit Werkzeugen, Speicher, Agenten]
```
### Wie geht es weiter?

**Starte jetzt mit dem Bauen:**
- Nutze diese Konzepte, um etwas zu erstellen, das DICH begeistert!
- Experimentiere mit verschiedenen KI-Modellen √ºber LangChain ‚Äì wie auf einem Spielplatz f√ºr KI-Modelle
- Erstelle Werkzeuge, die echte Probleme in deiner Arbeit oder deinen Projekten l√∂sen

**Bereit f√ºr die n√§chste Stufe?**
- **KI-Agenten**: Baue KI-Systeme, die komplexe Aufgaben selbstst√§ndig planen und ausf√ºhren k√∂nnen
- **RAG (Retrieval-Augmented Generation)**: Kombiniere KI mit eigenen Wissensdatenbanken f√ºr leistungsstarke Anwendungen
- **Multimodale KI**: Arbeite mit Text, Bildern und Audio zusammen ‚Äì die M√∂glichkeiten sind unbegrenzt!
- **Produktionseinf√ºhrung**: Lerne, wie du deine KI-Apps skalierst und im Realbetrieb √ºberwachst

**Tritt der Community bei:**
- Die LangChain-Community ist fantastisch, um auf dem neuesten Stand zu bleiben und Best Practices zu lernen
- GitHub Models gibt dir Zugang zu modernsten KI-F√§higkeiten ‚Äì ideal zum Experimentieren
- √úbe mit verschiedenen Anwendungsf√§llen ‚Äì jedes Projekt bringt dir etwas Neues bei

Du hast jetzt das Wissen, um intelligente, konversationelle Anwendungen zu bauen, die Menschen helfen, echte Probleme zu l√∂sen. Wie die Handwerker der Renaissance, die k√ºnstlerische Vision mit technischem K√∂nnen verbanden, kannst du jetzt KI-F√§higkeiten mit praktischer Anwendung vereinen. Die Frage ist: Was wirst du erschaffen? üöÄ

## GitHub Copilot Agent Challenge üöÄ

Nutze den Agent-Modus, um die folgende Herausforderung zu l√∂sen:

**Beschreibung:** Baue einen fortschrittlichen KI-gest√ºtzten Code-Review-Assistenten, der mehrere LangChain-Funktionen kombiniert, darunter Werkzeugaufrufe, strukturierte Ausgabe und Gespr√§chsspeicher, um umfassendes Feedback zu Codeeinreichungen zu geben.

**Aufgabe:** Erstelle eine `CodeReviewAssistant`-Klasse, die implementiert:
1. Ein Werkzeug zur Analyse der Codekomplexit√§t und Verbesserungsvorschl√§gen
2. Ein Werkzeug zur Pr√ºfung von Code anhand bester Praktiken
3. Strukturierte Ausgabe mit Pydantic-Modellen f√ºr ein konsistentes Review-Format
4. Gespr√§chsspeicher, um Review-Sitzungen zu verfolgen
5. Eine Hauptchat-Schnittstelle, die Codeeinreichungen verarbeiten und detailliertes, umsetzbares Feedback geben kann

Der Assistent soll Code in mehreren Programmiersprachen bewerten, den Kontext √ºber mehrere Codeeinreichungen in einer Sitzung behalten und sowohl Zusammenfassungsnoten als auch detaillierte Verbesserungsvorschl√§ge liefern.

Mehr Informationen zum [Agent-Modus](https://code.visualstudio.com/blogs/2025/02/24/introducing-copilot-agent-mode) findest du hier.

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**Haftungsausschluss**:  
Dieses Dokument wurde mithilfe des KI-√úbersetzungsdienstes [Co-op Translator](https://github.com/Azure/co-op-translator) √ºbersetzt. Obwohl wir bem√ºht sind, Genauigkeit zu gew√§hrleisten, weisen wir darauf hin, dass automatisierte √úbersetzungen Fehler oder Ungenauigkeiten enthalten k√∂nnen. Das Originaldokument in seiner Ausgangssprache ist als ma√ügebliche Quelle zu betrachten. F√ºr kritische Informationen wird eine professionelle menschliche √úbersetzung empfohlen. Wir √ºbernehmen keine Haftung f√ºr Missverst√§ndnisse oder Fehlinterpretationen, die durch die Nutzung dieser √úbersetzung entstehen.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->