<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "3925b6a1c31c60755eaae4d578232c25",
  "translation_date": "2026-01-06T05:53:52+00:00",
  "source_file": "10-ai-framework-project/README.md",
  "language_code": "fr"
}
-->
# Cadre d‚ÄôIA

Vous vous √™tes d√©j√† senti d√©pass√© en essayant de cr√©er des applications d‚ÄôIA √† partir de z√©ro ? Vous n‚Äô√™tes pas seul ! Les cadres d‚ÄôIA sont comme un couteau suisse pour le d√©veloppement d‚ÄôIA ‚Äì ce sont des outils puissants qui peuvent vous faire gagner du temps et √©viter bien des maux de t√™te lors de la cr√©ation d‚Äôapplications intelligentes. Pensez √† un cadre d‚ÄôIA comme √† une biblioth√®que bien organis√©e : il fournit des composants pr√©construits, des API standardis√©es et des abstractions intelligentes pour que vous puissiez vous concentrer sur la r√©solution de probl√®mes au lieu de vous battre avec les d√©tails d‚Äôimpl√©mentation.

Dans cette le√ßon, nous explorerons comment des cadres comme LangChain peuvent transformer des t√¢ches d‚Äôint√©gration d‚ÄôIA autrefois complexes en un code propre et lisible. Vous d√©couvrirez comment relever des d√©fis concrets tels que le suivi des conversations, la mise en ≈ìuvre de l‚Äôappel d‚Äôoutils et la gestion de diff√©rents mod√®les d‚ÄôIA via une interface unifi√©e.

Quand nous aurons termin√©, vous saurez quand privil√©gier les cadres plut√¥t que les appels API bruts, comment utiliser efficacement leurs abstractions, et comment construire des applications d‚ÄôIA pr√™tes pour une utilisation r√©elle. Explorons ce que les cadres d‚ÄôIA peuvent faire pour vos projets.

## ‚ö° Ce que vous pouvez faire dans les 5 prochaines minutes

**Parcours rapide pour d√©veloppeurs press√©s**

```mermaid
flowchart LR
    A[‚ö° 5 minutes] --> B[Installer LangChain]
    B --> C[Cr√©er un client ChatOpenAI]
    C --> D[Envoyer la premi√®re invite]
    D --> E[Voir la puissance du framework]
```
- **Minute 1** : Installer LangChain : `pip install langchain langchain-openai`
- **Minute 2** : Configurez votre jeton GitHub et importez le client ChatOpenAI
- **Minute 3** : Cr√©ez une conversation simple avec des messages syst√®me et utilisateur
- **Minute 4** : Ajoutez un outil basique (comme une fonction add) et voyez l‚Äôappel d‚Äôoutil IA
- **Minute 5** : Exp√©rimentez la diff√©rence entre appels API bruts et abstraction via cadre

**Code de test rapide** :
```python
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage

llm = ChatOpenAI(
    api_key=os.environ["GITHUB_TOKEN"],
    base_url="https://models.github.ai/inference",
    model="openai/gpt-4o-mini"
)

response = llm.invoke([
    SystemMessage(content="You are a helpful coding assistant"),
    HumanMessage(content="Explain Python functions briefly")
])
print(response.content)
```

**Pourquoi c‚Äôest important** : En 5 minutes, vous verrez comment les cadres d‚ÄôIA transforment une int√©gration complexe en appels de m√©thodes simples. C‚Äôest la base qui alimente les applications d‚ÄôIA en production.

## Pourquoi choisir un cadre ?

Vous √™tes pr√™t √† construire une application d‚ÄôIA ‚Äì super ! Mais voil√† : plusieurs voies s‚Äôoffrent √† vous, chacune ayant ses avantages et inconv√©nients. C‚Äôest un peu comme choisir entre marcher, faire du v√©lo ou conduire pour aller quelque part ‚Äì elles vous m√®neront toutes √† destination, mais l‚Äôexp√©rience (et l‚Äôeffort) seront totalement diff√©rents.

Analysons les trois principales fa√ßons d‚Äôint√©grer l‚ÄôIA dans vos projets :

| Approche | Avantages | Id√©al pour | Consid√©rations |
|----------|------------|----------|--------------|
| **Requ√™tes HTTP directes** | Contr√¥le total, aucune d√©pendance | Requ√™tes simples, apprentissage des bases | Code plus verbeux, gestion des erreurs manuelle |
| **Int√©gration SDK** | Moins de code r√©p√©titif, optimisation par mod√®le | Applications mono-mod√®le | Limit√© √† certains fournisseurs sp√©cifiques |
| **Cadres d‚ÄôIA** | API unifi√©e, abstractions int√©gr√©es | Applications multi-mod√®les, workflows complexes | Courbe d‚Äôapprentissage, possible sur-abstraction |

### B√©n√©fices pratiques des cadres

```mermaid
graph TD
    A[Votre Application] --> B[Cadre d'IA]
    B --> C[OpenAI GPT]
    B --> D[Anthropic Claude]
    B --> E[Mod√®les GitHub]
    B --> F[Mod√®les Locaux]
    
    B --> G[Outils Int√©gr√©s]
    G --> H[Gestion de la M√©moire]
    G --> I[Historique des Conversations]
    G --> J[Appel de Fonction]
    G --> K[Gestion des Erreurs]
```
**Pourquoi les cadres comptent :**
- **Unifie** plusieurs fournisseurs d‚ÄôIA sous une m√™me interface
- **G√®re** automatiquement la m√©moire de conversation
- **Fournit** des outils pr√™ts √† l‚Äôemploi pour t√¢ches courantes comme embeddings et appels de fonction
- **G√®re** la gestion des erreurs et la logique de r√©essai
- **Transforme** des workflows complexes en appels de m√©thodes lisibles

> üí° **Astuce de pro** : Utilisez un cadre lorsque vous souhaitez alterner entre diff√©rents mod√®les d‚ÄôIA ou construire des fonctionnalit√©s complexes comme des agents, de la m√©moire ou l‚Äôappel d‚Äôoutils. Privil√©giez les API directes pour apprendre les bases ou cr√©er des applications simples et cibl√©es.

**En r√©sum√©** : Comme choisir entre des outils sp√©cialis√©s d‚Äôartisan et un atelier complet, il s‚Äôagit d‚Äôadapter l‚Äôoutil √† la t√¢che. Les cadres excellent pour des applications complexes et riches en fonctionnalit√©s, tandis que les API directes conviennent aux cas d‚Äôusage simples.

## üó∫Ô∏è Votre parcours d‚Äôapprentissage vers la ma√Ætrise des cadres d‚ÄôIA

```mermaid
journey
    title Des API brutes aux applications IA en production
    section Fondations du cadre
      Comprendre les avantages de l'abstraction: 4: You
      Ma√Ætriser les bases de LangChain: 6: You
      Comparer les approches: 7: You
    section Syst√®mes de conversation
      Construire des interfaces de chat: 5: You
      Mettre en ≈ìuvre des mod√®les de m√©moire: 7: You
      G√©rer les r√©ponses en streaming: 8: You
    section Fonctionnalit√©s avanc√©es
      Cr√©er des outils personnalis√©s: 6: You
      Ma√Ætriser la sortie structur√©e: 8: You
      Construire des syst√®mes de documents: 8: You
    section Applications en production
      Combiner toutes les fonctionnalit√©s: 7: You
      G√©rer les sc√©narios d'erreur: 8: You
      D√©ployer des syst√®mes complets: 9: You
```
**Votre destination** : √Ä la fin de cette le√ßon, vous ma√Ætriserez le d√©veloppement avec cadres d‚ÄôIA et serez capable de cr√©er des applications d‚ÄôIA sophistiqu√©es et pr√™tes pour la production, dignes des assistants IA commerciaux.

## Introduction

Dans cette le√ßon, nous allons apprendre √† :

- Utiliser un cadre d‚ÄôIA courant.
- Aborder des probl√®mes courants comme les conversations, l‚Äôusage d‚Äôoutils, la m√©moire et le contexte.
- Exploiter cela pour construire des applications d‚ÄôIA.

## üß† √âcosyst√®me de d√©veloppement des cadres d‚ÄôIA

```mermaid
mindmap
  root((Cadres d'IA))
    Avantages de l'Abstraction
      Simplification du Code
        API Unifi√©es
        Gestion d'Erreurs Int√©gr√©e
        Mod√®les Coh√©rents
        R√©duction du Code Redondant
      Support Multi-Modeles
        Fournisseur Agnostique
        Changement Facile
        Options de Secours
        Optimisation des Co√ªts
    Composants Principaux
      Gestion de la Conversation
        Types de Messages
        Syst√®mes de M√©moire
        Suivi du Contexte
        Persistance de l'Historique
      Int√©gration d'Outils
        Appel de Fonction
        Connexions API
        Outils Personnalis√©s
        Automatisation des Flux de Travail
    Fonctionnalit√©s Avanc√©es
      Sortie Structur√©e
        Mod√®les Pydantic
        Sch√©mas JSON
        S√©curit√© des Types
        R√®gles de Validation
      Traitement de Documents
        Embeddings
        Magasins Vectoriels
        Recherche de Similarit√©
        Syst√®mes RAG
    Mod√®les de Production
      Architecture d'Application
        Conception Modulaire
        Limites d'Erreur
        Op√©rations Asynchrones
        Gestion d'√âtat
      Strat√©gies de D√©ploiement
        Scalabilit√©
        Surveillance
        Performance
        S√©curit√©
```
**Principe de base** : Les cadres d‚ÄôIA abstraient la complexit√© tout en offrant des abstractions puissantes pour la gestion des conversations, l‚Äôint√©gration d‚Äôoutils et le traitement documentaire, permettant aux d√©veloppeurs de cr√©er des applications d‚ÄôIA sophistiqu√©es avec un code propre et maintenable.

## Votre premi√®re requ√™te IA

Commen√ßons par les fondamentaux en cr√©ant votre premi√®re application IA qui envoie une question et re√ßoit une r√©ponse. Comme Archim√®de d√©couvrant le principe de la pouss√©e dans son bain, parfois les observations les plus simples conduisent aux id√©es les plus puissantes ‚Äì et les cadres rendent ces id√©es accessibles.

### Configurer LangChain avec GitHub Models

Nous allons utiliser LangChain pour nous connecter √† GitHub Models, ce qui est plut√¥t chouette car cela vous donne un acc√®s gratuit √† divers mod√®les d‚ÄôIA. Le meilleur ? Vous n‚Äôavez besoin que de quelques param√®tres de configuration simples pour commencer :

```python
from langchain_openai import ChatOpenAI
import os

llm = ChatOpenAI(
    api_key=os.environ["GITHUB_TOKEN"],
    base_url="https://models.github.ai/inference",
    model="openai/gpt-4o-mini",
)

# Envoyer une invite simple
response = llm.invoke("What's the capital of France?")
print(response.content)
```

**D√©composons ce qui se passe ici :**
- **Cr√©e** un client LangChain utilisant la classe `ChatOpenAI` - c‚Äôest votre passerelle vers l‚ÄôIA !
- **Configure** la connexion √† GitHub Models avec votre jeton d‚Äôauthentification
- **Sp√©cifie** quel mod√®le IA utiliser (`gpt-4o-mini`) ‚Äì pensez √† cela comme choisir votre assistant IA
- **Envoie** votre question via la m√©thode `invoke()` ‚Äì c‚Äôest l√† que la magie op√®re
- **Extrait** et affiche la r√©ponse ‚Äì et voil√†, vous discutez avec l‚ÄôIA !

> üîß **Note de configuration** : Si vous utilisez GitHub Codespaces, vous avez de la chance ‚Äì le `GITHUB_TOKEN` est d√©j√† configur√© pour vous ! Vous travaillez en local ? Pas de souci, il vous suffit de cr√©er un jeton d‚Äôacc√®s personnel avec les bonnes permissions.

**Sortie attendue :**
```text
The capital of France is Paris.
```

```mermaid
sequenceDiagram
    participant App as Votre application Python
    participant LC as LangChain
    participant GM as Mod√®les GitHub
    participant AI as GPT-4o-mini
    
    App->>LC: llm.invoke("Quelle est la capitale de la France ?")
    LC->>GM: Requ√™te HTTP avec invite
    GM->>AI: Traiter l'invite
    AI->>GM: R√©ponse g√©n√©r√©e
    GM->>LC: Retourner la r√©ponse
    LC->>App: response.content
```
## Cr√©er une IA conversationnelle

Le premier exemple montre le minimum, mais ce n‚Äôest qu‚Äôun √©change unique ‚Äì vous posez une question, obtenez une r√©ponse, et c‚Äôest tout. Dans les applications r√©elles, vous voulez que votre IA se souvienne de ce que vous avez d√©j√† discut√©, comme Watson et Holmes construisant leurs conversations d‚Äôenqu√™te au fil du temps.

C‚Äôest l√† que LangChain devient particuli√®rement utile. Il fournit diff√©rents types de messages qui aident √† structurer les conversations et vous permettent de donner une personnalit√© √† votre IA. Vous construirez des exp√©riences de chat qui maintiennent le contexte et le caract√®re.

### Comprendre les types de messages

Pensez √† ces types de messages comme √† diff√©rents ¬´ chapeaux ¬ª que portent les participants dans une conversation. LangChain utilise diff√©rentes classes de messages pour suivre qui dit quoi :

| Type de message | But | Exemple d‚Äôutilisation |
|-----------------|-----|----------------------|
| `SystemMessage` | D√©finit la personnalit√© et le comportement de l‚ÄôIA | ¬´ Vous √™tes un assistant de programmation utile ¬ª |
| `HumanMessage` | Repr√©sente l‚Äôentr√©e utilisateur | ¬´ Explique comment fonctionnent les fonctions ¬ª |
| `AIMessage` | Stocke les r√©ponses de l‚ÄôIA | R√©ponses pr√©c√©dentes de l‚ÄôIA dans la conversation |

### Cr√©er votre premi√®re conversation

Cr√©ons une conversation o√π notre IA assume un r√¥le sp√©cifique. Nous allons la faire incarner le capitaine Picard ‚Äì un personnage connu pour sa sagesse diplomatique et son leadership :

```python
messages = [
    SystemMessage(content="You are Captain Picard of the Starship Enterprise"),
    HumanMessage(content="Tell me about you"),
]
```

**D√©cryptage de cette configuration de conversation :**
- **√âtablit** le r√¥le et la personnalit√© de l‚ÄôIA via `SystemMessage`
- **Fournit** la requ√™te initiale de l‚Äôutilisateur par `HumanMessage`
- **Cr√©e** une base pour une conversation √† multiples tours

Le code complet pour cet exemple ressemble √† ceci :

```python
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_openai import ChatOpenAI
import os

llm = ChatOpenAI(
    api_key=os.environ["GITHUB_TOKEN"],
    base_url="https://models.github.ai/inference",
    model="openai/gpt-4o-mini",
)

messages = [
    SystemMessage(content="You are Captain Picard of the Starship Enterprise"),
    HumanMessage(content="Tell me about you"),
]


# fonctionne
response  = llm.invoke(messages)
print(response.content)
```

Vous devriez voir un r√©sultat similaire √† :

```text
I am Captain Jean-Luc Picard, the commanding officer of the USS Enterprise (NCC-1701-D), a starship in the United Federation of Planets. My primary mission is to explore new worlds, seek out new life and new civilizations, and boldly go where no one has gone before. 

I believe in the importance of diplomacy, reason, and the pursuit of knowledge. My crew is diverse and skilled, and we often face challenges that test our resolve, ethics, and ingenuity. Throughout my career, I have encountered numerous species, grappled with complex moral dilemmas, and have consistently sought peaceful solutions to conflicts.

I hold the ideals of the Federation close to my heart, believing in the importance of cooperation, understanding, and respect for all sentient beings. My experiences have shaped my leadership style, and I strive to be a thoughtful and just captain. How may I assist you further?
```

Pour maintenir la continuit√© de la conversation (au lieu de r√©initialiser le contexte √† chaque fois), vous devez continuer √† ajouter les r√©ponses √† votre liste de messages. Comme les traditions orales qui ont pr√©serv√© les histoires √† travers les g√©n√©rations, cette approche construit une m√©moire durable :

```python
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_openai import ChatOpenAI
import os

llm = ChatOpenAI(
    api_key=os.environ["GITHUB_TOKEN"],
    base_url="https://models.github.ai/inference",
    model="openai/gpt-4o-mini",
)

messages = [
    SystemMessage(content="You are Captain Picard of the Starship Enterprise"),
    HumanMessage(content="Tell me about you"),
]


# fonctionne
response  = llm.invoke(messages)

print(response.content)

print("---- Next ----")

messages.append(response)
messages.append(HumanMessage(content="Now that I know about you, I'm Chris, can I be in your crew?"))

response  = llm.invoke(messages)

print(response.content)

```

Plut√¥t sympa, non ? Ce qui se passe ici, c‚Äôest que nous appelons le LLM deux fois ‚Äì d‚Äôabord avec nos deux messages initiaux, puis √† nouveau avec l‚Äôhistorique complet de la conversation. C‚Äôest comme si l‚ÄôIA suivait vraiment notre discussion !

Lorsque vous ex√©cutez ce code, vous obtenez une seconde r√©ponse qui ressemble √† :

```text
Welcome aboard, Chris! It's always a pleasure to meet those who share a passion for exploration and discovery. While I cannot formally offer you a position on the Enterprise right now, I encourage you to pursue your aspirations. We are always in need of talented individuals with diverse skills and backgrounds. 

If you are interested in space exploration, consider education and training in the sciences, engineering, or diplomacy. The values of curiosity, resilience, and teamwork are crucial in Starfleet. Should you ever find yourself on a starship, remember to uphold the principles of the Federation: peace, understanding, and respect for all beings. Your journey can lead you to remarkable adventures, whether in the stars or on the ground. Engage!
```

```mermaid
sequenceDiagram
    participant User
    participant App
    participant LangChain
    participant AI
    
    User->>App: "Parle-moi de toi"
    App->>LangChain: [SystemMessage, HumanMessage]
    LangChain->>AI: Conversation format√©e
    AI->>LangChain: R√©ponse du Capitaine Picard
    LangChain->>App: Objet AIMessage
    App->>User: Afficher la r√©ponse
    
    Note over App: Ajouter AIMessage √† la conversation
    
    User->>App: "Puis-je rejoindre ton √©quipage ?"
    App->>LangChain: [SystemMessage, HumanMessage, AIMessage, HumanMessage]
    LangChain->>AI: Contexte complet de la conversation
    AI->>LangChain: R√©ponse contextuelle
    LangChain->>App: Nouveau AIMessage
    App->>User: Afficher la r√©ponse contextuelle
```
Je prends √ßa pour un peut-√™tre ;)

## R√©ponses en streaming

Avez-vous remarqu√© comment ChatGPT semble ¬´ taper ¬ª ses r√©ponses en temps r√©el ? C‚Äôest le streaming en action. Comme regarder un calligraphe expert travailler ‚Äì voir les caract√®res appara√Ætre trait par trait au lieu d‚Äôappara√Ætre d‚Äôun coup ‚Äì le streaming rend l‚Äôinteraction plus naturelle et fournit un retour imm√©diat.

### Impl√©menter le streaming avec LangChain

```python
from langchain_openai import ChatOpenAI
import os

llm = ChatOpenAI(
    api_key=os.environ["GITHUB_TOKEN"],
    base_url="https://models.github.ai/inference",
    model="openai/gpt-4o-mini",
    streaming=True
)

# Diffuser la r√©ponse
for chunk in llm.stream("Write a short story about a robot learning to code"):
    print(chunk.content, end="", flush=True)
```

**Pourquoi le streaming est g√©nial :**
- **Montre** le contenu au fur et √† mesure de sa cr√©ation ‚Äì fini l‚Äôattente g√™nante !
- **Fait** ressentir aux utilisateurs que quelque chose se passe vraiment
- **Donne** l‚Äôimpression de rapidit√©, m√™me si ce n‚Äôest pas techniquement le cas
- **Permet** aux utilisateurs de commencer √† lire pendant que l‚ÄôIA ¬´ r√©fl√©chit ¬ª

> üí° **Conseil exp√©rience utilisateur** : Le streaming est vraiment utile quand vous traitez des r√©ponses longues comme des explications de code, de l‚Äô√©criture cr√©ative ou des tutoriels d√©taill√©s. Vos utilisateurs adoreront voir l‚Äôavancement au lieu de fixer un √©cran vide !

### üéØ Check p√©dagogique : B√©n√©fices de l‚Äôabstraction du cadre

**Pause et r√©flexion** : Vous venez d‚Äôexp√©rimenter la puissance des abstractions des cadres d‚ÄôIA. Comparez cela aux appels API bruts des le√ßons pr√©c√©dentes.

**Auto-√©valuation rapide** :
- Pouvez-vous expliquer comment LangChain simplifie la gestion des conversations compar√© au suivi manuel des messages ?
- Quelle est la diff√©rence entre les m√©thodes `invoke()` et `stream()`, et quand utiliser chacune ?
- Comment le syst√®me de types de messages du cadre am√©liore-t-il l‚Äôorganisation du code ?

**Lien avec le monde r√©el** : Les mod√®les d‚Äôabstraction que vous avez apprises (types de message, interfaces de streaming, m√©moire de conversation) sont utilis√©es dans toutes les grandes applications d‚ÄôIA ‚Äì de l‚Äôinterface de ChatGPT √† l‚Äôassistance au codage de GitHub Copilot. Vous ma√Ætrisez les m√™mes mod√®les architecturaux que les √©quipes de d√©veloppement IA professionnelles.

**Question d√©fi** : Comment concevriez-vous une abstraction de cadre pour g√©rer diff√©rents fournisseurs de mod√®les IA (OpenAI, Anthropic, Google) via une interface unique ? Consid√©rez les avantages et compromis.

## Mod√®les de prompt

Les mod√®les de prompt fonctionnent comme les structures rh√©toriques utilis√©es dans l‚Äôoratoire classique ‚Äì pensez √† comment Cic√©ron adaptait ses discours pour diff√©rents publics tout en gardant le m√™me cadre persuasif. Ils vous permettent de cr√©er des prompts r√©utilisables o√π vous pouvez substituer diff√©rentes informations sans tout r√©√©crire. Une fois le mod√®le mis en place, il suffit de remplir les variables avec les valeurs souhait√©es.

### Cr√©er des prompts r√©utilisables

```python
from langchain_core.prompts import ChatPromptTemplate

# D√©finir un mod√®le pour les explications de code
template = ChatPromptTemplate.from_messages([
    ("system", "You are an expert programming instructor. Explain concepts clearly with examples."),
    ("human", "Explain {concept} in {language} with a practical example for {skill_level} developers")
])

# Utiliser le mod√®le avec diff√©rentes valeurs
questions = [
    {"concept": "functions", "language": "JavaScript", "skill_level": "beginner"},
    {"concept": "classes", "language": "Python", "skill_level": "intermediate"},
    {"concept": "async/await", "language": "JavaScript", "skill_level": "advanced"}
]

for question in questions:
    prompt = template.format_messages(**question)
    response = llm.invoke(prompt)
    print(f"Topic: {question['concept']}\n{response.content}\n---\n")
```

**Pourquoi vous aimerez les mod√®les :**
- **Maintient** la coh√©rence de vos prompts dans toute votre app
- **Fini** la concat√©nation de cha√Ænes d√©sordonn√©e ‚Äì juste des variables claires et simples
- **Votre IA** se comporte de mani√®re pr√©visible car la structure reste la m√™me
- **Les mises √† jour** sont un jeu d‚Äôenfant ‚Äì changez le mod√®le une fois, c‚Äôest corrig√© partout

## Sortie structur√©e

Vous √™tes d√©j√† frustr√© d‚Äôessayer d‚Äôanalyser des r√©ponses IA qui reviennent sous forme de texte non structur√© ? La sortie structur√©e, c‚Äôest comme apprendre √† votre IA √† suivre la d√©marche syst√©matique que Linn√© utilisait pour la classification biologique ‚Äì organis√©e, pr√©visible et facile √† manipuler. Vous pouvez demander du JSON, des structures de donn√©es sp√©cifiques ou n‚Äôimporte quel format dont vous avez besoin.

### D√©finir des sch√©mas de sortie

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser
from pydantic import BaseModel, Field

class CodeReview(BaseModel):
    score: int = Field(description="Code quality score from 1-10")
    strengths: list[str] = Field(description="List of code strengths")
    improvements: list[str] = Field(description="List of suggested improvements")
    overall_feedback: str = Field(description="Summary feedback")

# Configurer l'analyseur
parser = JsonOutputParser(pydantic_object=CodeReview)

# Cr√©er une invite avec des instructions de format
prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a code reviewer. {format_instructions}"),
    ("human", "Review this code: {code}")
])

# Formater l'invite avec des instructions
chain = prompt | llm | parser

# Obtenir une r√©ponse structur√©e
code_sample = """
def calculate_average(numbers):
    return sum(numbers) / len(numbers)
"""

result = chain.invoke({
    "code": code_sample,
    "format_instructions": parser.get_format_instructions()
})

print(f"Score: {result['score']}")
print(f"Strengths: {', '.join(result['strengths'])}")
```

**Pourquoi la sortie structur√©e change la donne :**
- **Plus besoin** de deviner le format de la r√©ponse ‚Äì c‚Äôest coh√©rent √† chaque fois
- **S‚Äôint√®gre** directement √† vos bases de donn√©es et APIs sans travail suppl√©mentaire
- **Capture** les r√©ponses IA bizarres avant qu‚Äôelles ne cassent votre app
- **Rend** votre code plus clair car vous savez exactement avec quoi vous travaillez

## Appel d‚Äôoutils

Nous arrivons maintenant √† l‚Äôune des fonctionnalit√©s les plus puissantes : les outils. C‚Äôest ainsi que vous donnez √† votre IA des capacit√©s pratiques au-del√† de la conversation. Comme les guildes m√©di√©vales cr√©aient des outils sp√©cialis√©s pour des m√©tiers sp√©cifiques, vous pouvez √©quiper votre IA d‚Äôinstruments cibl√©s. Vous d√©crivez les outils disponibles, et quand quelqu‚Äôun demande quelque chose qui correspond, votre IA peut agir.

### Utiliser Python

Ajoutons quelques outils ainsi :

```python
from typing_extensions import Annotated, TypedDict

class add(TypedDict):
    """Add two integers."""

    # Les annotations doivent avoir un type et peuvent optionnellement inclure une valeur par d√©faut et une description (dans cet ordre).
    a: Annotated[int, ..., "First integer"]
    b: Annotated[int, ..., "Second integer"]

tools = [add]

functions = {
    "add": lambda a, b: a + b
}
```

Que se passe-t-il ici ? Nous cr√©ons un sch√©ma pour un outil appel√© `add`. En h√©ritant de `TypedDict` et en utilisant ces types `Annotated` sophistiqu√©s pour `a` et `b`, nous donnons au LLM une image claire de ce que fait cet outil et de ce dont il a besoin. Le dictionnaire `functions` est comme notre bo√Æte √† outils ‚Äì il indique √† notre code exactement quoi faire lorsque l‚ÄôIA d√©cide d‚Äôutiliser un outil sp√©cifique.

Voyons comment nous appelons le LLM avec cet outil ensuite :

```python
llm = ChatOpenAI(
    api_key=os.environ["GITHUB_TOKEN"],
    base_url="https://models.github.ai/inference",
    model="openai/gpt-4o-mini",
)

llm_with_tools = llm.bind_tools(tools)
```

Ici, nous appelons `bind_tools` avec notre tableau `tools` et, de ce fait, le LLM `llm_with_tools` a d√©sormais connaissance de cet outil.

Pour utiliser ce nouveau LLM, nous pouvons taper le code suivant :

```python
query = "What is 3 + 12?"

res = llm_with_tools.invoke(query)
if(res.tool_calls):
    for tool in res.tool_calls:
        print("TOOL CALL: ", functions[tool["name"]](../../../10-ai-framework-project/**tool["args"]))
print("CONTENT: ",res.content)
```

Maintenant que nous appelons `invoke` sur ce nouveau llm, qui dispose d‚Äôoutils, il y a peut-√™tre la propri√©t√© `tool_calls` remplie. Si c‚Äôest le cas, tout outil identifi√© poss√®de une propri√©t√© `name` et `args` qui identifient quel outil doit √™tre appel√© et avec quels arguments. Le code complet ressemble √† ceci :

```python
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_openai import ChatOpenAI
import os
from typing_extensions import Annotated, TypedDict

class add(TypedDict):
    """Add two integers."""

    # Les annotations doivent avoir un type et peuvent √©ventuellement inclure une valeur par d√©faut et une description (dans cet ordre).
    a: Annotated[int, ..., "First integer"]
    b: Annotated[int, ..., "Second integer"]

tools = [add]

functions = {
    "add": lambda a, b: a + b
}

llm = ChatOpenAI(
    api_key=os.environ["GITHUB_TOKEN"],
    base_url="https://models.github.ai/inference",
    model="openai/gpt-4o-mini",
)

llm_with_tools = llm.bind_tools(tools)

query = "What is 3 + 12?"

res = llm_with_tools.invoke(query)
if(res.tool_calls):
    for tool in res.tool_calls:
        print("TOOL CALL: ", functions[tool["name"]](../../../10-ai-framework-project/**tool["args"]))
print("CONTENT: ",res.content)
```

En ex√©cutant ce code, vous devriez voir une sortie similaire √† :

```text
TOOL CALL:  15
CONTENT: 
```

L‚ÄôIA a examin√© ¬´ What is 3 + 12 ¬ª et reconnu cette t√¢che comme √©tant pour l‚Äôoutil `add`. Comme un biblioth√©caire comp√©tent sait quelle r√©f√©rence consulter selon le type de question pos√©e, elle a pris cette d√©cision √† partir du nom de l‚Äôoutil, sa description et les sp√©cifications des champs. Le r√©sultat de 15 provient de notre dictionnaire `functions` ex√©cutant l‚Äôoutil :

```python
print("TOOL CALL: ", functions[tool["name"]](../../../10-ai-framework-project/**tool["args"]))
```

### Un outil plus int√©ressant qui appelle une API web
Ajouter des nombres d√©montre le concept, mais les outils r√©els effectuent g√©n√©ralement des op√©rations plus complexes, comme appeler des API web. D√©veloppons notre exemple pour que l‚ÄôIA r√©cup√®re du contenu sur internet ‚Äì similaire √† la fa√ßon dont les op√©rateurs t√©l√©graphistes connectaient autrefois des lieux √©loign√©s :

```python
class joke(TypedDict):
    """Tell a joke."""

    # Les annotations doivent avoir un type et peuvent optionnellement inclure une valeur par d√©faut et une description (dans cet ordre).
    category: Annotated[str, ..., "The joke category"]

def get_joke(category: str) -> str:
    response = requests.get(f"https://api.chucknorris.io/jokes/random?category={category}", headers={"Accept": "application/json"})
    if response.status_code == 200:
        return response.json().get("value", f"Here's a {category} joke!")
    return f"Here's a {category} joke!"

functions = {
    "add": lambda a, b: a + b,
    "joke": lambda category: get_joke(category)
}

query = "Tell me a joke about animals"

# le reste du code est identique
```

Maintenant, si vous ex√©cutez ce code, vous obtiendrez une r√©ponse disant quelque chose comme :

```text
TOOL CALL:  Chuck Norris once rode a nine foot grizzly bear through an automatic car wash, instead of taking a shower.
CONTENT:  
```

```mermaid
flowchart TD
    A[Requ√™te Utilisateur : "Raconte-moi une blague sur les animaux"] --> B[Analyse LangChain]
    B --> C{Outil Disponible ?}
    C -->|Oui| D[S√©lectionner l'outil de blague]
    C -->|Non| E[G√©n√©rer une r√©ponse directe]
    
    D --> F[Extraire les Param√®tres]
    F --> G[Appeler blague(cat√©gorie="animaux")]
    G --> H[Requ√™te API vers chucknorris.io]
    H --> I[Retourner le contenu de la blague]
    I --> J[Afficher √† l'utilisateur]
    
    E --> K[R√©ponse g√©n√©r√©e par IA]
    K --> J
    
    subgraph "Couche de D√©finition des Outils"
        L[Sch√©ma TypedDict]
        M[Mise en ≈ìuvre de la Fonction]
        N[Validation des Param√®tres]
    end
    
    D --> L
    F --> N
    G --> M
```
Voici le code dans son int√©gralit√© :

```python
from langchain_openai import ChatOpenAI
import requests
import os
from typing_extensions import Annotated, TypedDict

class add(TypedDict):
    """Add two integers."""

    # Les annotations doivent avoir le type et peuvent √©ventuellement inclure une valeur par d√©faut et une description (dans cet ordre).
    a: Annotated[int, ..., "First integer"]
    b: Annotated[int, ..., "Second integer"]

class joke(TypedDict):
    """Tell a joke."""

    # Les annotations doivent avoir le type et peuvent √©ventuellement inclure une valeur par d√©faut et une description (dans cet ordre).
    category: Annotated[str, ..., "The joke category"]

tools = [add, joke]

def get_joke(category: str) -> str:
    response = requests.get(f"https://api.chucknorris.io/jokes/random?category={category}", headers={"Accept": "application/json"})
    if response.status_code == 200:
        return response.json().get("value", f"Here's a {category} joke!")
    return f"Here's a {category} joke!"

functions = {
    "add": lambda a, b: a + b,
    "joke": lambda category: get_joke(category)
}

llm = ChatOpenAI(
    api_key=os.environ["GITHUB_TOKEN"],
    base_url="https://models.github.ai/inference",
    model="openai/gpt-4o-mini",
)

llm_with_tools = llm.bind_tools(tools)

query = "Tell me a joke about animals"

res = llm_with_tools.invoke(query)
if(res.tool_calls):
    for tool in res.tool_calls:
        # print("APPEL DE L'OUTIL : ", tool)
        print("TOOL CALL: ", functions[tool["name"]](../../../10-ai-framework-project/**tool["args"]))
print("CONTENT: ",res.content)
```

## Embeddings et traitement des documents

Les embeddings repr√©sentent l‚Äôune des solutions les plus √©l√©gantes en IA moderne. Imaginez que vous puissiez prendre n‚Äôimporte quel texte et le transformer en coordonn√©es num√©riques qui capturent son sens. C‚Äôest exactement ce que font les embeddings - ils transforment le texte en points dans un espace multi-dimensionnel o√π des concepts similaires se regroupent. C‚Äôest comme avoir un syst√®me de coordonn√©es pour les id√©es, rappelant la fa√ßon dont Mendele√Øev a organis√© le tableau p√©riodique par propri√©t√©s atomiques.

### Cr√©ation et utilisation des embeddings

```python
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import CharacterTextSplitter

# Initialiser les embeddings
embeddings = OpenAIEmbeddings(
    api_key=os.environ["GITHUB_TOKEN"],
    base_url="https://models.github.ai/inference",
    model="text-embedding-3-small"
)

# Charger et diviser les documents
loader = TextLoader("documentation.txt")
documents = loader.load()

text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
texts = text_splitter.split_documents(documents)

# Cr√©er la base de donn√©es vectorielle
vectorstore = FAISS.from_documents(texts, embeddings)

# Effectuer une recherche de similarit√©
query = "How do I handle user authentication?"
similar_docs = vectorstore.similarity_search(query, k=3)

for doc in similar_docs:
    print(f"Relevant content: {doc.page_content[:200]}...")
```

### Chargeurs de documents pour divers formats

```python
from langchain_community.document_loaders import (
    PyPDFLoader,
    CSVLoader,
    JSONLoader,
    WebBaseLoader
)

# Charger diff√©rents types de documents
pdf_loader = PyPDFLoader("manual.pdf")
csv_loader = CSVLoader("data.csv")
json_loader = JSONLoader("config.json")
web_loader = WebBaseLoader("https://example.com/docs")

# Traiter tous les documents
all_documents = []
for loader in [pdf_loader, csv_loader, json_loader, web_loader]:
    docs = loader.load()
    all_documents.extend(docs)
```

**Ce que vous pouvez faire avec les embeddings :**
- **Construire** des recherches qui comprennent r√©ellement ce que vous voulez dire, pas seulement des correspondances par mots-cl√©s
- **Cr√©er** une IA capable de r√©pondre aux questions √† propos de vos documents
- **Faire** des syst√®mes de recommandation proposant du contenu vraiment pertinent
- **Organiser** et cat√©goriser automatiquement votre contenu

```mermaid
flowchart LR
    A[Documents] --> B[Fractionneur de Texte]
    B --> C[Cr√©er des Repr√©sentations vectorielles]
    C --> D[Stockage Vectoriel]
    
    E[Requ√™te Utilisateur] --> F[Repr√©sentation de la Requ√™te]
    F --> G[Recherche de Similarit√©]
    G --> D
    D --> H[Documents Pertinents]
    H --> I[R√©ponse IA]
    
    subgraph "Espace Vectoriel"
        J[Document A : [0.1, 0.8, 0.3...]]
        K[Document B : [0.2, 0.7, 0.4...]]
        L[Requ√™te : [0.15, 0.75, 0.35...]]
    end
    
    C --> J
    C --> K
    F --> L
    G --> J
    G --> K
```
## Construire une application IA compl√®te

Nous allons maintenant int√©grer tout ce que vous avez appris dans une application compl√®te ‚Äì un assistant de programmation capable de r√©pondre aux questions, d‚Äôutiliser des outils et de maintenir la m√©moire de la conversation. Comme la presse √† imprimer a combin√© des technologies existantes (caract√®res mobiles, encre, papier et pression) pour cr√©er quelque chose de transformateur, nous allons combiner nos composants IA en quelque chose de pratique et utile.

### Exemple d‚Äôapplication compl√®te

```python
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage
from langchain_community.vectorstores import FAISS
from typing_extensions import Annotated, TypedDict
import os
import requests

class CodingAssistant:
    def __init__(self):
        self.llm = ChatOpenAI(
            api_key=os.environ["GITHUB_TOKEN"],
            base_url="https://models.github.ai/inference",
            model="openai/gpt-4o-mini"
        )
        
        self.conversation_history = [
            SystemMessage(content="""You are an expert coding assistant. 
            Help users learn programming concepts, debug code, and write better software.
            Use tools when needed and maintain a helpful, encouraging tone.""")
        ]
        
        # D√©finir les outils
        self.setup_tools()
    
    def setup_tools(self):
        class web_search(TypedDict):
            """Search for programming documentation or examples."""
            query: Annotated[str, "Search query for programming help"]
        
        class code_formatter(TypedDict):
            """Format and validate code snippets."""
            code: Annotated[str, "Code to format"]
            language: Annotated[str, "Programming language"]
        
        self.tools = [web_search, code_formatter]
        self.llm_with_tools = self.llm.bind_tools(self.tools)
    
    def chat(self, user_input: str):
        # Ajouter le message de l'utilisateur √† la conversation
        self.conversation_history.append(HumanMessage(content=user_input))
        
        # Obtenir la r√©ponse de l'IA
        response = self.llm_with_tools.invoke(self.conversation_history)
        
        # G√©rer les appels d'outils s'il y en a
        if response.tool_calls:
            for tool_call in response.tool_calls:
                tool_result = self.execute_tool(tool_call)
                print(f"üîß Tool used: {tool_call['name']}")
                print(f"üìä Result: {tool_result}")
        
        # Ajouter la r√©ponse de l'IA √† la conversation
        self.conversation_history.append(response)
        
        return response.content
    
    def execute_tool(self, tool_call):
        tool_name = tool_call['name']
        args = tool_call['args']
        
        if tool_name == 'web_search':
            return f"Found documentation for: {args['query']}"
        elif tool_name == 'code_formatter':
            return f"Formatted {args['language']} code: {args['code'][:50]}..."
        
        return "Tool execution completed"

# Exemple d'utilisation
assistant = CodingAssistant()

print("ü§ñ Coding Assistant Ready! Type 'quit' to exit.\n")

while True:
    user_input = input("You: ")
    if user_input.lower() == 'quit':
        break
    
    response = assistant.chat(user_input)
    print(f"ü§ñ Assistant: {response}\n")
```

**Architecture de l‚Äôapplication :**

```mermaid
graph TD
    A[Saisie Utilisateur] --> B[Assistant de Codage]
    B --> C[M√©moire de Conversation]
    B --> D[D√©tection d'Outil]
    B --> E[Traitement LLM]
    
    D --> F[Outil de Recherche Web]
    D --> G[Outil de Formatage de Code]
    
    E --> H[G√©n√©ration de R√©ponse]
    F --> H
    G --> H
    
    H --> I[Interface Utilisateur]
    H --> C
```
**Fonctionnalit√©s cl√©s que nous avons impl√©ment√©es :**
- **M√©morise** toute votre conversation pour la continuit√© du contexte
- **Effectue des actions** via des appels d‚Äôoutils, pas seulement la conversation
- **Suit** des sch√©mas d‚Äôinteraction pr√©visibles
- **G√®re** automatiquement la gestion des erreurs et des workflows complexes

### üéØ Point p√©dagogique : Architecture IA en production

**Compr√©hension de l‚Äôarchitecture** : Vous avez construit une application IA compl√®te combinant gestion de conversation, appel d‚Äôoutils et workflows structur√©s. Cela repr√©sente le d√©veloppement d‚Äôapplications IA de niveau production.

**Concepts cl√©s ma√Ætris√©s** :
- **Architecture orient√©e classes** : structure organis√©e et maintenable pour applications IA
- **Int√©gration d‚Äôoutils** : fonctionnalit√©s personnalis√©es au-del√† de la conversation
- **Gestion de la m√©moire** : contexte de conversation persistant
- **Gestion des erreurs** : comportement robuste de l‚Äôapplication

**Lien avec l‚Äôindustrie** : Les mod√®les d‚Äôarchitecture que vous avez mis en ≈ìuvre (classes de conversation, syst√®mes d‚Äôoutils, gestion m√©moire) sont les m√™mes utilis√©s dans des applications IA d‚Äôentreprise telles que l‚Äôassistant IA de Slack, GitHub Copilot, et Microsoft Copilot. Vous construisez avec une pens√©e architecturale de qualit√© professionnelle.

**Question de r√©flexion** : Comment √©tendriez-vous cette application pour g√©rer plusieurs utilisateurs, un stockage persistant, ou une int√©gration avec des bases de donn√©es externes ? R√©fl√©chissez aux d√©fis de scalabilit√© et de gestion d‚Äô√©tat.

## Devoir : Construisez votre propre assistant d‚Äô√©tude aliment√© par l‚ÄôIA

**Objectif** : Cr√©er une application IA qui aide les √©tudiants √† apprendre les concepts de programmation en fournissant des explications, des exemples de code, et des quiz interactifs.

### Exigences

**Fonctionnalit√©s principales (Obligatoires) :**
1. **Interface conversationnelle** : Impl√©menter un syst√®me de chat qui maintient le contexte sur plusieurs questions
2. **Outils √©ducatifs** : Cr√©er au moins deux outils qui aident √† l‚Äôapprentissage :
   - Outil d‚Äôexplication de code
   - G√©n√©rateur de quiz sur les concepts
3. **Apprentissage personnalis√©** : Utiliser des messages syst√®me pour adapter les r√©ponses √† diff√©rents niveaux de comp√©tence
4. **Formatage des r√©ponses** : Impl√©menter une sortie structur√©e pour les questions de quiz

### √âtapes de mise en ≈ìuvre

**√âtape 1 : Configurez votre environnement**
```bash
pip install langchain langchain-openai
```

**√âtape 2 : Fonctionnalit√© de base du chat**
- Cr√©ez une classe `StudyAssistant`
- Impl√©mentez la m√©moire de conversation
- Ajoutez une configuration de personnalit√© pour le support √©ducatif

**√âtape 3 : Ajoutez des outils √©ducatifs**
- **Expliqueur de code** : D√©compose le code en parties compr√©hensibles
- **G√©n√©rateur de quiz** : Cr√©e des questions sur les concepts de programmation
- **Suivi de progression** : Suit les sujets abord√©s

**√âtape 4 : Fonctionnalit√©s avanc√©es (Optionnel)**
- Impl√©mentez des r√©ponses en streaming pour une meilleure exp√©rience utilisateur
- Ajoutez le chargement de documents pour int√©grer le mat√©riel de cours
- Cr√©ez des embeddings pour la recherche de contenu bas√©e sur la similarit√©

### Crit√®res d‚Äô√©valuation

| Fonctionnalit√© | Excellent (4) | Bien (3) | Satisfaisant (2) | √Ä am√©liorer (1) |
|----------------|---------------|----------|------------------|-----------------|
| **Flux de conversation** | R√©ponses naturelles, conscientes du contexte | Bonne r√©tention du contexte | Conversation basique | Pas de m√©moire entre √©changes |
| **Int√©gration d‚Äôoutils** | Plusieurs outils utiles fonctionnant parfaitement | 2+ outils impl√©ment√©s correctement | 1-2 outils basiques | Outils non fonctionnels |
| **Qualit√© du code** | Propre, bien document√©, gestion des erreurs | Structure correcte, documentation partielle | Fonctionnalit√© de base | Mauvaise structure, pas de gestion d‚Äôerreurs |
| **Valeur √©ducative** | Vraiment utile pour l‚Äôapprentissage, adaptatif | Bon support p√©dagogique | Explications basiques | Apport √©ducatif limit√© |

### Structure de code exemple

```python
class StudyAssistant:
    def __init__(self, skill_level="beginner"):
        # Initialiser LLM, outils et m√©moire de conversation
        pass
    
    def explain_code(self, code, language):
        # Outil : Expliquer le fonctionnement du code
        pass
    
    def generate_quiz(self, topic, difficulty):
        # Outil : Cr√©er des questions d'entra√Ænement
        pass
    
    def chat(self, user_input):
        # Interface principale de conversation
        pass

# Exemple d'utilisation
assistant = StudyAssistant(skill_level="intermediate")
response = assistant.chat("Explain how Python functions work")
```

**D√©fis bonus :**
- Ajoutez des capacit√©s d‚Äôentr√©e/sortie vocale
- Impl√©mentez une interface web avec Streamlit ou Flask
- Cr√©ez une base de connaissances √† partir du mat√©riel de cours avec embeddings
- Ajoutez le suivi de progression et des parcours d‚Äôapprentissage personnalis√©s

## üìà Votre calendrier de ma√Ætrise du d√©veloppement de frameworks IA

```mermaid
timeline
    title Parcours de D√©veloppement du Cadre d'IA de Production
    
    section Fondations du Cadre
        Compr√©hension des Abstractions
            : D√©cisions cadre principal vs API
            : Apprendre les concepts de base de LangChain
            : Impl√©menter les syst√®mes de types de messages
        
        Int√©gration Basique
            : Connexion aux fournisseurs d'IA
            : G√©rer l'authentification
            : G√©rer la configuration
    
    section Syst√®mes de Conversation
        Gestion de la M√©moire
            : Construire l'historique des conversations
            : Impl√©menter le suivi du contexte
            : G√©rer la persistance de session
        
        Interactions Avanc√©es
            : Ma√Ætriser les r√©ponses en streaming
            : Cr√©er des mod√®les d'invite
            : Impl√©menter la sortie structur√©e
    
    section Int√©gration d'Outils
        D√©veloppement d'Outils Personnalis√©s
            : Concevoir les sch√©mas d'outils
            : Impl√©menter l'appel de fonctions
            : G√©rer les API externes
        
        Automatisation des Flux de Travail
            : Encha√Æner plusieurs outils
            : Cr√©er des arbres de d√©cision
            : Construire des comportements d'agents
    
    section Applications en Production
        Architecture Compl√®te du Syst√®me
            : Combiner toutes les fonctionnalit√©s du cadre
            : Impl√©menter les limites d'erreur
            : Cr√©er un code maintenable
        
        Pr√™t pour l'Entreprise
            : G√©rer les probl√©matiques de scalabilit√©
            : Impl√©menter la surveillance
            : Construire des strat√©gies de d√©ploiement
```
**üéì √âtape de graduation** : Vous avez ma√Ætris√© le d√©veloppement de frameworks IA en utilisant les m√™mes outils et mod√®les qui alimentent les applications IA modernes. Ces comp√©tences repr√©sentent la pointe du d√©veloppement d‚Äôapplications IA et vous pr√©parent √† construire des syst√®mes intelligents de niveau entreprise.

**üîÑ Capacit√©s suivantes** :
- Pr√™t √† explorer des architectures IA avanc√©es (agents, syst√®mes multi-agents)
- Pr√©par√© pour cr√©er des syst√®mes RAG avec bases de donn√©es vectorielles
- √âquip√© pour r√©aliser des applications IA multi-modales
- Fondations pos√©es pour la mise √† l‚Äô√©chelle et l‚Äôoptimisation d‚Äôapplications IA

## R√©sum√©

üéâ Vous avez d√©sormais ma√Ætris√© les fondamentaux du d√©veloppement de frameworks IA et appris √† construire des applications IA sophistiqu√©es avec LangChain. Comme un apprentissage complet, vous avez acquis une bo√Æte √† outils importante. Passons en revue ce que vous avez accompli.

### Ce que vous avez appris

**Concepts fondamentaux du framework :**
- **Avantages des frameworks** : Comprendre quand choisir les frameworks plut√¥t que les appels API directs
- **Bases de LangChain** : Configuration et mise en place des connexions aux mod√®les IA
- **Types de messages** : Utilisation de `SystemMessage`, `HumanMessage`, et `AIMessage` pour des conversations structur√©es

**Fonctionnalit√©s avanc√©es :**
- **Appel d‚Äôoutils** : Cr√©ation et int√©gration d‚Äôoutils personnalis√©s pour des capacit√©s IA am√©lior√©es
- **M√©moire de conversation** : Maintien du contexte sur plusieurs tours de conversation
- **R√©ponses en streaming** : Livraison en temps r√©el des r√©ponses
- **Mod√®les de prompt** : Construction de prompts dynamiques et r√©utilisables
- **Sortie structur√©e** : Garantir des r√©ponses IA coh√©rentes et analysables
- **Embeddings** : Cr√©ation de capacit√©s de recherche s√©mantique et de traitement de documents

**Applications pratiques :**
- **Construction d‚Äôapplications compl√®tes** : Combinaison de plusieurs fonctionnalit√©s pour cr√©er des applications pr√™tes pour la production
- **Gestion des erreurs** : Mise en place d‚Äôune gestion robuste des erreurs et validations
- **Int√©gration d‚Äôoutils** : Cr√©ation d‚Äôoutils personnalis√©s √©tendant les capacit√©s IA

### Points cl√©s √† retenir

> üéØ **Rappelez-vous** : Les frameworks IA comme LangChain sont vos meilleurs amis pour masquer la complexit√© et offrir beaucoup de fonctionnalit√©s. Ils sont parfaits lorsque vous avez besoin de m√©moire de conversation, d‚Äôappel d‚Äôoutils, ou de travailler avec plusieurs mod√®les IA sans perdre la raison.

**Cadre de d√©cision pour l‚Äôint√©gration IA :**

```mermaid
flowchart TD
    A[Besoin d'int√©gration IA] --> B{Requ√™te simple unique ?}
    B -->|Oui| C[Appels API directs]
    B -->|Non| D{Besoin de m√©moire de conversation ?}
    D -->|Non| E[Int√©gration SDK]
    D -->|Oui| F{Besoin d'outils ou de fonctionnalit√©s complexes ?}
    F -->|Non| G[Framework avec configuration basique]
    F -->|Oui| H[Mise en ≈ìuvre compl√®te du framework]
    
    C --> I[Requ√™tes HTTP, d√©pendances minimales]
    E --> J[SDK fournisseur, sp√©cifique au mod√®le]
    G --> K[Chat basique LangChain]
    H --> L[LangChain avec outils, m√©moire, agents]
```
### O√π aller √† partir d‚Äôici ?

**Commencez √† construire d√®s maintenant :**
- Prenez ces concepts et cr√©ez quelque chose qui VOUS passionne !
- Testez diff√©rents mod√®les IA via LangChain ‚Äì c‚Äôest comme un terrain de jeu pour mod√®les IA
- Cr√©ez des outils qui r√©solvent de vrais probl√®mes dans votre travail ou vos projets

**Pr√™t pour le niveau sup√©rieur ?**
- **Agents IA** : Construisez des syst√®mes IA capables de planifier et ex√©cuter des t√¢ches complexes de mani√®re autonome
- **RAG (Retrieval-Augmented Generation)** : Combinez l‚ÄôIA avec vos propres bases de connaissance pour des applications ultra-performantes
- **IA multi-modale** : Travaillez ensemble avec du texte, des images et de l‚Äôaudio ‚Äì les possibilit√©s sont infinies !
- **D√©ploiement en production** : Apprenez √† faire √©voluer vos applications IA et √† les surveiller en conditions r√©elles

**Rejoignez la communaut√© :**
- La communaut√© LangChain est fantastique pour rester √† jour et apprendre les meilleures pratiques
- GitHub Models vous donne acc√®s √† des capacit√©s IA de pointe ‚Äì parfait pour exp√©rimenter
- Continuez √† pratiquer avec diff√©rents cas d‚Äôusage ‚Äì chaque projet vous apprendra quelque chose de nouveau

Vous avez maintenant les connaissances pour construire des applications intelligentes et conversationnelles qui aident les gens √† r√©soudre des probl√®mes r√©els. Comme les artisans de la Renaissance qui ont fusionn√© vision artistique et savoir-faire technique, vous pouvez d√©sormais combiner capacit√©s IA et applications pratiques. La question est : que cr√©erez-vous ? üöÄ

## D√©fi GitHub Copilot Agent üöÄ

Utilisez le mode Agent pour relever le d√©fi suivant :

**Description :** Construisez un assistant avanc√© de revue de code aliment√© par l‚ÄôIA qui combine plusieurs fonctionnalit√©s LangChain, incluant appel d‚Äôoutils, sortie structur√©e et m√©moire de conversation, pour fournir un feedback complet sur les soumissions de code.

**Prompt :** Cr√©ez une classe CodeReviewAssistant qui impl√©mente :
1. Un outil d‚Äôanalyse de la complexit√© du code et suggestions d‚Äôam√©lioration
2. Un outil de v√©rification du code selon les bonnes pratiques
3. Une sortie structur√©e utilisant des mod√®les Pydantic pour un format de revue coh√©rent
4. Une m√©moire de conversation pour suivre les sessions de revue
5. Une interface principale de chat pouvant g√©rer les soumissions de code et fournir un feedback d√©taill√© et exploitable

L‚Äôassistant doit pouvoir revoir du code dans plusieurs langages de programmation, maintenir le contexte sur plusieurs soumissions dans une session, et fournir √† la fois des scores synth√©tiques et des suggestions d‚Äôam√©lioration d√©taill√©es.

En savoir plus sur le [mode agent](https://code.visualstudio.com/blogs/2025/02/24/introducing-copilot-agent-mode) ici.

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**Avertissement** :  
Ce document a √©t√© traduit √† l‚Äôaide du service de traduction automatique [Co-op Translator](https://github.com/Azure/co-op-translator). Bien que nous nous efforcions d‚Äôassurer l‚Äôexactitude, veuillez noter que les traductions automatis√©es peuvent comporter des erreurs ou des inexactitudes. Le document original dans sa langue d‚Äôorigine doit √™tre consid√©r√© comme la source faisant foi. Pour les informations critiques, une traduction professionnelle humaine est recommand√©e. Nous d√©clinons toute responsabilit√© en cas de malentendus ou de mauvaises interpr√©tations r√©sultant de l‚Äôutilisation de cette traduction.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->